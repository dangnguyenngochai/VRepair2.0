int VP8_UVSSE ( MACROBLOCK * x ) //<S2SV> { //<S2SV> unsigned char * uptr , * vptr ; //<S2SV> unsigned char * upred_ptr = ( * ( x -> block [ 16 ] . base_src ) + x -> block [ 16 ] . src ) ; //<S2SV> unsigned char * vpred_ptr = ( * ( x -> block [ 20 ] . base_src ) + x -> block [ 20 ] . src ) ; //<S2SV> int uv_stride = x -> block [ 16 ] . src_stride ; //<S2SV> unsigned int sse1 = 0 ; //<S2SV> unsigned int sse2 = 0 ; //<S2SV> int mv_row = x -> e_mbd . mode_info_context -> mbmi . mv . as_mv . row ; //<S2SV> int mv_col = x -> e_mbd . mode_info_context -> mbmi . mv . as_mv . col ; //<S2SV> int offset ; //<S2SV> int pre_stride = x -> e_mbd . pre . uv_stride ; //<S2SV> if ( mv_row < 0 ) //<S2SV> mv_row -= 1 ; //<S2SV> else //<S2SV> mv_row += 1 ; //<S2SV> if ( mv_col < 0 ) //<S2SV> mv_col -= 1 ; //<S2SV> else //<S2SV> mv_col += 1 ; //<S2SV> mv_row /= 2 ; //<S2SV> mv_col /= 2 ; //<S2SV> offset = ( mv_row >> 3 ) * pre_stride + ( mv_col >> 3 ) ; //<S2SV> uptr = x -> e_mbd . pre . u_buffer + offset ; //<S2SV> vptr = x -> e_mbd . pre . v_buffer + offset ; //<S2SV> if ( ( mv_row | mv_col ) & 7 ) //<S2SV> { //<S2SV> vpx_sub_pixel_variance8x8 ( uptr , pre_stride , //<S2SV> mv_col & 7 , mv_row & 7 , upred_ptr , uv_stride , & sse2 ) ; //<S2SV> vpx_sub_pixel_variance8x8 ( vptr , pre_stride , //<S2SV> mv_col & 7 , mv_row & 7 , vpred_ptr , uv_stride , & sse1 ) ; //<S2SV> sse2 += sse1 ; //<S2SV> } //<S2SV> else //<S2SV> { //<S2SV> vpx_variance8x8 ( uptr , pre_stride , //<S2SV> upred_ptr , uv_stride , & sse2 ) ; //<S2SV> vpx_variance8x8 ( vptr , pre_stride , //<S2SV> vpred_ptr , uv_stride , & sse1 ) ; //<S2SV> sse2 += sse1 ; //<S2SV> } //<S2SV> return sse2 ; //<S2SV> } //<S2SV> 