int vp8_denoiser_filter_neon ( unsigned char * mc_running_avg_y , //<S2SV> int mc_running_avg_y_stride , //<S2SV> unsigned char * running_avg_y , //<S2SV> int running_avg_y_stride , //<S2SV> unsigned char * sig , int sig_stride , //<S2SV> unsigned int motion_magnitude , //<S2SV> int increase_denoising ) { //<S2SV> int shift_inc = ( increase_denoising && //<S2SV> motion_magnitude <= MOTION_MAGNITUDE_THRESHOLD ) ? 1 : 0 ; //<S2SV> const uint8x16_t v_level1_adjustment = vmovq_n_u8 ( //<S2SV> ( motion_magnitude <= MOTION_MAGNITUDE_THRESHOLD ) ? 4 + shift_inc : 3 ) ; //<S2SV> const uint8x16_t v_delta_level_1_and_2 = vdupq_n_u8 ( 1 ) ; //<S2SV> const uint8x16_t v_delta_level_2_and_3 = vdupq_n_u8 ( 2 ) ; //<S2SV> const uint8x16_t v_level1_threshold = vmovq_n_u8 ( 4 + shift_inc ) ; //<S2SV> const uint8x16_t v_level2_threshold = vdupq_n_u8 ( 8 ) ; //<S2SV> const uint8x16_t v_level3_threshold = vdupq_n_u8 ( 16 ) ; //<S2SV> int64x2_t v_sum_diff_total = vdupq_n_s64 ( 0 ) ; //<S2SV> int r ; //<S2SV> for ( r = 0 ; r < 16 ; ++ r ) { //<S2SV> const uint8x16_t v_sig = vld1q_u8 ( sig ) ; //<S2SV> const uint8x16_t v_mc_running_avg_y = vld1q_u8 ( mc_running_avg_y ) ; //<S2SV> const uint8x16_t v_abs_diff = vabdq_u8 ( v_sig , v_mc_running_avg_y ) ; //<S2SV> const uint8x16_t v_diff_pos_mask = vcltq_u8 ( v_sig , v_mc_running_avg_y ) ; //<S2SV> const uint8x16_t v_diff_neg_mask = vcgtq_u8 ( v_sig , v_mc_running_avg_y ) ; //<S2SV> const uint8x16_t v_level1_mask = vcleq_u8 ( v_level1_threshold , //<S2SV> v_abs_diff ) ; //<S2SV> const uint8x16_t v_level2_mask = vcleq_u8 ( v_level2_threshold , //<S2SV> v_abs_diff ) ; //<S2SV> const uint8x16_t v_level3_mask = vcleq_u8 ( v_level3_threshold , //<S2SV> v_abs_diff ) ; //<S2SV> const uint8x16_t v_level2_adjustment = vandq_u8 ( v_level2_mask , //<S2SV> v_delta_level_1_and_2 ) ; //<S2SV> const uint8x16_t v_level3_adjustment = vandq_u8 ( v_level3_mask , //<S2SV> v_delta_level_2_and_3 ) ; //<S2SV> const uint8x16_t v_level1and2_adjustment = vaddq_u8 ( v_level1_adjustment , //<S2SV> v_level2_adjustment ) ; //<S2SV> const uint8x16_t v_level1and2and3_adjustment = vaddq_u8 ( //<S2SV> v_level1and2_adjustment , v_level3_adjustment ) ; //<S2SV> const uint8x16_t v_abs_adjustment = vbslq_u8 ( v_level1_mask , //<S2SV> v_level1and2and3_adjustment , v_abs_diff ) ; //<S2SV> const uint8x16_t v_pos_adjustment = vandq_u8 ( v_diff_pos_mask , //<S2SV> v_abs_adjustment ) ; //<S2SV> const uint8x16_t v_neg_adjustment = vandq_u8 ( v_diff_neg_mask , //<S2SV> v_abs_adjustment ) ; //<S2SV> uint8x16_t v_running_avg_y = vqaddq_u8 ( v_sig , v_pos_adjustment ) ; //<S2SV> v_running_avg_y = vqsubq_u8 ( v_running_avg_y , v_neg_adjustment ) ; //<S2SV> vst1q_u8 ( running_avg_y , v_running_avg_y ) ; //<S2SV> { //<S2SV> const int8x16_t v_sum_diff = //<S2SV> vqsubq_s8 ( vreinterpretq_s8_u8 ( v_pos_adjustment ) , //<S2SV> vreinterpretq_s8_u8 ( v_neg_adjustment ) ) ; //<S2SV> const int16x8_t fe_dc_ba_98_76_54_32_10 = vpaddlq_s8 ( v_sum_diff ) ; //<S2SV> const int32x4_t fedc_ba98_7654_3210 = //<S2SV> vpaddlq_s16 ( fe_dc_ba_98_76_54_32_10 ) ; //<S2SV> const int64x2_t fedcba98_76543210 = //<S2SV> vpaddlq_s32 ( fedc_ba98_7654_3210 ) ; //<S2SV> v_sum_diff_total = vqaddq_s64 ( v_sum_diff_total , fedcba98_76543210 ) ; //<S2SV> } //<S2SV> sig += sig_stride ; //<S2SV> mc_running_avg_y += mc_running_avg_y_stride ; //<S2SV> running_avg_y += running_avg_y_stride ; //<S2SV> } //<S2SV> { //<S2SV> int64x1_t x = vqadd_s64 ( vget_high_s64 ( v_sum_diff_total ) , //<S2SV> vget_low_s64 ( v_sum_diff_total ) ) ; //<S2SV> int sum_diff = vget_lane_s32 ( vabs_s32 ( vreinterpret_s32_s64 ( x ) ) , 0 ) ; //<S2SV> int sum_diff_thresh = SUM_DIFF_THRESHOLD ; //<S2SV> if ( increase_denoising ) sum_diff_thresh = SUM_DIFF_THRESHOLD_HIGH ; //<S2SV> if ( sum_diff > sum_diff_thresh ) { //<S2SV> int delta = ( ( sum_diff - sum_diff_thresh ) >> 8 ) + 1 ; //<S2SV> if ( delta < 4 ) { //<S2SV> const uint8x16_t k_delta = vmovq_n_u8 ( delta ) ; //<S2SV> sig -= sig_stride * 16 ; //<S2SV> mc_running_avg_y -= mc_running_avg_y_stride * 16 ; //<S2SV> running_avg_y -= running_avg_y_stride * 16 ; //<S2SV> for ( r = 0 ; r < 16 ; ++ r ) { //<S2SV> uint8x16_t v_running_avg_y = vld1q_u8 ( running_avg_y ) ; //<S2SV> const uint8x16_t v_sig = vld1q_u8 ( sig ) ; //<S2SV> const uint8x16_t v_mc_running_avg_y = vld1q_u8 ( mc_running_avg_y ) ; //<S2SV> const uint8x16_t v_abs_diff = vabdq_u8 ( v_sig , //<S2SV> v_mc_running_avg_y ) ; //<S2SV> const uint8x16_t v_diff_pos_mask = vcltq_u8 ( v_sig , //<S2SV> v_mc_running_avg_y ) ; //<S2SV> const uint8x16_t v_diff_neg_mask = vcgtq_u8 ( v_sig , //<S2SV> v_mc_running_avg_y ) ; //<S2SV> const uint8x16_t v_abs_adjustment = //<S2SV> vminq_u8 ( v_abs_diff , ( k_delta ) ) ; //<S2SV> const uint8x16_t v_pos_adjustment = vandq_u8 ( v_diff_pos_mask , //<S2SV> v_abs_adjustment ) ; //<S2SV> const uint8x16_t v_neg_adjustment = vandq_u8 ( v_diff_neg_mask , //<S2SV> v_abs_adjustment ) ; //<S2SV> v_running_avg_y = vqsubq_u8 ( v_running_avg_y , v_pos_adjustment ) ; //<S2SV> v_running_avg_y = vqaddq_u8 ( v_running_avg_y , v_neg_adjustment ) ; //<S2SV> vst1q_u8 ( running_avg_y , v_running_avg_y ) ; //<S2SV> { //<S2SV> const int8x16_t v_sum_diff = //<S2SV> vqsubq_s8 ( vreinterpretq_s8_u8 ( v_neg_adjustment ) , //<S2SV> vreinterpretq_s8_u8 ( v_pos_adjustment ) ) ; //<S2SV> const int16x8_t fe_dc_ba_98_76_54_32_10 = //<S2SV> vpaddlq_s8 ( v_sum_diff ) ; //<S2SV> const int32x4_t fedc_ba98_7654_3210 = //<S2SV> vpaddlq_s16 ( fe_dc_ba_98_76_54_32_10 ) ; //<S2SV> const int64x2_t fedcba98_76543210 = //<S2SV> vpaddlq_s32 ( fedc_ba98_7654_3210 ) ; //<S2SV> v_sum_diff_total = vqaddq_s64 ( v_sum_diff_total , //<S2SV> fedcba98_76543210 ) ; //<S2SV> } //<S2SV> sig += sig_stride ; //<S2SV> mc_running_avg_y += mc_running_avg_y_stride ; //<S2SV> running_avg_y += running_avg_y_stride ; //<S2SV> } //<S2SV> { //<S2SV> x = vqadd_s64 ( vget_high_s64 ( v_sum_diff_total ) , //<S2SV> vget_low_s64 ( v_sum_diff_total ) ) ; //<S2SV> sum_diff = vget_lane_s32 ( vabs_s32 ( vreinterpret_s32_s64 ( x ) ) , 0 ) ; //<S2SV> if ( sum_diff > sum_diff_thresh ) { //<S2SV> return COPY_BLOCK ; //<S2SV> } //<S2SV> } //<S2SV> } else { //<S2SV> return COPY_BLOCK ; //<S2SV> } //<S2SV> } //<S2SV> } //<S2SV> running_avg_y -= running_avg_y_stride * 16 ; //<S2SV> sig -= sig_stride * 16 ; //<S2SV> vp8_copy_mem16x16 ( running_avg_y , running_avg_y_stride , sig , sig_stride ) ; //<S2SV> return FILTER_BLOCK ; //<S2SV> } //<S2SV> 