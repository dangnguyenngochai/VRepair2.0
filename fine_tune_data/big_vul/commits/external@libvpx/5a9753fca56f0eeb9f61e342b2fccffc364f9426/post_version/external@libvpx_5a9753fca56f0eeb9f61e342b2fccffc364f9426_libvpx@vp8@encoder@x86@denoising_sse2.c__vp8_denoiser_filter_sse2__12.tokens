int vp8_denoiser_filter_sse2 ( unsigned char * mc_running_avg_y , //<S2SV> int mc_avg_y_stride , //<S2SV> unsigned char * running_avg_y , int avg_y_stride , //<S2SV> unsigned char * sig , int sig_stride , //<S2SV> unsigned int motion_magnitude , //<S2SV> int increase_denoising ) //<S2SV> { //<S2SV> unsigned char * running_avg_y_start = running_avg_y ; //<S2SV> unsigned char * sig_start = sig ; //<S2SV> unsigned int sum_diff_thresh ; //<S2SV> int r ; //<S2SV> int shift_inc = ( increase_denoising && //<S2SV> motion_magnitude <= MOTION_MAGNITUDE_THRESHOLD ) ? 1 : 0 ; //<S2SV> __m128i acc_diff = _mm_setzero_si128 ( ) ; //<S2SV> const __m128i k_0 = _mm_setzero_si128 ( ) ; //<S2SV> const __m128i k_4 = _mm_set1_epi8 ( 4 + shift_inc ) ; //<S2SV> const __m128i k_8 = _mm_set1_epi8 ( 8 ) ; //<S2SV> const __m128i k_16 = _mm_set1_epi8 ( 16 ) ; //<S2SV> const __m128i l3 = _mm_set1_epi8 ( //<S2SV> ( motion_magnitude <= MOTION_MAGNITUDE_THRESHOLD ) ? //<S2SV> 7 + shift_inc : 6 ) ; //<S2SV> const __m128i l32 = _mm_set1_epi8 ( 2 ) ; //<S2SV> const __m128i l21 = _mm_set1_epi8 ( 1 ) ; //<S2SV> for ( r = 0 ; r < 16 ; ++ r ) //<S2SV> { //<S2SV> const __m128i v_sig = _mm_loadu_si128 ( ( __m128i * ) ( & sig [ 0 ] ) ) ; //<S2SV> const __m128i v_mc_running_avg_y = _mm_loadu_si128 ( //<S2SV> ( __m128i * ) ( & mc_running_avg_y [ 0 ] ) ) ; //<S2SV> __m128i v_running_avg_y ; //<S2SV> const __m128i pdiff = _mm_subs_epu8 ( v_mc_running_avg_y , v_sig ) ; //<S2SV> const __m128i ndiff = _mm_subs_epu8 ( v_sig , v_mc_running_avg_y ) ; //<S2SV> const __m128i diff_sign = _mm_cmpeq_epi8 ( pdiff , k_0 ) ; //<S2SV> const __m128i clamped_absdiff = _mm_min_epu8 ( //<S2SV> _mm_or_si128 ( pdiff , ndiff ) , k_16 ) ; //<S2SV> const __m128i mask2 = _mm_cmpgt_epi8 ( k_16 , clamped_absdiff ) ; //<S2SV> const __m128i mask1 = _mm_cmpgt_epi8 ( k_8 , clamped_absdiff ) ; //<S2SV> const __m128i mask0 = _mm_cmpgt_epi8 ( k_4 , clamped_absdiff ) ; //<S2SV> __m128i adj2 = _mm_and_si128 ( mask2 , l32 ) ; //<S2SV> const __m128i adj1 = _mm_and_si128 ( mask1 , l21 ) ; //<S2SV> const __m128i adj0 = _mm_and_si128 ( mask0 , clamped_absdiff ) ; //<S2SV> __m128i adj , padj , nadj ; //<S2SV> adj2 = _mm_add_epi8 ( adj2 , adj1 ) ; //<S2SV> adj = _mm_sub_epi8 ( l3 , adj2 ) ; //<S2SV> adj = _mm_andnot_si128 ( mask0 , adj ) ; //<S2SV> adj = _mm_or_si128 ( adj , adj0 ) ; //<S2SV> padj = _mm_andnot_si128 ( diff_sign , adj ) ; //<S2SV> nadj = _mm_and_si128 ( diff_sign , adj ) ; //<S2SV> v_running_avg_y = _mm_adds_epu8 ( v_sig , padj ) ; //<S2SV> v_running_avg_y = _mm_subs_epu8 ( v_running_avg_y , nadj ) ; //<S2SV> _mm_storeu_si128 ( ( __m128i * ) running_avg_y , v_running_avg_y ) ; //<S2SV> acc_diff = _mm_adds_epi8 ( acc_diff , padj ) ; //<S2SV> acc_diff = _mm_subs_epi8 ( acc_diff , nadj ) ; //<S2SV> sig += sig_stride ; //<S2SV> mc_running_avg_y += mc_avg_y_stride ; //<S2SV> running_avg_y += avg_y_stride ; //<S2SV> } //<S2SV> { //<S2SV> unsigned int abs_sum_diff = abs_sum_diff_16x1 ( acc_diff ) ; //<S2SV> sum_diff_thresh = SUM_DIFF_THRESHOLD ; //<S2SV> if ( increase_denoising ) sum_diff_thresh = SUM_DIFF_THRESHOLD_HIGH ; //<S2SV> if ( abs_sum_diff > sum_diff_thresh ) { //<S2SV> int delta = ( ( abs_sum_diff - sum_diff_thresh ) >> 8 ) + 1 ; //<S2SV> if ( delta < 4 ) { //<S2SV> const __m128i k_delta = _mm_set1_epi8 ( delta ) ; //<S2SV> sig -= sig_stride * 16 ; //<S2SV> mc_running_avg_y -= mc_avg_y_stride * 16 ; //<S2SV> running_avg_y -= avg_y_stride * 16 ; //<S2SV> for ( r = 0 ; r < 16 ; ++ r ) { //<S2SV> __m128i v_running_avg_y = //<S2SV> _mm_loadu_si128 ( ( __m128i * ) ( & running_avg_y [ 0 ] ) ) ; //<S2SV> const __m128i v_sig = _mm_loadu_si128 ( ( __m128i * ) ( & sig [ 0 ] ) ) ; //<S2SV> const __m128i v_mc_running_avg_y = //<S2SV> _mm_loadu_si128 ( ( __m128i * ) ( & mc_running_avg_y [ 0 ] ) ) ; //<S2SV> const __m128i pdiff = _mm_subs_epu8 ( v_mc_running_avg_y , v_sig ) ; //<S2SV> const __m128i ndiff = _mm_subs_epu8 ( v_sig , v_mc_running_avg_y ) ; //<S2SV> const __m128i diff_sign = _mm_cmpeq_epi8 ( pdiff , k_0 ) ; //<S2SV> const __m128i adj = //<S2SV> _mm_min_epu8 ( _mm_or_si128 ( pdiff , ndiff ) , k_delta ) ; //<S2SV> __m128i padj , nadj ; //<S2SV> padj = _mm_andnot_si128 ( diff_sign , adj ) ; //<S2SV> nadj = _mm_and_si128 ( diff_sign , adj ) ; //<S2SV> v_running_avg_y = _mm_subs_epu8 ( v_running_avg_y , padj ) ; //<S2SV> v_running_avg_y = _mm_adds_epu8 ( v_running_avg_y , nadj ) ; //<S2SV> _mm_storeu_si128 ( ( __m128i * ) running_avg_y , v_running_avg_y ) ; //<S2SV> acc_diff = _mm_subs_epi8 ( acc_diff , padj ) ; //<S2SV> acc_diff = _mm_adds_epi8 ( acc_diff , nadj ) ; //<S2SV> sig += sig_stride ; //<S2SV> mc_running_avg_y += mc_avg_y_stride ; //<S2SV> running_avg_y += avg_y_stride ; //<S2SV> } //<S2SV> abs_sum_diff = abs_sum_diff_16x1 ( acc_diff ) ; //<S2SV> if ( abs_sum_diff > sum_diff_thresh ) { //<S2SV> return COPY_BLOCK ; //<S2SV> } //<S2SV> } else { //<S2SV> return COPY_BLOCK ; //<S2SV> } //<S2SV> } //<S2SV> } //<S2SV> vp8_copy_mem16x16 ( running_avg_y_start , avg_y_stride , sig_start , sig_stride ) ; //<S2SV> return FILTER_BLOCK ; //<S2SV> } //<S2SV> 