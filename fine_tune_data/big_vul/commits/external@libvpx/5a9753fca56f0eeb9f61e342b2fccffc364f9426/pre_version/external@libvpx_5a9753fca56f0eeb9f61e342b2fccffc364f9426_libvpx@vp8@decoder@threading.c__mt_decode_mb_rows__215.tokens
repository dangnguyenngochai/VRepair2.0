static void mt_decode_mb_rows ( VP8D_COMP * pbi , MACROBLOCKD * xd , int start_mb_row ) //<S2SV> { //<S2SV> volatile const int * last_row_current_mb_col ; //<S2SV> volatile int * current_mb_col ; //<S2SV> int mb_row ; //<S2SV> VP8_COMMON * pc = & pbi -> common ; //<S2SV> const int nsync = pbi -> sync_range ; //<S2SV> const int first_row_no_sync_above = pc -> mb_cols + nsync ; //<S2SV> int num_part = 1 << pbi -> common . multi_token_partition ; //<S2SV> int last_mb_row = start_mb_row ; //<S2SV> YV12_BUFFER_CONFIG * yv12_fb_new = pbi -> dec_fb_ref [ INTRA_FRAME ] ; //<S2SV> YV12_BUFFER_CONFIG * yv12_fb_lst = pbi -> dec_fb_ref [ LAST_FRAME ] ; //<S2SV> int recon_y_stride = yv12_fb_new -> y_stride ; //<S2SV> int recon_uv_stride = yv12_fb_new -> uv_stride ; //<S2SV> unsigned char * ref_buffer [ MAX_REF_FRAMES ] [ 3 ] ; //<S2SV> unsigned char * dst_buffer [ 3 ] ; //<S2SV> int i ; //<S2SV> int ref_fb_corrupted [ MAX_REF_FRAMES ] ; //<S2SV> ref_fb_corrupted [ INTRA_FRAME ] = 0 ; //<S2SV> for ( i = 1 ; i < MAX_REF_FRAMES ; i ++ ) //<S2SV> { //<S2SV> YV12_BUFFER_CONFIG * this_fb = pbi -> dec_fb_ref [ i ] ; //<S2SV> ref_buffer [ i ] [ 0 ] = this_fb -> y_buffer ; //<S2SV> ref_buffer [ i ] [ 1 ] = this_fb -> u_buffer ; //<S2SV> ref_buffer [ i ] [ 2 ] = this_fb -> v_buffer ; //<S2SV> ref_fb_corrupted [ i ] = this_fb -> corrupted ; //<S2SV> } //<S2SV> dst_buffer [ 0 ] = yv12_fb_new -> y_buffer ; //<S2SV> dst_buffer [ 1 ] = yv12_fb_new -> u_buffer ; //<S2SV> dst_buffer [ 2 ] = yv12_fb_new -> v_buffer ; //<S2SV> xd -> up_available = ( start_mb_row != 0 ) ; //<S2SV> for ( mb_row = start_mb_row ; mb_row < pc -> mb_rows ; mb_row += ( pbi -> decoding_thread_count + 1 ) ) //<S2SV> { //<S2SV> int recon_yoffset , recon_uvoffset ; //<S2SV> int mb_col ; //<S2SV> int filter_level ; //<S2SV> loop_filter_info_n * lfi_n = & pc -> lf_info ; //<S2SV> last_mb_row = mb_row ; //<S2SV> xd -> current_bc = & pbi -> mbc [ mb_row % num_part ] ; //<S2SV> if ( mb_row > 0 ) //<S2SV> last_row_current_mb_col = & pbi -> mt_current_mb_col [ mb_row - 1 ] ; //<S2SV> else //<S2SV> last_row_current_mb_col = & first_row_no_sync_above ; //<S2SV> current_mb_col = & pbi -> mt_current_mb_col [ mb_row ] ; //<S2SV> recon_yoffset = mb_row * recon_y_stride * 16 ; //<S2SV> recon_uvoffset = mb_row * recon_uv_stride * 8 ; //<S2SV> xd -> above_context = pc -> above_context ; //<S2SV> vpx_memset ( xd -> left_context , 0 , sizeof ( ENTROPY_CONTEXT_PLANES ) ) ; //<S2SV> xd -> left_available = 0 ; //<S2SV> xd -> mb_to_top_edge = - ( ( mb_row * 16 ) ) << 3 ; //<S2SV> xd -> mb_to_bottom_edge = ( ( pc -> mb_rows - 1 - mb_row ) * 16 ) << 3 ; //<S2SV> if ( pbi -> common . filter_level ) //<S2SV> { //<S2SV> xd -> recon_above [ 0 ] = pbi -> mt_yabove_row [ mb_row ] + 0 * 16 + 32 ; //<S2SV> xd -> recon_above [ 1 ] = pbi -> mt_uabove_row [ mb_row ] + 0 * 8 + 16 ; //<S2SV> xd -> recon_above [ 2 ] = pbi -> mt_vabove_row [ mb_row ] + 0 * 8 + 16 ; //<S2SV> xd -> recon_left [ 0 ] = pbi -> mt_yleft_col [ mb_row ] ; //<S2SV> xd -> recon_left [ 1 ] = pbi -> mt_uleft_col [ mb_row ] ; //<S2SV> xd -> recon_left [ 2 ] = pbi -> mt_vleft_col [ mb_row ] ; //<S2SV> xd -> recon_left_stride [ 0 ] = 1 ; //<S2SV> xd -> recon_left_stride [ 1 ] = 1 ; //<S2SV> } //<S2SV> else //<S2SV> { //<S2SV> xd -> recon_above [ 0 ] = dst_buffer [ 0 ] + recon_yoffset ; //<S2SV> xd -> recon_above [ 1 ] = dst_buffer [ 1 ] + recon_uvoffset ; //<S2SV> xd -> recon_above [ 2 ] = dst_buffer [ 2 ] + recon_uvoffset ; //<S2SV> xd -> recon_left [ 0 ] = xd -> recon_above [ 0 ] - 1 ; //<S2SV> xd -> recon_left [ 1 ] = xd -> recon_above [ 1 ] - 1 ; //<S2SV> xd -> recon_left [ 2 ] = xd -> recon_above [ 2 ] - 1 ; //<S2SV> xd -> recon_above [ 0 ] -= xd -> dst . y_stride ; //<S2SV> xd -> recon_above [ 1 ] -= xd -> dst . uv_stride ; //<S2SV> xd -> recon_above [ 2 ] -= xd -> dst . uv_stride ; //<S2SV> xd -> recon_left_stride [ 0 ] = xd -> dst . y_stride ; //<S2SV> xd -> recon_left_stride [ 1 ] = xd -> dst . uv_stride ; //<S2SV> setup_intra_recon_left ( xd -> recon_left [ 0 ] , xd -> recon_left [ 1 ] , //<S2SV> xd -> recon_left [ 2 ] , xd -> dst . y_stride , //<S2SV> xd -> dst . uv_stride ) ; //<S2SV> } //<S2SV> for ( mb_col = 0 ; mb_col < pc -> mb_cols ; mb_col ++ ) //<S2SV> { //<S2SV> * current_mb_col = mb_col - 1 ; //<S2SV> if ( ( mb_col & ( nsync - 1 ) ) == 0 ) //<S2SV> { //<S2SV> while ( mb_col > ( * last_row_current_mb_col - nsync ) ) //<S2SV> { //<S2SV> x86_pause_hint ( ) ; //<S2SV> thread_sleep ( 0 ) ; //<S2SV> } //<S2SV> } //<S2SV> xd -> mb_to_left_edge = - ( ( mb_col * 16 ) << 3 ) ; //<S2SV> xd -> mb_to_right_edge = ( ( pc -> mb_cols - 1 - mb_col ) * 16 ) << 3 ; //<S2SV> # if CONFIG_ERROR_CONCEALMENT //<S2SV> { //<S2SV> int corrupt_residual = //<S2SV> ( ! pbi -> independent_partitions && //<S2SV> pbi -> frame_corrupt_residual ) || //<S2SV> vp8dx_bool_error ( xd -> current_bc ) ; //<S2SV> if ( pbi -> ec_active && //<S2SV> ( xd -> mode_info_context -> mbmi . ref_frame == //<S2SV> INTRA_FRAME ) && //<S2SV> corrupt_residual ) //<S2SV> { //<S2SV> vp8_interpolate_motion ( xd , //<S2SV> mb_row , mb_col , //<S2SV> pc -> mb_rows , pc -> mb_cols , //<S2SV> pc -> mode_info_stride ) ; //<S2SV> } //<S2SV> } //<S2SV> # endif //<S2SV> xd -> dst . y_buffer = dst_buffer [ 0 ] + recon_yoffset ; //<S2SV> xd -> dst . u_buffer = dst_buffer [ 1 ] + recon_uvoffset ; //<S2SV> xd -> dst . v_buffer = dst_buffer [ 2 ] + recon_uvoffset ; //<S2SV> xd -> pre . y_buffer = ref_buffer [ xd -> mode_info_context -> mbmi . ref_frame ] [ 0 ] + recon_yoffset ; //<S2SV> xd -> pre . u_buffer = ref_buffer [ xd -> mode_info_context -> mbmi . ref_frame ] [ 1 ] + recon_uvoffset ; //<S2SV> xd -> pre . v_buffer = ref_buffer [ xd -> mode_info_context -> mbmi . ref_frame ] [ 2 ] + recon_uvoffset ; //<S2SV> xd -> corrupted |= ref_fb_corrupted [ xd -> mode_info_context -> mbmi . ref_frame ] ; //<S2SV> mt_decode_macroblock ( pbi , xd , 0 ) ; //<S2SV> xd -> left_available = 1 ; //<S2SV> xd -> corrupted |= vp8dx_bool_error ( xd -> current_bc ) ; //<S2SV> xd -> recon_above [ 0 ] += 16 ; //<S2SV> xd -> recon_above [ 1 ] += 8 ; //<S2SV> xd -> recon_above [ 2 ] += 8 ; //<S2SV> if ( ! pbi -> common . filter_level ) //<S2SV> { //<S2SV> xd -> recon_left [ 0 ] += 16 ; //<S2SV> xd -> recon_left [ 1 ] += 8 ; //<S2SV> xd -> recon_left [ 2 ] += 8 ; //<S2SV> } //<S2SV> if ( pbi -> common . filter_level ) //<S2SV> { //<S2SV> int skip_lf = ( xd -> mode_info_context -> mbmi . mode != B_PRED && //<S2SV> xd -> mode_info_context -> mbmi . mode != SPLITMV && //<S2SV> xd -> mode_info_context -> mbmi . mb_skip_coeff ) ; //<S2SV> const int mode_index = lfi_n -> mode_lf_lut [ xd -> mode_info_context -> mbmi . mode ] ; //<S2SV> const int seg = xd -> mode_info_context -> mbmi . segment_id ; //<S2SV> const int ref_frame = xd -> mode_info_context -> mbmi . ref_frame ; //<S2SV> filter_level = lfi_n -> lvl [ seg ] [ ref_frame ] [ mode_index ] ; //<S2SV> if ( mb_row != pc -> mb_rows - 1 ) //<S2SV> { //<S2SV> vpx_memcpy ( ( pbi -> mt_yabove_row [ mb_row + 1 ] + 32 + mb_col * 16 ) , ( xd -> dst . y_buffer + 15 * recon_y_stride ) , 16 ) ; //<S2SV> vpx_memcpy ( ( pbi -> mt_uabove_row [ mb_row + 1 ] + 16 + mb_col * 8 ) , ( xd -> dst . u_buffer + 7 * recon_uv_stride ) , 8 ) ; //<S2SV> vpx_memcpy ( ( pbi -> mt_vabove_row [ mb_row + 1 ] + 16 + mb_col * 8 ) , ( xd -> dst . v_buffer + 7 * recon_uv_stride ) , 8 ) ; //<S2SV> } //<S2SV> if ( mb_col != pc -> mb_cols - 1 ) //<S2SV> { //<S2SV> MODE_INFO * next = xd -> mode_info_context + 1 ; //<S2SV> if ( next -> mbmi . ref_frame == INTRA_FRAME ) //<S2SV> { //<S2SV> for ( i = 0 ; i < 16 ; i ++ ) //<S2SV> pbi -> mt_yleft_col [ mb_row ] [ i ] = xd -> dst . y_buffer [ i * recon_y_stride + 15 ] ; //<S2SV> for ( i = 0 ; i < 8 ; i ++ ) //<S2SV> { //<S2SV> pbi -> mt_uleft_col [ mb_row ] [ i ] = xd -> dst . u_buffer [ i * recon_uv_stride + 7 ] ; //<S2SV> pbi -> mt_vleft_col [ mb_row ] [ i ] = xd -> dst . v_buffer [ i * recon_uv_stride + 7 ] ; //<S2SV> } //<S2SV> } //<S2SV> } //<S2SV> if ( filter_level ) //<S2SV> { //<S2SV> if ( pc -> filter_type == NORMAL_LOOPFILTER ) //<S2SV> { //<S2SV> loop_filter_info lfi ; //<S2SV> FRAME_TYPE frame_type = pc -> frame_type ; //<S2SV> const int hev_index = lfi_n -> hev_thr_lut [ frame_type ] [ filter_level ] ; //<S2SV> lfi . mblim = lfi_n -> mblim [ filter_level ] ; //<S2SV> lfi . blim = lfi_n -> blim [ filter_level ] ; //<S2SV> lfi . lim = lfi_n -> lim [ filter_level ] ; //<S2SV> lfi . hev_thr = lfi_n -> hev_thr [ hev_index ] ; //<S2SV> if ( mb_col > 0 ) //<S2SV> vp8_loop_filter_mbv //<S2SV> ( xd -> dst . y_buffer , xd -> dst . u_buffer , xd -> dst . v_buffer , recon_y_stride , recon_uv_stride , & lfi ) ; //<S2SV> if ( ! skip_lf ) //<S2SV> vp8_loop_filter_bv //<S2SV> ( xd -> dst . y_buffer , xd -> dst . u_buffer , xd -> dst . v_buffer , recon_y_stride , recon_uv_stride , & lfi ) ; //<S2SV> if ( mb_row > 0 ) //<S2SV> vp8_loop_filter_mbh //<S2SV> ( xd -> dst . y_buffer , xd -> dst . u_buffer , xd -> dst . v_buffer , recon_y_stride , recon_uv_stride , & lfi ) ; //<S2SV> if ( ! skip_lf ) //<S2SV> vp8_loop_filter_bh //<S2SV> ( xd -> dst . y_buffer , xd -> dst . u_buffer , xd -> dst . v_buffer , recon_y_stride , recon_uv_stride , & lfi ) ; //<S2SV> } //<S2SV> else //<S2SV> { //<S2SV> if ( mb_col > 0 ) //<S2SV> vp8_loop_filter_simple_mbv //<S2SV> ( xd -> dst . y_buffer , recon_y_stride , lfi_n -> mblim [ filter_level ] ) ; //<S2SV> if ( ! skip_lf ) //<S2SV> vp8_loop_filter_simple_bv //<S2SV> ( xd -> dst . y_buffer , recon_y_stride , lfi_n -> blim [ filter_level ] ) ; //<S2SV> if ( mb_row > 0 ) //<S2SV> vp8_loop_filter_simple_mbh //<S2SV> ( xd -> dst . y_buffer , recon_y_stride , lfi_n -> mblim [ filter_level ] ) ; //<S2SV> if ( ! skip_lf ) //<S2SV> vp8_loop_filter_simple_bh //<S2SV> ( xd -> dst . y_buffer , recon_y_stride , lfi_n -> blim [ filter_level ] ) ; //<S2SV> } //<S2SV> } //<S2SV> } //<S2SV> recon_yoffset += 16 ; //<S2SV> recon_uvoffset += 8 ; //<S2SV> ++ xd -> mode_info_context ; //<S2SV> xd -> above_context ++ ; //<S2SV> } //<S2SV> if ( pbi -> common . filter_level ) //<S2SV> { //<S2SV> if ( mb_row != pc -> mb_rows - 1 ) //<S2SV> { //<S2SV> int lasty = yv12_fb_lst -> y_width + VP8BORDERINPIXELS ; //<S2SV> int lastuv = ( yv12_fb_lst -> y_width >> 1 ) + ( VP8BORDERINPIXELS >> 1 ) ; //<S2SV> for ( i = 0 ; i < 4 ; i ++ ) //<S2SV> { //<S2SV> pbi -> mt_yabove_row [ mb_row + 1 ] [ lasty + i ] = pbi -> mt_yabove_row [ mb_row + 1 ] [ lasty - 1 ] ; //<S2SV> pbi -> mt_uabove_row [ mb_row + 1 ] [ lastuv + i ] = pbi -> mt_uabove_row [ mb_row + 1 ] [ lastuv - 1 ] ; //<S2SV> pbi -> mt_vabove_row [ mb_row + 1 ] [ lastuv + i ] = pbi -> mt_vabove_row [ mb_row + 1 ] [ lastuv - 1 ] ; //<S2SV> } //<S2SV> } //<S2SV> } //<S2SV> else //<S2SV> vp8_extend_mb_row ( yv12_fb_new , xd -> dst . y_buffer + 16 , //<S2SV> xd -> dst . u_buffer + 8 , xd -> dst . v_buffer + 8 ) ; //<S2SV> * current_mb_col = mb_col + nsync ; //<S2SV> ++ xd -> mode_info_context ; //<S2SV> xd -> up_available = 1 ; //<S2SV> xd -> mode_info_context += xd -> mode_info_stride * pbi -> decoding_thread_count ; //<S2SV> } //<S2SV> if ( last_mb_row == ( pc -> mb_rows - 1 ) ) //<S2SV> sem_post ( & pbi -> h_event_end_decoding ) ; //<S2SV> } //<S2SV> 