void vp8_bilinear_predict4x4_neon ( //<S2SV> unsigned char * src_ptr , //<S2SV> int src_pixels_per_line , //<S2SV> int xoffset , //<S2SV> int yoffset , //<S2SV> unsigned char * dst_ptr , //<S2SV> int dst_pitch ) { //<S2SV> uint8x8_t d0u8 , d1u8 , d2u8 , d3u8 , d4u8 , d5u8 , d6u8 ; //<S2SV> uint8x8_t d26u8 , d27u8 , d28u8 , d29u8 , d30u8 ; //<S2SV> uint32x2_t d28u32 , d29u32 , d30u32 ; //<S2SV> uint8x16_t q1u8 , q2u8 ; //<S2SV> uint16x8_t q1u16 , q2u16 ; //<S2SV> uint16x8_t q7u16 , q8u16 , q9u16 ; //<S2SV> uint64x2_t q4u64 , q5u64 ; //<S2SV> uint64x1_t d12u64 ; //<S2SV> uint32x2x2_t d0u32x2 , d1u32x2 , d2u32x2 , d3u32x2 ; //<S2SV> if ( xoffset == 0 ) { //<S2SV> d28u32 = vld1_lane_u32 ( ( const uint32_t * ) src_ptr , d28u32 , 0 ) ; //<S2SV> src_ptr += src_pixels_per_line ; //<S2SV> d28u32 = vld1_lane_u32 ( ( const uint32_t * ) src_ptr , d28u32 , 1 ) ; //<S2SV> src_ptr += src_pixels_per_line ; //<S2SV> d29u32 = vld1_lane_u32 ( ( const uint32_t * ) src_ptr , d29u32 , 0 ) ; //<S2SV> src_ptr += src_pixels_per_line ; //<S2SV> d29u32 = vld1_lane_u32 ( ( const uint32_t * ) src_ptr , d29u32 , 1 ) ; //<S2SV> src_ptr += src_pixels_per_line ; //<S2SV> d30u32 = vld1_lane_u32 ( ( const uint32_t * ) src_ptr , d30u32 , 0 ) ; //<S2SV> d28u8 = vreinterpret_u8_u32 ( d28u32 ) ; //<S2SV> d29u8 = vreinterpret_u8_u32 ( d29u32 ) ; //<S2SV> d30u8 = vreinterpret_u8_u32 ( d30u32 ) ; //<S2SV> } else { //<S2SV> d2u8 = vld1_u8 ( src_ptr ) ; src_ptr += src_pixels_per_line ; //<S2SV> d3u8 = vld1_u8 ( src_ptr ) ; src_ptr += src_pixels_per_line ; //<S2SV> d4u8 = vld1_u8 ( src_ptr ) ; src_ptr += src_pixels_per_line ; //<S2SV> d5u8 = vld1_u8 ( src_ptr ) ; src_ptr += src_pixels_per_line ; //<S2SV> d6u8 = vld1_u8 ( src_ptr ) ; //<S2SV> q1u8 = vcombine_u8 ( d2u8 , d3u8 ) ; //<S2SV> q2u8 = vcombine_u8 ( d4u8 , d5u8 ) ; //<S2SV> d0u8 = vdup_n_u8 ( ( uint8_t ) bifilter4_coeff [ xoffset ] [ 0 ] ) ; //<S2SV> d1u8 = vdup_n_u8 ( ( uint8_t ) bifilter4_coeff [ xoffset ] [ 1 ] ) ; //<S2SV> q4u64 = vshrq_n_u64 ( vreinterpretq_u64_u8 ( q1u8 ) , 8 ) ; //<S2SV> q5u64 = vshrq_n_u64 ( vreinterpretq_u64_u8 ( q2u8 ) , 8 ) ; //<S2SV> d12u64 = vshr_n_u64 ( vreinterpret_u64_u8 ( d6u8 ) , 8 ) ; //<S2SV> d0u32x2 = vzip_u32 ( vreinterpret_u32_u8 ( vget_low_u8 ( q1u8 ) ) , //<S2SV> vreinterpret_u32_u8 ( vget_high_u8 ( q1u8 ) ) ) ; //<S2SV> d1u32x2 = vzip_u32 ( vreinterpret_u32_u8 ( vget_low_u8 ( q2u8 ) ) , //<S2SV> vreinterpret_u32_u8 ( vget_high_u8 ( q2u8 ) ) ) ; //<S2SV> d2u32x2 = vzip_u32 ( vreinterpret_u32_u64 ( vget_low_u64 ( q4u64 ) ) , //<S2SV> vreinterpret_u32_u64 ( vget_high_u64 ( q4u64 ) ) ) ; //<S2SV> d3u32x2 = vzip_u32 ( vreinterpret_u32_u64 ( vget_low_u64 ( q5u64 ) ) , //<S2SV> vreinterpret_u32_u64 ( vget_high_u64 ( q5u64 ) ) ) ; //<S2SV> q7u16 = vmull_u8 ( vreinterpret_u8_u32 ( d0u32x2 . val [ 0 ] ) , d0u8 ) ; //<S2SV> q8u16 = vmull_u8 ( vreinterpret_u8_u32 ( d1u32x2 . val [ 0 ] ) , d0u8 ) ; //<S2SV> q9u16 = vmull_u8 ( d6u8 , d0u8 ) ; //<S2SV> q7u16 = vmlal_u8 ( q7u16 , vreinterpret_u8_u32 ( d2u32x2 . val [ 0 ] ) , d1u8 ) ; //<S2SV> q8u16 = vmlal_u8 ( q8u16 , vreinterpret_u8_u32 ( d3u32x2 . val [ 0 ] ) , d1u8 ) ; //<S2SV> q9u16 = vmlal_u8 ( q9u16 , vreinterpret_u8_u64 ( d12u64 ) , d1u8 ) ; //<S2SV> d28u8 = vqrshrn_n_u16 ( q7u16 , 7 ) ; //<S2SV> d29u8 = vqrshrn_n_u16 ( q8u16 , 7 ) ; //<S2SV> d30u8 = vqrshrn_n_u16 ( q9u16 , 7 ) ; //<S2SV> } //<S2SV> if ( yoffset == 0 ) { //<S2SV> vst1_lane_u32 ( ( uint32_t * ) dst_ptr , vreinterpret_u32_u8 ( d28u8 ) , 0 ) ; //<S2SV> dst_ptr += dst_pitch ; //<S2SV> vst1_lane_u32 ( ( uint32_t * ) dst_ptr , vreinterpret_u32_u8 ( d28u8 ) , 1 ) ; //<S2SV> dst_ptr += dst_pitch ; //<S2SV> vst1_lane_u32 ( ( uint32_t * ) dst_ptr , vreinterpret_u32_u8 ( d29u8 ) , 0 ) ; //<S2SV> dst_ptr += dst_pitch ; //<S2SV> vst1_lane_u32 ( ( uint32_t * ) dst_ptr , vreinterpret_u32_u8 ( d29u8 ) , 1 ) ; //<S2SV> } else { //<S2SV> d0u8 = vdup_n_u8 ( bifilter4_coeff [ yoffset ] [ 0 ] ) ; //<S2SV> d1u8 = vdup_n_u8 ( bifilter4_coeff [ yoffset ] [ 1 ] ) ; //<S2SV> q1u16 = vmull_u8 ( d28u8 , d0u8 ) ; //<S2SV> q2u16 = vmull_u8 ( d29u8 , d0u8 ) ; //<S2SV> d26u8 = vext_u8 ( d28u8 , d29u8 , 4 ) ; //<S2SV> d27u8 = vext_u8 ( d29u8 , d30u8 , 4 ) ; //<S2SV> q1u16 = vmlal_u8 ( q1u16 , d26u8 , d1u8 ) ; //<S2SV> q2u16 = vmlal_u8 ( q2u16 , d27u8 , d1u8 ) ; //<S2SV> d2u8 = vqrshrn_n_u16 ( q1u16 , 7 ) ; //<S2SV> d3u8 = vqrshrn_n_u16 ( q2u16 , 7 ) ; //<S2SV> vst1_lane_u32 ( ( uint32_t * ) dst_ptr , vreinterpret_u32_u8 ( d2u8 ) , 0 ) ; //<S2SV> dst_ptr += dst_pitch ; //<S2SV> vst1_lane_u32 ( ( uint32_t * ) dst_ptr , vreinterpret_u32_u8 ( d2u8 ) , 1 ) ; //<S2SV> dst_ptr += dst_pitch ; //<S2SV> vst1_lane_u32 ( ( uint32_t * ) dst_ptr , vreinterpret_u32_u8 ( d3u8 ) , 0 ) ; //<S2SV> dst_ptr += dst_pitch ; //<S2SV> vst1_lane_u32 ( ( uint32_t * ) dst_ptr , vreinterpret_u32_u8 ( d3u8 ) , 1 ) ; //<S2SV> } //<S2SV> return ; //<S2SV> } //<S2SV> 