static int tcp_v6_rcv ( struct sk_buff * skb ) //<S2SV> { //<S2SV> const struct tcphdr * th ; //<S2SV> const struct ipv6hdr * hdr ; //<S2SV> bool refcounted ; //<S2SV> struct sock * sk ; //<S2SV> int ret ; //<S2SV> struct net * net = dev_net ( skb -> dev ) ; //<S2SV> if ( skb -> pkt_type != PACKET_HOST ) //<S2SV> goto discard_it ; //<S2SV> __TCP_INC_STATS ( net , TCP_MIB_INSEGS ) ; //<S2SV> if ( ! pskb_may_pull ( skb , sizeof ( struct tcphdr ) ) ) //<S2SV> goto discard_it ; //<S2SV> th = ( const struct tcphdr * ) skb -> data ; //<S2SV> if ( unlikely ( th -> doff < sizeof ( struct tcphdr ) / 4 ) ) //<S2SV> goto bad_packet ; //<S2SV> if ( ! pskb_may_pull ( skb , th -> doff * 4 ) ) //<S2SV> goto discard_it ; //<S2SV> if ( skb_checksum_init ( skb , IPPROTO_TCP , ip6_compute_pseudo ) ) //<S2SV> goto csum_error ; //<S2SV> th = ( const struct tcphdr * ) skb -> data ; //<S2SV> hdr = ipv6_hdr ( skb ) ; //<S2SV> lookup : //<S2SV> sk = __inet6_lookup_skb ( & tcp_hashinfo , skb , __tcp_hdrlen ( th ) , //<S2SV> th -> source , th -> dest , inet6_iif ( skb ) , //<S2SV> & refcounted ) ; //<S2SV> if ( ! sk ) //<S2SV> goto no_tcp_socket ; //<S2SV> process : //<S2SV> if ( sk -> sk_state == TCP_TIME_WAIT ) //<S2SV> goto do_time_wait ; //<S2SV> if ( sk -> sk_state == TCP_NEW_SYN_RECV ) { //<S2SV> struct request_sock * req = inet_reqsk ( sk ) ; //<S2SV> struct sock * nsk ; //<S2SV> sk = req -> rsk_listener ; //<S2SV> tcp_v6_fill_cb ( skb , hdr , th ) ; //<S2SV> if ( tcp_v6_inbound_md5_hash ( sk , skb ) ) { //<S2SV> sk_drops_add ( sk , skb ) ; //<S2SV> reqsk_put ( req ) ; //<S2SV> goto discard_it ; //<S2SV> } //<S2SV> if ( unlikely ( sk -> sk_state != TCP_LISTEN ) ) { //<S2SV> inet_csk_reqsk_queue_drop_and_put ( sk , req ) ; //<S2SV> goto lookup ; //<S2SV> } //<S2SV> sock_hold ( sk ) ; //<S2SV> refcounted = true ; //<S2SV> nsk = tcp_check_req ( sk , skb , req , false ) ; //<S2SV> if ( ! nsk ) { //<S2SV> reqsk_put ( req ) ; //<S2SV> goto discard_and_relse ; //<S2SV> } //<S2SV> if ( nsk == sk ) { //<S2SV> reqsk_put ( req ) ; //<S2SV> tcp_v6_restore_cb ( skb ) ; //<S2SV> } else if ( tcp_child_process ( sk , nsk , skb ) ) { //<S2SV> tcp_v6_send_reset ( nsk , skb ) ; //<S2SV> goto discard_and_relse ; //<S2SV> } else { //<S2SV> sock_put ( sk ) ; //<S2SV> return 0 ; //<S2SV> } //<S2SV> } //<S2SV> if ( hdr -> hop_limit < inet6_sk ( sk ) -> min_hopcount ) { //<S2SV> __NET_INC_STATS ( net , LINUX_MIB_TCPMINTTLDROP ) ; //<S2SV> goto discard_and_relse ; //<S2SV> } //<S2SV> if ( ! xfrm6_policy_check ( sk , XFRM_POLICY_IN , skb ) ) //<S2SV> goto discard_and_relse ; //<S2SV> tcp_v6_fill_cb ( skb , hdr , th ) ; //<S2SV> if ( tcp_v6_inbound_md5_hash ( sk , skb ) ) //<S2SV> goto discard_and_relse ; //<S2SV> if ( tcp_filter ( sk , skb ) ) //<S2SV> goto discard_and_relse ; //<S2SV> th = ( const struct tcphdr * ) skb -> data ; //<S2SV> hdr = ipv6_hdr ( skb ) ; //<S2SV> skb -> dev = NULL ; //<S2SV> if ( sk -> sk_state == TCP_LISTEN ) { //<S2SV> ret = tcp_v6_do_rcv ( sk , skb ) ; //<S2SV> goto put_and_return ; //<S2SV> } //<S2SV> sk_incoming_cpu_update ( sk ) ; //<S2SV> bh_lock_sock_nested ( sk ) ; //<S2SV> tcp_segs_in ( tcp_sk ( sk ) , skb ) ; //<S2SV> ret = 0 ; //<S2SV> if ( ! sock_owned_by_user ( sk ) ) { //<S2SV> if ( ! tcp_prequeue ( sk , skb ) ) //<S2SV> ret = tcp_v6_do_rcv ( sk , skb ) ; //<S2SV> } else if ( tcp_add_backlog ( sk , skb ) ) { //<S2SV> goto discard_and_relse ; //<S2SV> } //<S2SV> bh_unlock_sock ( sk ) ; //<S2SV> put_and_return : //<S2SV> if ( refcounted ) //<S2SV> sock_put ( sk ) ; //<S2SV> return ret ? - 1 : 0 ; //<S2SV> no_tcp_socket : //<S2SV> if ( ! xfrm6_policy_check ( NULL , XFRM_POLICY_IN , skb ) ) //<S2SV> goto discard_it ; //<S2SV> tcp_v6_fill_cb ( skb , hdr , th ) ; //<S2SV> if ( tcp_checksum_complete ( skb ) ) { //<S2SV> csum_error : //<S2SV> __TCP_INC_STATS ( net , TCP_MIB_CSUMERRORS ) ; //<S2SV> bad_packet : //<S2SV> __TCP_INC_STATS ( net , TCP_MIB_INERRS ) ; //<S2SV> } else { //<S2SV> tcp_v6_send_reset ( NULL , skb ) ; //<S2SV> } //<S2SV> discard_it : //<S2SV> kfree_skb ( skb ) ; //<S2SV> return 0 ; //<S2SV> discard_and_relse : //<S2SV> sk_drops_add ( sk , skb ) ; //<S2SV> if ( refcounted ) //<S2SV> sock_put ( sk ) ; //<S2SV> goto discard_it ; //<S2SV> do_time_wait : //<S2SV> if ( ! xfrm6_policy_check ( NULL , XFRM_POLICY_IN , skb ) ) { //<S2SV> inet_twsk_put ( inet_twsk ( sk ) ) ; //<S2SV> goto discard_it ; //<S2SV> } //<S2SV> tcp_v6_fill_cb ( skb , hdr , th ) ; //<S2SV> if ( tcp_checksum_complete ( skb ) ) { //<S2SV> inet_twsk_put ( inet_twsk ( sk ) ) ; //<S2SV> goto csum_error ; //<S2SV> } //<S2SV> switch ( tcp_timewait_state_process ( inet_twsk ( sk ) , skb , th ) ) { //<S2SV> case TCP_TW_SYN : //<S2SV> { //<S2SV> struct sock * sk2 ; //<S2SV> sk2 = inet6_lookup_listener ( dev_net ( skb -> dev ) , & tcp_hashinfo , //<S2SV> skb , __tcp_hdrlen ( th ) , //<S2SV> & ipv6_hdr ( skb ) -> saddr , th -> source , //<S2SV> & ipv6_hdr ( skb ) -> daddr , //<S2SV> ntohs ( th -> dest ) , tcp_v6_iif ( skb ) ) ; //<S2SV> if ( sk2 ) { //<S2SV> struct inet_timewait_sock * tw = inet_twsk ( sk ) ; //<S2SV> inet_twsk_deschedule_put ( tw ) ; //<S2SV> sk = sk2 ; //<S2SV> tcp_v6_restore_cb ( skb ) ; //<S2SV> refcounted = false ; //<S2SV> goto process ; //<S2SV> } //<S2SV> } //<S2SV> case TCP_TW_ACK : //<S2SV> tcp_v6_timewait_ack ( sk , skb ) ; //<S2SV> break ; //<S2SV> case TCP_TW_RST : //<S2SV> tcp_v6_restore_cb ( skb ) ; //<S2SV> tcp_v6_send_reset ( sk , skb ) ; //<S2SV> inet_twsk_deschedule_put ( inet_twsk ( sk ) ) ; //<S2SV> goto discard_it ; //<S2SV> case TCP_TW_SUCCESS : //<S2SV> ; //<S2SV> } //<S2SV> goto discard_it ; //<S2SV> } //<S2SV> 