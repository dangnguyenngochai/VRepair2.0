int tcp_v4_rcv ( struct sk_buff * skb ) //<S2SV> { //<S2SV> struct net * net = dev_net ( skb -> dev ) ; //<S2SV> const struct iphdr * iph ; //<S2SV> const struct tcphdr * th ; //<S2SV> bool refcounted ; //<S2SV> struct sock * sk ; //<S2SV> int ret ; //<S2SV> if ( skb -> pkt_type != PACKET_HOST ) //<S2SV> goto discard_it ; //<S2SV> __TCP_INC_STATS ( net , TCP_MIB_INSEGS ) ; //<S2SV> if ( ! pskb_may_pull ( skb , sizeof ( struct tcphdr ) ) ) //<S2SV> goto discard_it ; //<S2SV> th = ( const struct tcphdr * ) skb -> data ; //<S2SV> if ( unlikely ( th -> doff < sizeof ( struct tcphdr ) / 4 ) ) //<S2SV> goto bad_packet ; //<S2SV> if ( ! pskb_may_pull ( skb , th -> doff * 4 ) ) //<S2SV> goto discard_it ; //<S2SV> if ( skb_checksum_init ( skb , IPPROTO_TCP , inet_compute_pseudo ) ) //<S2SV> goto csum_error ; //<S2SV> th = ( const struct tcphdr * ) skb -> data ; //<S2SV> iph = ip_hdr ( skb ) ; //<S2SV> memmove ( & TCP_SKB_CB ( skb ) -> header . h4 , IPCB ( skb ) , //<S2SV> sizeof ( struct inet_skb_parm ) ) ; //<S2SV> barrier ( ) ; //<S2SV> TCP_SKB_CB ( skb ) -> seq = ntohl ( th -> seq ) ; //<S2SV> TCP_SKB_CB ( skb ) -> end_seq = ( TCP_SKB_CB ( skb ) -> seq + th -> syn + th -> fin + //<S2SV> skb -> len - th -> doff * 4 ) ; //<S2SV> TCP_SKB_CB ( skb ) -> ack_seq = ntohl ( th -> ack_seq ) ; //<S2SV> TCP_SKB_CB ( skb ) -> tcp_flags = tcp_flag_byte ( th ) ; //<S2SV> TCP_SKB_CB ( skb ) -> tcp_tw_isn = 0 ; //<S2SV> TCP_SKB_CB ( skb ) -> ip_dsfield = ipv4_get_dsfield ( iph ) ; //<S2SV> TCP_SKB_CB ( skb ) -> sacked = 0 ; //<S2SV> lookup : //<S2SV> sk = __inet_lookup_skb ( & tcp_hashinfo , skb , __tcp_hdrlen ( th ) , th -> source , //<S2SV> th -> dest , & refcounted ) ; //<S2SV> if ( ! sk ) //<S2SV> goto no_tcp_socket ; //<S2SV> process : //<S2SV> if ( sk -> sk_state == TCP_TIME_WAIT ) //<S2SV> goto do_time_wait ; //<S2SV> if ( sk -> sk_state == TCP_NEW_SYN_RECV ) { //<S2SV> struct request_sock * req = inet_reqsk ( sk ) ; //<S2SV> struct sock * nsk ; //<S2SV> sk = req -> rsk_listener ; //<S2SV> if ( unlikely ( tcp_v4_inbound_md5_hash ( sk , skb ) ) ) { //<S2SV> sk_drops_add ( sk , skb ) ; //<S2SV> reqsk_put ( req ) ; //<S2SV> goto discard_it ; //<S2SV> } //<S2SV> if ( unlikely ( sk -> sk_state != TCP_LISTEN ) ) { //<S2SV> inet_csk_reqsk_queue_drop_and_put ( sk , req ) ; //<S2SV> goto lookup ; //<S2SV> } //<S2SV> sock_hold ( sk ) ; //<S2SV> refcounted = true ; //<S2SV> nsk = tcp_check_req ( sk , skb , req , false ) ; //<S2SV> if ( ! nsk ) { //<S2SV> reqsk_put ( req ) ; //<S2SV> goto discard_and_relse ; //<S2SV> } //<S2SV> if ( nsk == sk ) { //<S2SV> reqsk_put ( req ) ; //<S2SV> } else if ( tcp_child_process ( sk , nsk , skb ) ) { //<S2SV> tcp_v4_send_reset ( nsk , skb ) ; //<S2SV> goto discard_and_relse ; //<S2SV> } else { //<S2SV> sock_put ( sk ) ; //<S2SV> return 0 ; //<S2SV> } //<S2SV> } //<S2SV> if ( unlikely ( iph -> ttl < inet_sk ( sk ) -> min_ttl ) ) { //<S2SV> __NET_INC_STATS ( net , LINUX_MIB_TCPMINTTLDROP ) ; //<S2SV> goto discard_and_relse ; //<S2SV> } //<S2SV> if ( ! xfrm4_policy_check ( sk , XFRM_POLICY_IN , skb ) ) //<S2SV> goto discard_and_relse ; //<S2SV> if ( tcp_v4_inbound_md5_hash ( sk , skb ) ) //<S2SV> goto discard_and_relse ; //<S2SV> nf_reset ( skb ) ; //<S2SV> if ( sk_filter ( sk , skb ) ) //<S2SV> goto discard_and_relse ; //<S2SV> skb -> dev = NULL ; //<S2SV> if ( sk -> sk_state == TCP_LISTEN ) { //<S2SV> ret = tcp_v4_do_rcv ( sk , skb ) ; //<S2SV> goto put_and_return ; //<S2SV> } //<S2SV> sk_incoming_cpu_update ( sk ) ; //<S2SV> bh_lock_sock_nested ( sk ) ; //<S2SV> tcp_segs_in ( tcp_sk ( sk ) , skb ) ; //<S2SV> ret = 0 ; //<S2SV> if ( ! sock_owned_by_user ( sk ) ) { //<S2SV> if ( ! tcp_prequeue ( sk , skb ) ) //<S2SV> ret = tcp_v4_do_rcv ( sk , skb ) ; //<S2SV> } else if ( tcp_add_backlog ( sk , skb ) ) { //<S2SV> goto discard_and_relse ; //<S2SV> } //<S2SV> bh_unlock_sock ( sk ) ; //<S2SV> put_and_return : //<S2SV> if ( refcounted ) //<S2SV> sock_put ( sk ) ; //<S2SV> return ret ; //<S2SV> no_tcp_socket : //<S2SV> if ( ! xfrm4_policy_check ( NULL , XFRM_POLICY_IN , skb ) ) //<S2SV> goto discard_it ; //<S2SV> if ( tcp_checksum_complete ( skb ) ) { //<S2SV> csum_error : //<S2SV> __TCP_INC_STATS ( net , TCP_MIB_CSUMERRORS ) ; //<S2SV> bad_packet : //<S2SV> __TCP_INC_STATS ( net , TCP_MIB_INERRS ) ; //<S2SV> } else { //<S2SV> tcp_v4_send_reset ( NULL , skb ) ; //<S2SV> } //<S2SV> discard_it : //<S2SV> kfree_skb ( skb ) ; //<S2SV> return 0 ; //<S2SV> discard_and_relse : //<S2SV> sk_drops_add ( sk , skb ) ; //<S2SV> if ( refcounted ) //<S2SV> sock_put ( sk ) ; //<S2SV> goto discard_it ; //<S2SV> do_time_wait : //<S2SV> if ( ! xfrm4_policy_check ( NULL , XFRM_POLICY_IN , skb ) ) { //<S2SV> inet_twsk_put ( inet_twsk ( sk ) ) ; //<S2SV> goto discard_it ; //<S2SV> } //<S2SV> if ( tcp_checksum_complete ( skb ) ) { //<S2SV> inet_twsk_put ( inet_twsk ( sk ) ) ; //<S2SV> goto csum_error ; //<S2SV> } //<S2SV> switch ( tcp_timewait_state_process ( inet_twsk ( sk ) , skb , th ) ) { //<S2SV> case TCP_TW_SYN : { //<S2SV> struct sock * sk2 = inet_lookup_listener ( dev_net ( skb -> dev ) , //<S2SV> & tcp_hashinfo , skb , //<S2SV> __tcp_hdrlen ( th ) , //<S2SV> iph -> saddr , th -> source , //<S2SV> iph -> daddr , th -> dest , //<S2SV> inet_iif ( skb ) ) ; //<S2SV> if ( sk2 ) { //<S2SV> inet_twsk_deschedule_put ( inet_twsk ( sk ) ) ; //<S2SV> sk = sk2 ; //<S2SV> refcounted = false ; //<S2SV> goto process ; //<S2SV> } //<S2SV> } //<S2SV> case TCP_TW_ACK : //<S2SV> tcp_v4_timewait_ack ( sk , skb ) ; //<S2SV> break ; //<S2SV> case TCP_TW_RST : //<S2SV> tcp_v4_send_reset ( sk , skb ) ; //<S2SV> inet_twsk_deschedule_put ( inet_twsk ( sk ) ) ; //<S2SV> goto discard_it ; //<S2SV> case TCP_TW_SUCCESS : ; //<S2SV> } //<S2SV> goto discard_it ; //<S2SV> } //<S2SV> 