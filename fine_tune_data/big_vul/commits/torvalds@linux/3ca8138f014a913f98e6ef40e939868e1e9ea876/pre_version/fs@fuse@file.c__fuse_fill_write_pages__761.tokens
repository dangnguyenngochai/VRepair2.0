static ssize_t fuse_fill_write_pages ( struct fuse_req * req , //<S2SV> struct address_space * mapping , //<S2SV> struct iov_iter * ii , loff_t pos ) //<S2SV> { //<S2SV> struct fuse_conn * fc = get_fuse_conn ( mapping -> host ) ; //<S2SV> unsigned offset = pos & ( PAGE_CACHE_SIZE - 1 ) ; //<S2SV> size_t count = 0 ; //<S2SV> int err ; //<S2SV> req -> in . argpages = 1 ; //<S2SV> req -> page_descs [ 0 ] . offset = offset ; //<S2SV> do { //<S2SV> size_t tmp ; //<S2SV> struct page * page ; //<S2SV> pgoff_t index = pos >> PAGE_CACHE_SHIFT ; //<S2SV> size_t bytes = min_t ( size_t , PAGE_CACHE_SIZE - offset , //<S2SV> iov_iter_count ( ii ) ) ; //<S2SV> bytes = min_t ( size_t , bytes , fc -> max_write - count ) ; //<S2SV> again : //<S2SV> err = - EFAULT ; //<S2SV> if ( iov_iter_fault_in_readable ( ii , bytes ) ) //<S2SV> break ; //<S2SV> err = - ENOMEM ; //<S2SV> page = grab_cache_page_write_begin ( mapping , index , 0 ) ; //<S2SV> if ( ! page ) //<S2SV> break ; //<S2SV> if ( mapping_writably_mapped ( mapping ) ) //<S2SV> flush_dcache_page ( page ) ; //<S2SV> tmp = iov_iter_copy_from_user_atomic ( page , ii , offset , bytes ) ; //<S2SV> flush_dcache_page ( page ) ; //<S2SV> if ( ! tmp ) { //<S2SV> unlock_page ( page ) ; //<S2SV> page_cache_release ( page ) ; //<S2SV> bytes = min ( bytes , iov_iter_single_seg_count ( ii ) ) ; //<S2SV> goto again ; //<S2SV> } //<S2SV> err = 0 ; //<S2SV> req -> pages [ req -> num_pages ] = page ; //<S2SV> req -> page_descs [ req -> num_pages ] . length = tmp ; //<S2SV> req -> num_pages ++ ; //<S2SV> iov_iter_advance ( ii , tmp ) ; //<S2SV> count += tmp ; //<S2SV> pos += tmp ; //<S2SV> offset += tmp ; //<S2SV> if ( offset == PAGE_CACHE_SIZE ) //<S2SV> offset = 0 ; //<S2SV> if ( ! fc -> big_writes ) //<S2SV> break ; //<S2SV> } while ( iov_iter_count ( ii ) && count < fc -> max_write && //<S2SV> req -> num_pages < req -> max_pages && offset == 0 ) ; //<S2SV> return count > 0 ? count : err ; //<S2SV> } //<S2SV> 