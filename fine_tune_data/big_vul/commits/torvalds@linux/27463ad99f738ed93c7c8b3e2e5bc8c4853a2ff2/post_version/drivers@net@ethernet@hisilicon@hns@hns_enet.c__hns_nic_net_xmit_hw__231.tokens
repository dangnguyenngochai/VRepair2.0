netdev_tx_t hns_nic_net_xmit_hw ( struct net_device * ndev , //<S2SV> struct sk_buff * skb , //<S2SV> struct hns_nic_ring_data * ring_data ) //<S2SV> { //<S2SV> struct hns_nic_priv * priv = netdev_priv ( ndev ) ; //<S2SV> struct hnae_ring * ring = ring_data -> ring ; //<S2SV> struct device * dev = ring_to_dev ( ring ) ; //<S2SV> struct netdev_queue * dev_queue ; //<S2SV> struct skb_frag_struct * frag ; //<S2SV> int buf_num ; //<S2SV> int seg_num ; //<S2SV> dma_addr_t dma ; //<S2SV> int size , next_to_use ; //<S2SV> int i ; //<S2SV> switch ( priv -> ops . maybe_stop_tx ( & skb , & buf_num , ring ) ) { //<S2SV> case - EBUSY : //<S2SV> ring -> stats . tx_busy ++ ; //<S2SV> goto out_net_tx_busy ; //<S2SV> case - ENOMEM : //<S2SV> ring -> stats . sw_err_cnt ++ ; //<S2SV> netdev_err ( ndev , "no<S2SV_blank>memory<S2SV_blank>to<S2SV_blank>xmit!\\n" ) ; //<S2SV> goto out_err_tx_ok ; //<S2SV> default : //<S2SV> break ; //<S2SV> } //<S2SV> seg_num = skb_shinfo ( skb ) -> nr_frags + 1 ; //<S2SV> next_to_use = ring -> next_to_use ; //<S2SV> size = skb_headlen ( skb ) ; //<S2SV> dma = dma_map_single ( dev , skb -> data , size , DMA_TO_DEVICE ) ; //<S2SV> if ( dma_mapping_error ( dev , dma ) ) { //<S2SV> netdev_err ( ndev , "TX<S2SV_blank>head<S2SV_blank>DMA<S2SV_blank>map<S2SV_blank>failed\\n" ) ; //<S2SV> ring -> stats . sw_err_cnt ++ ; //<S2SV> goto out_err_tx_ok ; //<S2SV> } //<S2SV> priv -> ops . fill_desc ( ring , skb , size , dma , seg_num == 1 ? 1 : 0 , //<S2SV> buf_num , DESC_TYPE_SKB , ndev -> mtu ) ; //<S2SV> for ( i = 1 ; i < seg_num ; i ++ ) { //<S2SV> frag = & skb_shinfo ( skb ) -> frags [ i - 1 ] ; //<S2SV> size = skb_frag_size ( frag ) ; //<S2SV> dma = skb_frag_dma_map ( dev , frag , 0 , size , DMA_TO_DEVICE ) ; //<S2SV> if ( dma_mapping_error ( dev , dma ) ) { //<S2SV> netdev_err ( ndev , "TX<S2SV_blank>frag(%d)<S2SV_blank>DMA<S2SV_blank>map<S2SV_blank>failed\\n" , i ) ; //<S2SV> ring -> stats . sw_err_cnt ++ ; //<S2SV> goto out_map_frag_fail ; //<S2SV> } //<S2SV> priv -> ops . fill_desc ( ring , skb_frag_page ( frag ) , size , dma , //<S2SV> seg_num - 1 == i ? 1 : 0 , buf_num , //<S2SV> DESC_TYPE_PAGE , ndev -> mtu ) ; //<S2SV> } //<S2SV> dev_queue = netdev_get_tx_queue ( ndev , skb -> queue_mapping ) ; //<S2SV> netdev_tx_sent_queue ( dev_queue , skb -> len ) ; //<S2SV> netif_trans_update ( ndev ) ; //<S2SV> ndev -> stats . tx_bytes += skb -> len ; //<S2SV> ndev -> stats . tx_packets ++ ; //<S2SV> wmb ( ) ; //<S2SV> assert ( skb -> queue_mapping < priv -> ae_handle -> q_num ) ; //<S2SV> hnae_queue_xmit ( priv -> ae_handle -> qs [ skb -> queue_mapping ] , buf_num ) ; //<S2SV> ring -> stats . tx_pkts ++ ; //<S2SV> ring -> stats . tx_bytes += skb -> len ; //<S2SV> return NETDEV_TX_OK ; //<S2SV> out_map_frag_fail : //<S2SV> while ( ring -> next_to_use != next_to_use ) { //<S2SV> unfill_desc ( ring ) ; //<S2SV> if ( ring -> next_to_use != next_to_use ) //<S2SV> dma_unmap_page ( dev , //<S2SV> ring -> desc_cb [ ring -> next_to_use ] . dma , //<S2SV> ring -> desc_cb [ ring -> next_to_use ] . length , //<S2SV> DMA_TO_DEVICE ) ; //<S2SV> else //<S2SV> dma_unmap_single ( dev , //<S2SV> ring -> desc_cb [ next_to_use ] . dma , //<S2SV> ring -> desc_cb [ next_to_use ] . length , //<S2SV> DMA_TO_DEVICE ) ; //<S2SV> } //<S2SV> out_err_tx_ok : //<S2SV> dev_kfree_skb_any ( skb ) ; //<S2SV> return NETDEV_TX_OK ; //<S2SV> out_net_tx_busy : //<S2SV> netif_stop_subqueue ( ndev , skb -> queue_mapping ) ; //<S2SV> smp_mb ( ) ; //<S2SV> return NETDEV_TX_BUSY ; //<S2SV> } //<S2SV> 