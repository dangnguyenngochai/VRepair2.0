static unsigned xen_netbk_tx_build_gops ( struct xen_netbk * netbk ) //<S2SV> { //<S2SV> struct gnttab_copy * gop = netbk -> tx_copy_ops , * request_gop ; //<S2SV> struct sk_buff * skb ; //<S2SV> int ret ; //<S2SV> while ( ( ( nr_pending_reqs ( netbk ) + MAX_SKB_FRAGS ) < MAX_PENDING_REQS ) && //<S2SV> ! list_empty ( & netbk -> net_schedule_list ) ) { //<S2SV> struct xenvif * vif ; //<S2SV> struct xen_netif_tx_request txreq ; //<S2SV> struct xen_netif_tx_request txfrags [ MAX_SKB_FRAGS ] ; //<S2SV> struct page * page ; //<S2SV> struct xen_netif_extra_info extras [ XEN_NETIF_EXTRA_TYPE_MAX - 1 ] ; //<S2SV> u16 pending_idx ; //<S2SV> RING_IDX idx ; //<S2SV> int work_to_do ; //<S2SV> unsigned int data_len ; //<S2SV> pending_ring_idx_t index ; //<S2SV> vif = poll_net_schedule_list ( netbk ) ; //<S2SV> if ( ! vif ) //<S2SV> continue ; //<S2SV> RING_FINAL_CHECK_FOR_REQUESTS ( & vif -> tx , work_to_do ) ; //<S2SV> if ( ! work_to_do ) { //<S2SV> xenvif_put ( vif ) ; //<S2SV> continue ; //<S2SV> } //<S2SV> idx = vif -> tx . req_cons ; //<S2SV> rmb ( ) ; //<S2SV> memcpy ( & txreq , RING_GET_REQUEST ( & vif -> tx , idx ) , sizeof ( txreq ) ) ; //<S2SV> if ( txreq . size > vif -> remaining_credit && //<S2SV> tx_credit_exceeded ( vif , txreq . size ) ) { //<S2SV> xenvif_put ( vif ) ; //<S2SV> continue ; //<S2SV> } //<S2SV> vif -> remaining_credit -= txreq . size ; //<S2SV> work_to_do -- ; //<S2SV> vif -> tx . req_cons = ++ idx ; //<S2SV> memset ( extras , 0 , sizeof ( extras ) ) ; //<S2SV> if ( txreq . flags & XEN_NETTXF_extra_info ) { //<S2SV> work_to_do = xen_netbk_get_extras ( vif , extras , //<S2SV> work_to_do ) ; //<S2SV> idx = vif -> tx . req_cons ; //<S2SV> if ( unlikely ( work_to_do < 0 ) ) { //<S2SV> netbk_tx_err ( vif , & txreq , idx ) ; //<S2SV> continue ; //<S2SV> } //<S2SV> } //<S2SV> ret = netbk_count_requests ( vif , & txreq , txfrags , work_to_do ) ; //<S2SV> if ( unlikely ( ret < 0 ) ) { //<S2SV> netbk_tx_err ( vif , & txreq , idx - ret ) ; //<S2SV> continue ; //<S2SV> } //<S2SV> idx += ret ; //<S2SV> if ( unlikely ( txreq . size < ETH_HLEN ) ) { //<S2SV> netdev_dbg ( vif -> dev , //<S2SV> "Bad<S2SV_blank>packet<S2SV_blank>size:<S2SV_blank>%d\\n" , txreq . size ) ; //<S2SV> netbk_tx_err ( vif , & txreq , idx ) ; //<S2SV> continue ; //<S2SV> } //<S2SV> if ( unlikely ( ( txreq . offset + txreq . size ) > PAGE_SIZE ) ) { //<S2SV> netdev_dbg ( vif -> dev , //<S2SV> "txreq.offset:<S2SV_blank>%x,<S2SV_blank>size:<S2SV_blank>%u,<S2SV_blank>end:<S2SV_blank>%lu\\n" , //<S2SV> txreq . offset , txreq . size , //<S2SV> ( txreq . offset & ~ PAGE_MASK ) + txreq . size ) ; //<S2SV> netbk_tx_err ( vif , & txreq , idx ) ; //<S2SV> continue ; //<S2SV> } //<S2SV> index = pending_index ( netbk -> pending_cons ) ; //<S2SV> pending_idx = netbk -> pending_ring [ index ] ; //<S2SV> data_len = ( txreq . size > PKT_PROT_LEN && //<S2SV> ret < MAX_SKB_FRAGS ) ? //<S2SV> PKT_PROT_LEN : txreq . size ; //<S2SV> skb = alloc_skb ( data_len + NET_SKB_PAD + NET_IP_ALIGN , //<S2SV> GFP_ATOMIC | __GFP_NOWARN ) ; //<S2SV> if ( unlikely ( skb == NULL ) ) { //<S2SV> netdev_dbg ( vif -> dev , //<S2SV> "Can\'t<S2SV_blank>allocate<S2SV_blank>a<S2SV_blank>skb<S2SV_blank>in<S2SV_blank>start_xmit.\\n" ) ; //<S2SV> netbk_tx_err ( vif , & txreq , idx ) ; //<S2SV> break ; //<S2SV> } //<S2SV> skb_reserve ( skb , NET_SKB_PAD + NET_IP_ALIGN ) ; //<S2SV> if ( extras [ XEN_NETIF_EXTRA_TYPE_GSO - 1 ] . type ) { //<S2SV> struct xen_netif_extra_info * gso ; //<S2SV> gso = & extras [ XEN_NETIF_EXTRA_TYPE_GSO - 1 ] ; //<S2SV> if ( netbk_set_skb_gso ( vif , skb , gso ) ) { //<S2SV> kfree_skb ( skb ) ; //<S2SV> netbk_tx_err ( vif , & txreq , idx ) ; //<S2SV> continue ; //<S2SV> } //<S2SV> } //<S2SV> page = xen_netbk_alloc_page ( netbk , skb , pending_idx ) ; //<S2SV> if ( ! page ) { //<S2SV> kfree_skb ( skb ) ; //<S2SV> netbk_tx_err ( vif , & txreq , idx ) ; //<S2SV> continue ; //<S2SV> } //<S2SV> gop -> source . u . ref = txreq . gref ; //<S2SV> gop -> source . domid = vif -> domid ; //<S2SV> gop -> source . offset = txreq . offset ; //<S2SV> gop -> dest . u . gmfn = virt_to_mfn ( page_address ( page ) ) ; //<S2SV> gop -> dest . domid = DOMID_SELF ; //<S2SV> gop -> dest . offset = txreq . offset ; //<S2SV> gop -> len = txreq . size ; //<S2SV> gop -> flags = GNTCOPY_source_gref ; //<S2SV> gop ++ ; //<S2SV> memcpy ( & netbk -> pending_tx_info [ pending_idx ] . req , //<S2SV> & txreq , sizeof ( txreq ) ) ; //<S2SV> netbk -> pending_tx_info [ pending_idx ] . vif = vif ; //<S2SV> * ( ( u16 * ) skb -> data ) = pending_idx ; //<S2SV> __skb_put ( skb , data_len ) ; //<S2SV> skb_shinfo ( skb ) -> nr_frags = ret ; //<S2SV> if ( data_len < txreq . size ) { //<S2SV> skb_shinfo ( skb ) -> nr_frags ++ ; //<S2SV> frag_set_pending_idx ( & skb_shinfo ( skb ) -> frags [ 0 ] , //<S2SV> pending_idx ) ; //<S2SV> } else { //<S2SV> frag_set_pending_idx ( & skb_shinfo ( skb ) -> frags [ 0 ] , //<S2SV> INVALID_PENDING_IDX ) ; //<S2SV> } //<S2SV> netbk -> pending_cons ++ ; //<S2SV> request_gop = xen_netbk_get_requests ( netbk , vif , //<S2SV> skb , txfrags , gop ) ; //<S2SV> if ( request_gop == NULL ) { //<S2SV> kfree_skb ( skb ) ; //<S2SV> netbk_tx_err ( vif , & txreq , idx ) ; //<S2SV> continue ; //<S2SV> } //<S2SV> gop = request_gop ; //<S2SV> __skb_queue_tail ( & netbk -> tx_queue , skb ) ; //<S2SV> vif -> tx . req_cons = idx ; //<S2SV> xen_netbk_check_rx_xenvif ( vif ) ; //<S2SV> if ( ( gop - netbk -> tx_copy_ops ) >= ARRAY_SIZE ( netbk -> tx_copy_ops ) ) //<S2SV> break ; //<S2SV> } //<S2SV> return gop - netbk -> tx_copy_ops ; //<S2SV> } //<S2SV> 