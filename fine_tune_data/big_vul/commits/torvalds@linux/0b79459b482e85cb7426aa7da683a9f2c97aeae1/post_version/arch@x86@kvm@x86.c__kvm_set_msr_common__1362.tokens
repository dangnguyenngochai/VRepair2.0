int kvm_set_msr_common ( struct kvm_vcpu * vcpu , struct msr_data * msr_info ) //<S2SV> { //<S2SV> bool pr = false ; //<S2SV> u32 msr = msr_info -> index ; //<S2SV> u64 data = msr_info -> data ; //<S2SV> switch ( msr ) { //<S2SV> case MSR_AMD64_NB_CFG : //<S2SV> case MSR_IA32_UCODE_REV : //<S2SV> case MSR_IA32_UCODE_WRITE : //<S2SV> case MSR_VM_HSAVE_PA : //<S2SV> case MSR_AMD64_PATCH_LOADER : //<S2SV> case MSR_AMD64_BU_CFG2 : //<S2SV> break ; //<S2SV> case MSR_EFER : //<S2SV> return set_efer ( vcpu , data ) ; //<S2SV> case MSR_K7_HWCR : //<S2SV> data &= ~ ( u64 ) 0x40 ; //<S2SV> data &= ~ ( u64 ) 0x100 ; //<S2SV> data &= ~ ( u64 ) 0x8 ; //<S2SV> if ( data != 0 ) { //<S2SV> vcpu_unimpl ( vcpu , "unimplemented<S2SV_blank>HWCR<S2SV_blank>wrmsr:<S2SV_blank>0x%llx\\n" , //<S2SV> data ) ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> break ; //<S2SV> case MSR_FAM10H_MMIO_CONF_BASE : //<S2SV> if ( data != 0 ) { //<S2SV> vcpu_unimpl ( vcpu , "unimplemented<S2SV_blank>MMIO_CONF_BASE<S2SV_blank>wrmsr:<S2SV_blank>" //<S2SV> "0x%llx\\n" , data ) ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> break ; //<S2SV> case MSR_IA32_DEBUGCTLMSR : //<S2SV> if ( ! data ) { //<S2SV> break ; //<S2SV> } else if ( data & ~ ( DEBUGCTLMSR_LBR | DEBUGCTLMSR_BTF ) ) { //<S2SV> return 1 ; //<S2SV> } //<S2SV> vcpu_unimpl ( vcpu , "%s:<S2SV_blank>MSR_IA32_DEBUGCTLMSR<S2SV_blank>0x%llx,<S2SV_blank>nop\\n" , //<S2SV> __func__ , data ) ; //<S2SV> break ; //<S2SV> case 0x200 ... 0x2ff : //<S2SV> return set_msr_mtrr ( vcpu , msr , data ) ; //<S2SV> case MSR_IA32_APICBASE : //<S2SV> kvm_set_apic_base ( vcpu , data ) ; //<S2SV> break ; //<S2SV> case APIC_BASE_MSR ... APIC_BASE_MSR + 0x3ff : //<S2SV> return kvm_x2apic_msr_write ( vcpu , msr , data ) ; //<S2SV> case MSR_IA32_TSCDEADLINE : //<S2SV> kvm_set_lapic_tscdeadline_msr ( vcpu , data ) ; //<S2SV> break ; //<S2SV> case MSR_IA32_TSC_ADJUST : //<S2SV> if ( guest_cpuid_has_tsc_adjust ( vcpu ) ) { //<S2SV> if ( ! msr_info -> host_initiated ) { //<S2SV> u64 adj = data - vcpu -> arch . ia32_tsc_adjust_msr ; //<S2SV> kvm_x86_ops -> adjust_tsc_offset ( vcpu , adj , true ) ; //<S2SV> } //<S2SV> vcpu -> arch . ia32_tsc_adjust_msr = data ; //<S2SV> } //<S2SV> break ; //<S2SV> case MSR_IA32_MISC_ENABLE : //<S2SV> vcpu -> arch . ia32_misc_enable_msr = data ; //<S2SV> break ; //<S2SV> case MSR_KVM_WALL_CLOCK_NEW : //<S2SV> case MSR_KVM_WALL_CLOCK : //<S2SV> vcpu -> kvm -> arch . wall_clock = data ; //<S2SV> kvm_write_wall_clock ( vcpu -> kvm , data ) ; //<S2SV> break ; //<S2SV> case MSR_KVM_SYSTEM_TIME_NEW : //<S2SV> case MSR_KVM_SYSTEM_TIME : { //<S2SV> u64 gpa_offset ; //<S2SV> kvmclock_reset ( vcpu ) ; //<S2SV> vcpu -> arch . time = data ; //<S2SV> kvm_make_request ( KVM_REQ_CLOCK_UPDATE , vcpu ) ; //<S2SV> if ( ! ( data & 1 ) ) //<S2SV> break ; //<S2SV> gpa_offset = data & ~ ( PAGE_MASK | 1 ) ; //<S2SV> if ( gpa_offset & ( sizeof ( struct pvclock_vcpu_time_info ) - 1 ) ) //<S2SV> break ; //<S2SV> if ( kvm_gfn_to_hva_cache_init ( vcpu -> kvm , //<S2SV> & vcpu -> arch . pv_time , data & ~ 1ULL ) ) //<S2SV> vcpu -> arch . pv_time_enabled = false ; //<S2SV> else //<S2SV> vcpu -> arch . pv_time_enabled = true ; //<S2SV> break ; //<S2SV> } //<S2SV> case MSR_KVM_ASYNC_PF_EN : //<S2SV> if ( kvm_pv_enable_async_pf ( vcpu , data ) ) //<S2SV> return 1 ; //<S2SV> break ; //<S2SV> case MSR_KVM_STEAL_TIME : //<S2SV> if ( unlikely ( ! sched_info_on ( ) ) ) //<S2SV> return 1 ; //<S2SV> if ( data & KVM_STEAL_RESERVED_MASK ) //<S2SV> return 1 ; //<S2SV> if ( kvm_gfn_to_hva_cache_init ( vcpu -> kvm , & vcpu -> arch . st . stime , //<S2SV> data & KVM_STEAL_VALID_BITS ) ) //<S2SV> return 1 ; //<S2SV> vcpu -> arch . st . msr_val = data ; //<S2SV> if ( ! ( data & KVM_MSR_ENABLED ) ) //<S2SV> break ; //<S2SV> vcpu -> arch . st . last_steal = current -> sched_info . run_delay ; //<S2SV> preempt_disable ( ) ; //<S2SV> accumulate_steal_time ( vcpu ) ; //<S2SV> preempt_enable ( ) ; //<S2SV> kvm_make_request ( KVM_REQ_STEAL_UPDATE , vcpu ) ; //<S2SV> break ; //<S2SV> case MSR_KVM_PV_EOI_EN : //<S2SV> if ( kvm_lapic_enable_pv_eoi ( vcpu , data ) ) //<S2SV> return 1 ; //<S2SV> break ; //<S2SV> case MSR_IA32_MCG_CTL : //<S2SV> case MSR_IA32_MCG_STATUS : //<S2SV> case MSR_IA32_MC0_CTL ... MSR_IA32_MC0_CTL + 4 * KVM_MAX_MCE_BANKS - 1 : //<S2SV> return set_msr_mce ( vcpu , msr , data ) ; //<S2SV> case MSR_K7_EVNTSEL0 : //<S2SV> case MSR_K7_EVNTSEL1 : //<S2SV> case MSR_K7_EVNTSEL2 : //<S2SV> case MSR_K7_EVNTSEL3 : //<S2SV> if ( data != 0 ) //<S2SV> vcpu_unimpl ( vcpu , "unimplemented<S2SV_blank>perfctr<S2SV_blank>wrmsr:<S2SV_blank>" //<S2SV> "0x%x<S2SV_blank>data<S2SV_blank>0x%llx\\n" , msr , data ) ; //<S2SV> break ; //<S2SV> case MSR_K7_PERFCTR0 : //<S2SV> case MSR_K7_PERFCTR1 : //<S2SV> case MSR_K7_PERFCTR2 : //<S2SV> case MSR_K7_PERFCTR3 : //<S2SV> vcpu_unimpl ( vcpu , "unimplemented<S2SV_blank>perfctr<S2SV_blank>wrmsr:<S2SV_blank>" //<S2SV> "0x%x<S2SV_blank>data<S2SV_blank>0x%llx\\n" , msr , data ) ; //<S2SV> break ; //<S2SV> case MSR_P6_PERFCTR0 : //<S2SV> case MSR_P6_PERFCTR1 : //<S2SV> pr = true ; //<S2SV> case MSR_P6_EVNTSEL0 : //<S2SV> case MSR_P6_EVNTSEL1 : //<S2SV> if ( kvm_pmu_msr ( vcpu , msr ) ) //<S2SV> return kvm_pmu_set_msr ( vcpu , msr , data ) ; //<S2SV> if ( pr || data != 0 ) //<S2SV> vcpu_unimpl ( vcpu , "disabled<S2SV_blank>perfctr<S2SV_blank>wrmsr:<S2SV_blank>" //<S2SV> "0x%x<S2SV_blank>data<S2SV_blank>0x%llx\\n" , msr , data ) ; //<S2SV> break ; //<S2SV> case MSR_K7_CLK_CTL : //<S2SV> break ; //<S2SV> case HV_X64_MSR_GUEST_OS_ID ... HV_X64_MSR_SINT15 : //<S2SV> if ( kvm_hv_msr_partition_wide ( msr ) ) { //<S2SV> int r ; //<S2SV> mutex_lock ( & vcpu -> kvm -> lock ) ; //<S2SV> r = set_msr_hyperv_pw ( vcpu , msr , data ) ; //<S2SV> mutex_unlock ( & vcpu -> kvm -> lock ) ; //<S2SV> return r ; //<S2SV> } else //<S2SV> return set_msr_hyperv ( vcpu , msr , data ) ; //<S2SV> break ; //<S2SV> case MSR_IA32_BBL_CR_CTL3 : //<S2SV> vcpu_unimpl ( vcpu , "ignored<S2SV_blank>wrmsr:<S2SV_blank>0x%x<S2SV_blank>data<S2SV_blank>%llx\\n" , msr , data ) ; //<S2SV> break ; //<S2SV> case MSR_AMD64_OSVW_ID_LENGTH : //<S2SV> if ( ! guest_cpuid_has_osvw ( vcpu ) ) //<S2SV> return 1 ; //<S2SV> vcpu -> arch . osvw . length = data ; //<S2SV> break ; //<S2SV> case MSR_AMD64_OSVW_STATUS : //<S2SV> if ( ! guest_cpuid_has_osvw ( vcpu ) ) //<S2SV> return 1 ; //<S2SV> vcpu -> arch . osvw . status = data ; //<S2SV> break ; //<S2SV> default : //<S2SV> if ( msr && ( msr == vcpu -> kvm -> arch . xen_hvm_config . msr ) ) //<S2SV> return xen_hvm_config ( vcpu , data ) ; //<S2SV> if ( kvm_pmu_msr ( vcpu , msr ) ) //<S2SV> return kvm_pmu_set_msr ( vcpu , msr , data ) ; //<S2SV> if ( ! ignore_msrs ) { //<S2SV> vcpu_unimpl ( vcpu , "unhandled<S2SV_blank>wrmsr:<S2SV_blank>0x%x<S2SV_blank>data<S2SV_blank>%llx\\n" , //<S2SV> msr , data ) ; //<S2SV> return 1 ; //<S2SV> } else { //<S2SV> vcpu_unimpl ( vcpu , "ignored<S2SV_blank>wrmsr:<S2SV_blank>0x%x<S2SV_blank>data<S2SV_blank>%llx\\n" , //<S2SV> msr , data ) ; //<S2SV> break ; //<S2SV> } //<S2SV> } //<S2SV> return 0 ; //<S2SV> } //<S2SV> 