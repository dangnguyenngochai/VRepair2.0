static int kvm_guest_time_update ( struct kvm_vcpu * v ) //<S2SV> { //<S2SV> unsigned long flags , this_tsc_khz ; //<S2SV> struct kvm_vcpu_arch * vcpu = & v -> arch ; //<S2SV> struct kvm_arch * ka = & v -> kvm -> arch ; //<S2SV> s64 kernel_ns , max_kernel_ns ; //<S2SV> u64 tsc_timestamp , host_tsc ; //<S2SV> struct pvclock_vcpu_time_info guest_hv_clock ; //<S2SV> u8 pvclock_flags ; //<S2SV> bool use_master_clock ; //<S2SV> kernel_ns = 0 ; //<S2SV> host_tsc = 0 ; //<S2SV> spin_lock ( & ka -> pvclock_gtod_sync_lock ) ; //<S2SV> use_master_clock = ka -> use_master_clock ; //<S2SV> if ( use_master_clock ) { //<S2SV> host_tsc = ka -> master_cycle_now ; //<S2SV> kernel_ns = ka -> master_kernel_ns ; //<S2SV> } //<S2SV> spin_unlock ( & ka -> pvclock_gtod_sync_lock ) ; //<S2SV> local_irq_save ( flags ) ; //<S2SV> this_tsc_khz = __get_cpu_var ( cpu_tsc_khz ) ; //<S2SV> if ( unlikely ( this_tsc_khz == 0 ) ) { //<S2SV> local_irq_restore ( flags ) ; //<S2SV> kvm_make_request ( KVM_REQ_CLOCK_UPDATE , v ) ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> if ( ! use_master_clock ) { //<S2SV> host_tsc = native_read_tsc ( ) ; //<S2SV> kernel_ns = get_kernel_ns ( ) ; //<S2SV> } //<S2SV> tsc_timestamp = kvm_x86_ops -> read_l1_tsc ( v , host_tsc ) ; //<S2SV> if ( vcpu -> tsc_catchup ) { //<S2SV> u64 tsc = compute_guest_tsc ( v , kernel_ns ) ; //<S2SV> if ( tsc > tsc_timestamp ) { //<S2SV> adjust_tsc_offset_guest ( v , tsc - tsc_timestamp ) ; //<S2SV> tsc_timestamp = tsc ; //<S2SV> } //<S2SV> } //<S2SV> local_irq_restore ( flags ) ; //<S2SV> if ( ! vcpu -> pv_time_enabled ) //<S2SV> return 0 ; //<S2SV> max_kernel_ns = 0 ; //<S2SV> if ( vcpu -> hv_clock . tsc_timestamp ) { //<S2SV> max_kernel_ns = vcpu -> last_guest_tsc - //<S2SV> vcpu -> hv_clock . tsc_timestamp ; //<S2SV> max_kernel_ns = pvclock_scale_delta ( max_kernel_ns , //<S2SV> vcpu -> hv_clock . tsc_to_system_mul , //<S2SV> vcpu -> hv_clock . tsc_shift ) ; //<S2SV> max_kernel_ns += vcpu -> last_kernel_ns ; //<S2SV> } //<S2SV> if ( unlikely ( vcpu -> hw_tsc_khz != this_tsc_khz ) ) { //<S2SV> kvm_get_time_scale ( NSEC_PER_SEC / 1000 , this_tsc_khz , //<S2SV> & vcpu -> hv_clock . tsc_shift , //<S2SV> & vcpu -> hv_clock . tsc_to_system_mul ) ; //<S2SV> vcpu -> hw_tsc_khz = this_tsc_khz ; //<S2SV> } //<S2SV> if ( ! use_master_clock ) { //<S2SV> if ( max_kernel_ns > kernel_ns ) //<S2SV> kernel_ns = max_kernel_ns ; //<S2SV> } //<S2SV> vcpu -> hv_clock . tsc_timestamp = tsc_timestamp ; //<S2SV> vcpu -> hv_clock . system_time = kernel_ns + v -> kvm -> arch . kvmclock_offset ; //<S2SV> vcpu -> last_kernel_ns = kernel_ns ; //<S2SV> vcpu -> last_guest_tsc = tsc_timestamp ; //<S2SV> vcpu -> hv_clock . version += 2 ; //<S2SV> if ( unlikely ( kvm_read_guest_cached ( v -> kvm , & vcpu -> pv_time , //<S2SV> & guest_hv_clock , sizeof ( guest_hv_clock ) ) ) ) //<S2SV> return 0 ; //<S2SV> pvclock_flags = ( guest_hv_clock . flags & PVCLOCK_GUEST_STOPPED ) ; //<S2SV> if ( vcpu -> pvclock_set_guest_stopped_request ) { //<S2SV> pvclock_flags |= PVCLOCK_GUEST_STOPPED ; //<S2SV> vcpu -> pvclock_set_guest_stopped_request = false ; //<S2SV> } //<S2SV> if ( use_master_clock ) //<S2SV> pvclock_flags |= PVCLOCK_TSC_STABLE_BIT ; //<S2SV> vcpu -> hv_clock . flags = pvclock_flags ; //<S2SV> kvm_write_guest_cached ( v -> kvm , & vcpu -> pv_time , //<S2SV> & vcpu -> hv_clock , //<S2SV> sizeof ( vcpu -> hv_clock ) ) ; //<S2SV> return 0 ; //<S2SV> } //<S2SV> 