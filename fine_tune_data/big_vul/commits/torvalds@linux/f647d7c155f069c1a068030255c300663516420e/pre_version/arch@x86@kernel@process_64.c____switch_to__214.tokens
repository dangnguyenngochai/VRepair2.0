__visible __notrace_funcgraph struct task_struct * //<S2SV> __switch_to ( struct task_struct * prev_p , struct task_struct * next_p ) //<S2SV> { //<S2SV> struct thread_struct * prev = & prev_p -> thread ; //<S2SV> struct thread_struct * next = & next_p -> thread ; //<S2SV> int cpu = smp_processor_id ( ) ; //<S2SV> struct tss_struct * tss = & per_cpu ( init_tss , cpu ) ; //<S2SV> unsigned fsindex , gsindex ; //<S2SV> fpu_switch_t fpu ; //<S2SV> fpu = switch_fpu_prepare ( prev_p , next_p , cpu ) ; //<S2SV> load_sp0 ( tss , next ) ; //<S2SV> savesegment ( es , prev -> es ) ; //<S2SV> if ( unlikely ( next -> es | prev -> es ) ) //<S2SV> loadsegment ( es , next -> es ) ; //<S2SV> savesegment ( ds , prev -> ds ) ; //<S2SV> if ( unlikely ( next -> ds | prev -> ds ) ) //<S2SV> loadsegment ( ds , next -> ds ) ; //<S2SV> savesegment ( fs , fsindex ) ; //<S2SV> savesegment ( gs , gsindex ) ; //<S2SV> load_TLS ( next , cpu ) ; //<S2SV> arch_end_context_switch ( next_p ) ; //<S2SV> if ( unlikely ( fsindex | next -> fsindex | prev -> fs ) ) { //<S2SV> loadsegment ( fs , next -> fsindex ) ; //<S2SV> if ( fsindex ) //<S2SV> prev -> fs = 0 ; //<S2SV> } //<S2SV> if ( next -> fs ) //<S2SV> wrmsrl ( MSR_FS_BASE , next -> fs ) ; //<S2SV> prev -> fsindex = fsindex ; //<S2SV> if ( unlikely ( gsindex | next -> gsindex | prev -> gs ) ) { //<S2SV> load_gs_index ( next -> gsindex ) ; //<S2SV> if ( gsindex ) //<S2SV> prev -> gs = 0 ; //<S2SV> } //<S2SV> if ( next -> gs ) //<S2SV> wrmsrl ( MSR_KERNEL_GS_BASE , next -> gs ) ; //<S2SV> prev -> gsindex = gsindex ; //<S2SV> switch_fpu_finish ( next_p , fpu ) ; //<S2SV> prev -> usersp = this_cpu_read ( old_rsp ) ; //<S2SV> this_cpu_write ( old_rsp , next -> usersp ) ; //<S2SV> this_cpu_write ( current_task , next_p ) ; //<S2SV> task_thread_info ( prev_p ) -> saved_preempt_count = this_cpu_read ( __preempt_count ) ; //<S2SV> this_cpu_write ( __preempt_count , task_thread_info ( next_p ) -> saved_preempt_count ) ; //<S2SV> this_cpu_write ( kernel_stack , //<S2SV> ( unsigned long ) task_stack_page ( next_p ) + //<S2SV> THREAD_SIZE - KERNEL_STACK_OFFSET ) ; //<S2SV> if ( unlikely ( task_thread_info ( next_p ) -> flags & _TIF_WORK_CTXSW_NEXT || //<S2SV> task_thread_info ( prev_p ) -> flags & _TIF_WORK_CTXSW_PREV ) ) //<S2SV> __switch_to_xtra ( prev_p , next_p , tss ) ; //<S2SV> return prev_p ; //<S2SV> } //<S2SV> 