int hugetlb_mcopy_atomic_pte ( struct mm_struct * dst_mm , //<S2SV> pte_t * dst_pte , //<S2SV> struct vm_area_struct * dst_vma , //<S2SV> unsigned long dst_addr , //<S2SV> unsigned long src_addr , //<S2SV> struct page * * pagep ) //<S2SV> { //<S2SV> int vm_shared = dst_vma -> vm_flags & VM_SHARED ; //<S2SV> struct hstate * h = hstate_vma ( dst_vma ) ; //<S2SV> pte_t _dst_pte ; //<S2SV> spinlock_t * ptl ; //<S2SV> int ret ; //<S2SV> struct page * page ; //<S2SV> if ( ! * pagep ) { //<S2SV> ret = - ENOMEM ; //<S2SV> page = alloc_huge_page ( dst_vma , dst_addr , 0 ) ; //<S2SV> if ( IS_ERR ( page ) ) //<S2SV> goto out ; //<S2SV> ret = copy_huge_page_from_user ( page , //<S2SV> ( const void __user * ) src_addr , //<S2SV> pages_per_huge_page ( h ) , false ) ; //<S2SV> if ( unlikely ( ret ) ) { //<S2SV> ret = - EFAULT ; //<S2SV> * pagep = page ; //<S2SV> goto out ; //<S2SV> } //<S2SV> } else { //<S2SV> page = * pagep ; //<S2SV> * pagep = NULL ; //<S2SV> } //<S2SV> __SetPageUptodate ( page ) ; //<S2SV> set_page_huge_active ( page ) ; //<S2SV> if ( vm_shared ) { //<S2SV> struct address_space * mapping = dst_vma -> vm_file -> f_mapping ; //<S2SV> pgoff_t idx = vma_hugecache_offset ( h , dst_vma , dst_addr ) ; //<S2SV> ret = huge_add_to_page_cache ( page , mapping , idx ) ; //<S2SV> if ( ret ) //<S2SV> goto out_release_nounlock ; //<S2SV> } //<S2SV> ptl = huge_pte_lockptr ( h , dst_mm , dst_pte ) ; //<S2SV> spin_lock ( ptl ) ; //<S2SV> ret = - EEXIST ; //<S2SV> if ( ! huge_pte_none ( huge_ptep_get ( dst_pte ) ) ) //<S2SV> goto out_release_unlock ; //<S2SV> if ( vm_shared ) { //<S2SV> page_dup_rmap ( page , true ) ; //<S2SV> } else { //<S2SV> ClearPagePrivate ( page ) ; //<S2SV> hugepage_add_new_anon_rmap ( page , dst_vma , dst_addr ) ; //<S2SV> } //<S2SV> _dst_pte = make_huge_pte ( dst_vma , page , dst_vma -> vm_flags & VM_WRITE ) ; //<S2SV> if ( dst_vma -> vm_flags & VM_WRITE ) //<S2SV> _dst_pte = huge_pte_mkdirty ( _dst_pte ) ; //<S2SV> _dst_pte = pte_mkyoung ( _dst_pte ) ; //<S2SV> set_huge_pte_at ( dst_mm , dst_addr , dst_pte , _dst_pte ) ; //<S2SV> ( void ) huge_ptep_set_access_flags ( dst_vma , dst_addr , dst_pte , _dst_pte , //<S2SV> dst_vma -> vm_flags & VM_WRITE ) ; //<S2SV> hugetlb_count_add ( pages_per_huge_page ( h ) , dst_mm ) ; //<S2SV> update_mmu_cache ( dst_vma , dst_addr , dst_pte ) ; //<S2SV> spin_unlock ( ptl ) ; //<S2SV> if ( vm_shared ) //<S2SV> unlock_page ( page ) ; //<S2SV> ret = 0 ; //<S2SV> out : //<S2SV> return ret ; //<S2SV> out_release_unlock : //<S2SV> spin_unlock ( ptl ) ; //<S2SV> if ( vm_shared ) //<S2SV> unlock_page ( page ) ; //<S2SV> out_release_nounlock : //<S2SV> put_page ( page ) ; //<S2SV> goto out ; //<S2SV> } //<S2SV> 