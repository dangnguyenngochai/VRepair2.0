long follow_hugetlb_page ( struct mm_struct * mm , struct vm_area_struct * vma , //<S2SV> struct page * * pages , struct vm_area_struct * * vmas , //<S2SV> unsigned long * position , unsigned long * nr_pages , //<S2SV> long i , unsigned int flags , int * nonblocking ) //<S2SV> { //<S2SV> unsigned long pfn_offset ; //<S2SV> unsigned long vaddr = * position ; //<S2SV> unsigned long remainder = * nr_pages ; //<S2SV> struct hstate * h = hstate_vma ( vma ) ; //<S2SV> int err = - EFAULT ; //<S2SV> while ( vaddr < vma -> vm_end && remainder ) { //<S2SV> pte_t * pte ; //<S2SV> spinlock_t * ptl = NULL ; //<S2SV> int absent ; //<S2SV> struct page * page ; //<S2SV> if ( fatal_signal_pending ( current ) ) { //<S2SV> remainder = 0 ; //<S2SV> break ; //<S2SV> } //<S2SV> pte = huge_pte_offset ( mm , vaddr & huge_page_mask ( h ) , //<S2SV> huge_page_size ( h ) ) ; //<S2SV> if ( pte ) //<S2SV> ptl = huge_pte_lock ( h , mm , pte ) ; //<S2SV> absent = ! pte || huge_pte_none ( huge_ptep_get ( pte ) ) ; //<S2SV> if ( absent && ( flags & FOLL_DUMP ) && //<S2SV> ! hugetlbfs_pagecache_present ( h , vma , vaddr ) ) { //<S2SV> if ( pte ) //<S2SV> spin_unlock ( ptl ) ; //<S2SV> remainder = 0 ; //<S2SV> break ; //<S2SV> } //<S2SV> if ( absent || is_swap_pte ( huge_ptep_get ( pte ) ) || //<S2SV> ( ( flags & FOLL_WRITE ) && //<S2SV> ! huge_pte_write ( huge_ptep_get ( pte ) ) ) ) { //<S2SV> vm_fault_t ret ; //<S2SV> unsigned int fault_flags = 0 ; //<S2SV> if ( pte ) //<S2SV> spin_unlock ( ptl ) ; //<S2SV> if ( flags & FOLL_WRITE ) //<S2SV> fault_flags |= FAULT_FLAG_WRITE ; //<S2SV> if ( nonblocking ) //<S2SV> fault_flags |= FAULT_FLAG_ALLOW_RETRY ; //<S2SV> if ( flags & FOLL_NOWAIT ) //<S2SV> fault_flags |= FAULT_FLAG_ALLOW_RETRY | //<S2SV> FAULT_FLAG_RETRY_NOWAIT ; //<S2SV> if ( flags & FOLL_TRIED ) { //<S2SV> VM_WARN_ON_ONCE ( fault_flags & //<S2SV> FAULT_FLAG_ALLOW_RETRY ) ; //<S2SV> fault_flags |= FAULT_FLAG_TRIED ; //<S2SV> } //<S2SV> ret = hugetlb_fault ( mm , vma , vaddr , fault_flags ) ; //<S2SV> if ( ret & VM_FAULT_ERROR ) { //<S2SV> err = vm_fault_to_errno ( ret , flags ) ; //<S2SV> remainder = 0 ; //<S2SV> break ; //<S2SV> } //<S2SV> if ( ret & VM_FAULT_RETRY ) { //<S2SV> if ( nonblocking && //<S2SV> ! ( fault_flags & FAULT_FLAG_RETRY_NOWAIT ) ) //<S2SV> * nonblocking = 0 ; //<S2SV> * nr_pages = 0 ; //<S2SV> return i ; //<S2SV> } //<S2SV> continue ; //<S2SV> } //<S2SV> pfn_offset = ( vaddr & ~ huge_page_mask ( h ) ) >> PAGE_SHIFT ; //<S2SV> page = pte_page ( huge_ptep_get ( pte ) ) ; //<S2SV> if ( unlikely ( page_count ( page ) <= 0 ) ) { //<S2SV> if ( pages ) { //<S2SV> spin_unlock ( ptl ) ; //<S2SV> remainder = 0 ; //<S2SV> err = - ENOMEM ; //<S2SV> break ; //<S2SV> } //<S2SV> } //<S2SV> same_page : //<S2SV> if ( pages ) { //<S2SV> pages [ i ] = mem_map_offset ( page , pfn_offset ) ; //<S2SV> get_page ( pages [ i ] ) ; //<S2SV> } //<S2SV> if ( vmas ) //<S2SV> vmas [ i ] = vma ; //<S2SV> vaddr += PAGE_SIZE ; //<S2SV> ++ pfn_offset ; //<S2SV> -- remainder ; //<S2SV> ++ i ; //<S2SV> if ( vaddr < vma -> vm_end && remainder && //<S2SV> pfn_offset < pages_per_huge_page ( h ) ) { //<S2SV> goto same_page ; //<S2SV> } //<S2SV> spin_unlock ( ptl ) ; //<S2SV> } //<S2SV> * nr_pages = remainder ; //<S2SV> * position = vaddr ; //<S2SV> return i ? i : err ; //<S2SV> } //<S2SV> 