static void * __dma_alloc_coherent ( struct device * dev , size_t size , //<S2SV> dma_addr_t * dma_handle , gfp_t flags , //<S2SV> struct dma_attrs * attrs ) //<S2SV> { //<S2SV> if ( dev == NULL ) { //<S2SV> WARN_ONCE ( 1 , "Use<S2SV_blank>an<S2SV_blank>actual<S2SV_blank>device<S2SV_blank>structure<S2SV_blank>for<S2SV_blank>DMA<S2SV_blank>allocation\\n" ) ; //<S2SV> return NULL ; //<S2SV> } //<S2SV> if ( IS_ENABLED ( CONFIG_ZONE_DMA ) && //<S2SV> dev -> coherent_dma_mask <= DMA_BIT_MASK ( 32 ) ) //<S2SV> flags |= GFP_DMA ; //<S2SV> if ( IS_ENABLED ( CONFIG_DMA_CMA ) && ( flags & __GFP_WAIT ) ) { //<S2SV> struct page * page ; //<S2SV> void * addr ; //<S2SV> size = PAGE_ALIGN ( size ) ; //<S2SV> page = dma_alloc_from_contiguous ( dev , size >> PAGE_SHIFT , //<S2SV> get_order ( size ) ) ; //<S2SV> if ( ! page ) //<S2SV> return NULL ; //<S2SV> * dma_handle = phys_to_dma ( dev , page_to_phys ( page ) ) ; //<S2SV> addr = page_address ( page ) ; //<S2SV> if ( flags & __GFP_ZERO ) //<S2SV> memset ( addr , 0 , size ) ; //<S2SV> return addr ; //<S2SV> } else { //<S2SV> return swiotlb_alloc_coherent ( dev , size , dma_handle , flags ) ; //<S2SV> } //<S2SV> } //<S2SV> 