int ext4_ext_insert_extent ( handle_t * handle , struct inode * inode , //<S2SV> struct ext4_ext_path * path , //<S2SV> struct ext4_extent * newext , int flag ) //<S2SV> { //<S2SV> struct ext4_extent_header * eh ; //<S2SV> struct ext4_extent * ex , * fex ; //<S2SV> struct ext4_extent * nearex ; //<S2SV> struct ext4_ext_path * npath = NULL ; //<S2SV> int depth , len , err ; //<S2SV> ext4_lblk_t next ; //<S2SV> unsigned uninitialized = 0 ; //<S2SV> BUG_ON ( ext4_ext_get_actual_len ( newext ) == 0 ) ; //<S2SV> depth = ext_depth ( inode ) ; //<S2SV> ex = path [ depth ] . p_ext ; //<S2SV> BUG_ON ( path [ depth ] . p_hdr == NULL ) ; //<S2SV> if ( ex && ! ( flag & EXT4_GET_BLOCKS_PRE_IO ) //<S2SV> && ext4_can_extents_be_merged ( inode , ex , newext ) ) { //<S2SV> ext_debug ( "append<S2SV_blank>[%d]%d<S2SV_blank>block<S2SV_blank>to<S2SV_blank>%d:[%d]%d<S2SV_blank>(from<S2SV_blank>%llu)\\n" , //<S2SV> ext4_ext_is_uninitialized ( newext ) , //<S2SV> ext4_ext_get_actual_len ( newext ) , //<S2SV> le32_to_cpu ( ex -> ee_block ) , //<S2SV> ext4_ext_is_uninitialized ( ex ) , //<S2SV> ext4_ext_get_actual_len ( ex ) , ext_pblock ( ex ) ) ; //<S2SV> err = ext4_ext_get_access ( handle , inode , path + depth ) ; //<S2SV> if ( err ) //<S2SV> return err ; //<S2SV> if ( ext4_ext_is_uninitialized ( ex ) ) //<S2SV> uninitialized = 1 ; //<S2SV> ex -> ee_len = cpu_to_le16 ( ext4_ext_get_actual_len ( ex ) //<S2SV> + ext4_ext_get_actual_len ( newext ) ) ; //<S2SV> if ( uninitialized ) //<S2SV> ext4_ext_mark_uninitialized ( ex ) ; //<S2SV> eh = path [ depth ] . p_hdr ; //<S2SV> nearex = ex ; //<S2SV> goto merge ; //<S2SV> } //<S2SV> repeat : //<S2SV> depth = ext_depth ( inode ) ; //<S2SV> eh = path [ depth ] . p_hdr ; //<S2SV> if ( le16_to_cpu ( eh -> eh_entries ) < le16_to_cpu ( eh -> eh_max ) ) //<S2SV> goto has_space ; //<S2SV> fex = EXT_LAST_EXTENT ( eh ) ; //<S2SV> next = ext4_ext_next_leaf_block ( inode , path ) ; //<S2SV> if ( le32_to_cpu ( newext -> ee_block ) > le32_to_cpu ( fex -> ee_block ) //<S2SV> && next != EXT_MAX_BLOCK ) { //<S2SV> ext_debug ( "next<S2SV_blank>leaf<S2SV_blank>block<S2SV_blank>-<S2SV_blank>%d\\n" , next ) ; //<S2SV> BUG_ON ( npath != NULL ) ; //<S2SV> npath = ext4_ext_find_extent ( inode , next , NULL ) ; //<S2SV> if ( IS_ERR ( npath ) ) //<S2SV> return PTR_ERR ( npath ) ; //<S2SV> BUG_ON ( npath -> p_depth != path -> p_depth ) ; //<S2SV> eh = npath [ depth ] . p_hdr ; //<S2SV> if ( le16_to_cpu ( eh -> eh_entries ) < le16_to_cpu ( eh -> eh_max ) ) { //<S2SV> ext_debug ( "next<S2SV_blank>leaf<S2SV_blank>isnt<S2SV_blank>full(%d)\\n" , //<S2SV> le16_to_cpu ( eh -> eh_entries ) ) ; //<S2SV> path = npath ; //<S2SV> goto repeat ; //<S2SV> } //<S2SV> ext_debug ( "next<S2SV_blank>leaf<S2SV_blank>has<S2SV_blank>no<S2SV_blank>free<S2SV_blank>space(%d,%d)\\n" , //<S2SV> le16_to_cpu ( eh -> eh_entries ) , le16_to_cpu ( eh -> eh_max ) ) ; //<S2SV> } //<S2SV> err = ext4_ext_create_new_leaf ( handle , inode , path , newext ) ; //<S2SV> if ( err ) //<S2SV> goto cleanup ; //<S2SV> depth = ext_depth ( inode ) ; //<S2SV> eh = path [ depth ] . p_hdr ; //<S2SV> has_space : //<S2SV> nearex = path [ depth ] . p_ext ; //<S2SV> err = ext4_ext_get_access ( handle , inode , path + depth ) ; //<S2SV> if ( err ) //<S2SV> goto cleanup ; //<S2SV> if ( ! nearex ) { //<S2SV> ext_debug ( "first<S2SV_blank>extent<S2SV_blank>in<S2SV_blank>the<S2SV_blank>leaf:<S2SV_blank>%d:%llu:[%d]%d\\n" , //<S2SV> le32_to_cpu ( newext -> ee_block ) , //<S2SV> ext_pblock ( newext ) , //<S2SV> ext4_ext_is_uninitialized ( newext ) , //<S2SV> ext4_ext_get_actual_len ( newext ) ) ; //<S2SV> path [ depth ] . p_ext = EXT_FIRST_EXTENT ( eh ) ; //<S2SV> } else if ( le32_to_cpu ( newext -> ee_block ) //<S2SV> > le32_to_cpu ( nearex -> ee_block ) ) { //<S2SV> if ( nearex != EXT_LAST_EXTENT ( eh ) ) { //<S2SV> len = EXT_MAX_EXTENT ( eh ) - nearex ; //<S2SV> len = ( len - 1 ) * sizeof ( struct ext4_extent ) ; //<S2SV> len = len < 0 ? 0 : len ; //<S2SV> ext_debug ( "insert<S2SV_blank>%d:%llu:[%d]%d<S2SV_blank>after:<S2SV_blank>nearest<S2SV_blank>0x%p,<S2SV_blank>" //<S2SV> "move<S2SV_blank>%d<S2SV_blank>from<S2SV_blank>0x%p<S2SV_blank>to<S2SV_blank>0x%p\\n" , //<S2SV> le32_to_cpu ( newext -> ee_block ) , //<S2SV> ext_pblock ( newext ) , //<S2SV> ext4_ext_is_uninitialized ( newext ) , //<S2SV> ext4_ext_get_actual_len ( newext ) , //<S2SV> nearex , len , nearex + 1 , nearex + 2 ) ; //<S2SV> memmove ( nearex + 2 , nearex + 1 , len ) ; //<S2SV> } //<S2SV> path [ depth ] . p_ext = nearex + 1 ; //<S2SV> } else { //<S2SV> BUG_ON ( newext -> ee_block == nearex -> ee_block ) ; //<S2SV> len = ( EXT_MAX_EXTENT ( eh ) - nearex ) * sizeof ( struct ext4_extent ) ; //<S2SV> len = len < 0 ? 0 : len ; //<S2SV> ext_debug ( "insert<S2SV_blank>%d:%llu:[%d]%d<S2SV_blank>before:<S2SV_blank>nearest<S2SV_blank>0x%p,<S2SV_blank>" //<S2SV> "move<S2SV_blank>%d<S2SV_blank>from<S2SV_blank>0x%p<S2SV_blank>to<S2SV_blank>0x%p\\n" , //<S2SV> le32_to_cpu ( newext -> ee_block ) , //<S2SV> ext_pblock ( newext ) , //<S2SV> ext4_ext_is_uninitialized ( newext ) , //<S2SV> ext4_ext_get_actual_len ( newext ) , //<S2SV> nearex , len , nearex + 1 , nearex + 2 ) ; //<S2SV> memmove ( nearex + 1 , nearex , len ) ; //<S2SV> path [ depth ] . p_ext = nearex ; //<S2SV> } //<S2SV> le16_add_cpu ( & eh -> eh_entries , 1 ) ; //<S2SV> nearex = path [ depth ] . p_ext ; //<S2SV> nearex -> ee_block = newext -> ee_block ; //<S2SV> ext4_ext_store_pblock ( nearex , ext_pblock ( newext ) ) ; //<S2SV> nearex -> ee_len = newext -> ee_len ; //<S2SV> merge : //<S2SV> if ( ! ( flag & EXT4_GET_BLOCKS_PRE_IO ) ) //<S2SV> ext4_ext_try_to_merge ( inode , path , nearex ) ; //<S2SV> err = ext4_ext_correct_indexes ( handle , inode , path ) ; //<S2SV> if ( err ) //<S2SV> goto cleanup ; //<S2SV> err = ext4_ext_dirty ( handle , inode , path + depth ) ; //<S2SV> cleanup : //<S2SV> if ( npath ) { //<S2SV> ext4_ext_drop_refs ( npath ) ; //<S2SV> kfree ( npath ) ; //<S2SV> } //<S2SV> ext4_ext_invalidate_cache ( inode ) ; //<S2SV> return err ; //<S2SV> } //<S2SV> 