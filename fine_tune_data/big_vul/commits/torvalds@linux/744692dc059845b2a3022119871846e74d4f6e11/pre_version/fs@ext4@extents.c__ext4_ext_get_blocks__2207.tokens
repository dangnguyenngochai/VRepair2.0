int ext4_ext_get_blocks ( handle_t * handle , struct inode * inode , //<S2SV> ext4_lblk_t iblock , //<S2SV> unsigned int max_blocks , struct buffer_head * bh_result , //<S2SV> int flags ) //<S2SV> { //<S2SV> struct ext4_ext_path * path = NULL ; //<S2SV> struct ext4_extent_header * eh ; //<S2SV> struct ext4_extent newex , * ex , * last_ex ; //<S2SV> ext4_fsblk_t newblock ; //<S2SV> int err = 0 , depth , ret , cache_type ; //<S2SV> unsigned int allocated = 0 ; //<S2SV> struct ext4_allocation_request ar ; //<S2SV> ext4_io_end_t * io = EXT4_I ( inode ) -> cur_aio_dio ; //<S2SV> __clear_bit ( BH_New , & bh_result -> b_state ) ; //<S2SV> ext_debug ( "blocks<S2SV_blank>%u/%u<S2SV_blank>requested<S2SV_blank>for<S2SV_blank>inode<S2SV_blank>%lu\\n" , //<S2SV> iblock , max_blocks , inode -> i_ino ) ; //<S2SV> cache_type = ext4_ext_in_cache ( inode , iblock , & newex ) ; //<S2SV> if ( cache_type ) { //<S2SV> if ( cache_type == EXT4_EXT_CACHE_GAP ) { //<S2SV> if ( ( flags & EXT4_GET_BLOCKS_CREATE ) == 0 ) { //<S2SV> goto out2 ; //<S2SV> } //<S2SV> } else if ( cache_type == EXT4_EXT_CACHE_EXTENT ) { //<S2SV> newblock = iblock //<S2SV> - le32_to_cpu ( newex . ee_block ) //<S2SV> + ext_pblock ( & newex ) ; //<S2SV> allocated = ext4_ext_get_actual_len ( & newex ) - //<S2SV> ( iblock - le32_to_cpu ( newex . ee_block ) ) ; //<S2SV> goto out ; //<S2SV> } else { //<S2SV> BUG ( ) ; //<S2SV> } //<S2SV> } //<S2SV> path = ext4_ext_find_extent ( inode , iblock , NULL ) ; //<S2SV> if ( IS_ERR ( path ) ) { //<S2SV> err = PTR_ERR ( path ) ; //<S2SV> path = NULL ; //<S2SV> goto out2 ; //<S2SV> } //<S2SV> depth = ext_depth ( inode ) ; //<S2SV> if ( path [ depth ] . p_ext == NULL && depth != 0 ) { //<S2SV> ext4_error ( inode -> i_sb , "bad<S2SV_blank>extent<S2SV_blank>address<S2SV_blank>" //<S2SV> "inode:<S2SV_blank>%lu,<S2SV_blank>iblock:<S2SV_blank>%d,<S2SV_blank>depth:<S2SV_blank>%d" , //<S2SV> inode -> i_ino , iblock , depth ) ; //<S2SV> err = - EIO ; //<S2SV> goto out2 ; //<S2SV> } //<S2SV> eh = path [ depth ] . p_hdr ; //<S2SV> ex = path [ depth ] . p_ext ; //<S2SV> if ( ex ) { //<S2SV> ext4_lblk_t ee_block = le32_to_cpu ( ex -> ee_block ) ; //<S2SV> ext4_fsblk_t ee_start = ext_pblock ( ex ) ; //<S2SV> unsigned short ee_len ; //<S2SV> ee_len = ext4_ext_get_actual_len ( ex ) ; //<S2SV> if ( iblock >= ee_block && iblock < ee_block + ee_len ) { //<S2SV> newblock = iblock - ee_block + ee_start ; //<S2SV> allocated = ee_len - ( iblock - ee_block ) ; //<S2SV> ext_debug ( "%u<S2SV_blank>fit<S2SV_blank>into<S2SV_blank>%u:%d<S2SV_blank>-><S2SV_blank>%llu\\n" , iblock , //<S2SV> ee_block , ee_len , newblock ) ; //<S2SV> if ( ! ext4_ext_is_uninitialized ( ex ) ) { //<S2SV> ext4_ext_put_in_cache ( inode , ee_block , //<S2SV> ee_len , ee_start , //<S2SV> EXT4_EXT_CACHE_EXTENT ) ; //<S2SV> goto out ; //<S2SV> } //<S2SV> ret = ext4_ext_handle_uninitialized_extents ( handle , //<S2SV> inode , iblock , max_blocks , path , //<S2SV> flags , allocated , bh_result , newblock ) ; //<S2SV> return ret ; //<S2SV> } //<S2SV> } //<S2SV> if ( ( flags & EXT4_GET_BLOCKS_CREATE ) == 0 ) { //<S2SV> ext4_ext_put_gap_in_cache ( inode , path , iblock ) ; //<S2SV> goto out2 ; //<S2SV> } //<S2SV> ar . lleft = iblock ; //<S2SV> err = ext4_ext_search_left ( inode , path , & ar . lleft , & ar . pleft ) ; //<S2SV> if ( err ) //<S2SV> goto out2 ; //<S2SV> ar . lright = iblock ; //<S2SV> err = ext4_ext_search_right ( inode , path , & ar . lright , & ar . pright ) ; //<S2SV> if ( err ) //<S2SV> goto out2 ; //<S2SV> if ( max_blocks > EXT_INIT_MAX_LEN && //<S2SV> ! ( flags & EXT4_GET_BLOCKS_UNINIT_EXT ) ) //<S2SV> max_blocks = EXT_INIT_MAX_LEN ; //<S2SV> else if ( max_blocks > EXT_UNINIT_MAX_LEN && //<S2SV> ( flags & EXT4_GET_BLOCKS_UNINIT_EXT ) ) //<S2SV> max_blocks = EXT_UNINIT_MAX_LEN ; //<S2SV> newex . ee_block = cpu_to_le32 ( iblock ) ; //<S2SV> newex . ee_len = cpu_to_le16 ( max_blocks ) ; //<S2SV> err = ext4_ext_check_overlap ( inode , & newex , path ) ; //<S2SV> if ( err ) //<S2SV> allocated = ext4_ext_get_actual_len ( & newex ) ; //<S2SV> else //<S2SV> allocated = max_blocks ; //<S2SV> ar . inode = inode ; //<S2SV> ar . goal = ext4_ext_find_goal ( inode , path , iblock ) ; //<S2SV> ar . logical = iblock ; //<S2SV> ar . len = allocated ; //<S2SV> if ( S_ISREG ( inode -> i_mode ) ) //<S2SV> ar . flags = EXT4_MB_HINT_DATA ; //<S2SV> else //<S2SV> ar . flags = 0 ; //<S2SV> newblock = ext4_mb_new_blocks ( handle , & ar , & err ) ; //<S2SV> if ( ! newblock ) //<S2SV> goto out2 ; //<S2SV> ext_debug ( "allocate<S2SV_blank>new<S2SV_blank>block:<S2SV_blank>goal<S2SV_blank>%llu,<S2SV_blank>found<S2SV_blank>%llu/%u\\n" , //<S2SV> ar . goal , newblock , allocated ) ; //<S2SV> ext4_ext_store_pblock ( & newex , newblock ) ; //<S2SV> newex . ee_len = cpu_to_le16 ( ar . len ) ; //<S2SV> if ( flags & EXT4_GET_BLOCKS_UNINIT_EXT ) { //<S2SV> ext4_ext_mark_uninitialized ( & newex ) ; //<S2SV> if ( flags == EXT4_GET_BLOCKS_PRE_IO ) { //<S2SV> if ( io ) //<S2SV> io -> flag = EXT4_IO_UNWRITTEN ; //<S2SV> else //<S2SV> ext4_set_inode_state ( inode , //<S2SV> EXT4_STATE_DIO_UNWRITTEN ) ; //<S2SV> } //<S2SV> } //<S2SV> if ( unlikely ( EXT4_I ( inode ) -> i_flags & EXT4_EOFBLOCKS_FL ) ) { //<S2SV> if ( eh -> eh_entries ) { //<S2SV> last_ex = EXT_LAST_EXTENT ( eh ) ; //<S2SV> if ( iblock + ar . len > le32_to_cpu ( last_ex -> ee_block ) //<S2SV> + ext4_ext_get_actual_len ( last_ex ) ) //<S2SV> EXT4_I ( inode ) -> i_flags &= ~ EXT4_EOFBLOCKS_FL ; //<S2SV> } else { //<S2SV> WARN_ON ( eh -> eh_entries == 0 ) ; //<S2SV> ext4_error ( inode -> i_sb , __func__ , //<S2SV> "inode#%lu,<S2SV_blank>eh->eh_entries<S2SV_blank>=<S2SV_blank>0!" , inode -> i_ino ) ; //<S2SV> } //<S2SV> } //<S2SV> err = ext4_ext_insert_extent ( handle , inode , path , & newex , flags ) ; //<S2SV> if ( err ) { //<S2SV> ext4_discard_preallocations ( inode ) ; //<S2SV> ext4_free_blocks ( handle , inode , 0 , ext_pblock ( & newex ) , //<S2SV> ext4_ext_get_actual_len ( & newex ) , 0 ) ; //<S2SV> goto out2 ; //<S2SV> } //<S2SV> newblock = ext_pblock ( & newex ) ; //<S2SV> allocated = ext4_ext_get_actual_len ( & newex ) ; //<S2SV> if ( allocated > max_blocks ) //<S2SV> allocated = max_blocks ; //<S2SV> set_buffer_new ( bh_result ) ; //<S2SV> if ( flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE ) //<S2SV> ext4_da_update_reserve_space ( inode , allocated , 1 ) ; //<S2SV> if ( ( flags & EXT4_GET_BLOCKS_UNINIT_EXT ) == 0 ) { //<S2SV> ext4_ext_put_in_cache ( inode , iblock , allocated , newblock , //<S2SV> EXT4_EXT_CACHE_EXTENT ) ; //<S2SV> ext4_update_inode_fsync_trans ( handle , inode , 1 ) ; //<S2SV> } else //<S2SV> ext4_update_inode_fsync_trans ( handle , inode , 0 ) ; //<S2SV> out : //<S2SV> if ( allocated > max_blocks ) //<S2SV> allocated = max_blocks ; //<S2SV> ext4_ext_show_leaf ( inode , path ) ; //<S2SV> set_buffer_mapped ( bh_result ) ; //<S2SV> bh_result -> b_bdev = inode -> i_sb -> s_bdev ; //<S2SV> bh_result -> b_blocknr = newblock ; //<S2SV> out2 : //<S2SV> if ( path ) { //<S2SV> ext4_ext_drop_refs ( path ) ; //<S2SV> kfree ( path ) ; //<S2SV> } //<S2SV> return err ? err : allocated ; //<S2SV> } //<S2SV> 