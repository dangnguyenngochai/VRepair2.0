static void flush_end_io ( struct request * flush_rq , int error ) //<S2SV> { //<S2SV> struct request_queue * q = flush_rq -> q ; //<S2SV> struct list_head * running ; //<S2SV> bool queued = false ; //<S2SV> struct request * rq , * n ; //<S2SV> unsigned long flags = 0 ; //<S2SV> struct blk_flush_queue * fq = blk_get_flush_queue ( q , flush_rq -> mq_ctx ) ; //<S2SV> if ( q -> mq_ops ) { //<S2SV> spin_lock_irqsave ( & fq -> mq_flush_lock , flags ) ; //<S2SV> flush_rq -> tag = - 1 ; //<S2SV> } //<S2SV> running = & fq -> flush_queue [ fq -> flush_running_idx ] ; //<S2SV> BUG_ON ( fq -> flush_pending_idx == fq -> flush_running_idx ) ; //<S2SV> fq -> flush_running_idx ^= 1 ; //<S2SV> if ( ! q -> mq_ops ) //<S2SV> elv_completed_request ( q , flush_rq ) ; //<S2SV> list_for_each_entry_safe ( rq , n , running , flush . list ) { //<S2SV> unsigned int seq = blk_flush_cur_seq ( rq ) ; //<S2SV> BUG_ON ( seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH ) ; //<S2SV> queued |= blk_flush_complete_seq ( rq , fq , seq , error ) ; //<S2SV> } //<S2SV> if ( queued || fq -> flush_queue_delayed ) { //<S2SV> WARN_ON ( q -> mq_ops ) ; //<S2SV> blk_run_queue_async ( q ) ; //<S2SV> } //<S2SV> fq -> flush_queue_delayed = 0 ; //<S2SV> if ( q -> mq_ops ) //<S2SV> spin_unlock_irqrestore ( & fq -> mq_flush_lock , flags ) ; //<S2SV> } //<S2SV> 