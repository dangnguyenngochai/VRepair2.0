int expand_downwards ( struct vm_area_struct * vma , //<S2SV> unsigned long address ) //<S2SV> { //<S2SV> struct mm_struct * mm = vma -> vm_mm ; //<S2SV> struct vm_area_struct * prev ; //<S2SV> int error = 0 ; //<S2SV> address &= PAGE_MASK ; //<S2SV> if ( address < mmap_min_addr ) //<S2SV> return - EPERM ; //<S2SV> prev = vma -> vm_prev ; //<S2SV> if ( prev && ! ( prev -> vm_flags & VM_GROWSDOWN ) && //<S2SV> ( prev -> vm_flags & ( VM_WRITE | VM_READ | VM_EXEC ) ) ) { //<S2SV> if ( address - prev -> vm_end < stack_guard_gap ) //<S2SV> return - ENOMEM ; //<S2SV> } //<S2SV> if ( unlikely ( anon_vma_prepare ( vma ) ) ) //<S2SV> return - ENOMEM ; //<S2SV> anon_vma_lock_write ( vma -> anon_vma ) ; //<S2SV> if ( address < vma -> vm_start ) { //<S2SV> unsigned long size , grow ; //<S2SV> size = vma -> vm_end - address ; //<S2SV> grow = ( vma -> vm_start - address ) >> PAGE_SHIFT ; //<S2SV> error = - ENOMEM ; //<S2SV> if ( grow <= vma -> vm_pgoff ) { //<S2SV> error = acct_stack_growth ( vma , size , grow ) ; //<S2SV> if ( ! error ) { //<S2SV> spin_lock ( & mm -> page_table_lock ) ; //<S2SV> if ( vma -> vm_flags & VM_LOCKED ) //<S2SV> mm -> locked_vm += grow ; //<S2SV> vm_stat_account ( mm , vma -> vm_flags , grow ) ; //<S2SV> anon_vma_interval_tree_pre_update_vma ( vma ) ; //<S2SV> vma -> vm_start = address ; //<S2SV> vma -> vm_pgoff -= grow ; //<S2SV> anon_vma_interval_tree_post_update_vma ( vma ) ; //<S2SV> vma_gap_update ( vma ) ; //<S2SV> spin_unlock ( & mm -> page_table_lock ) ; //<S2SV> perf_event_mmap ( vma ) ; //<S2SV> } //<S2SV> } //<S2SV> } //<S2SV> anon_vma_unlock_write ( vma -> anon_vma ) ; //<S2SV> khugepaged_enter_vma_merge ( vma , vma -> vm_flags ) ; //<S2SV> validate_mm ( mm ) ; //<S2SV> return error ; //<S2SV> } //<S2SV> 