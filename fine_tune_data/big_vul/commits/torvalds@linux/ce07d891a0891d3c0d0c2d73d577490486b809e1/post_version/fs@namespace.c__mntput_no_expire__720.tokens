static void mntput_no_expire ( struct mount * mnt ) //<S2SV> { //<S2SV> rcu_read_lock ( ) ; //<S2SV> mnt_add_count ( mnt , - 1 ) ; //<S2SV> if ( likely ( mnt -> mnt_ns ) ) { //<S2SV> rcu_read_unlock ( ) ; //<S2SV> return ; //<S2SV> } //<S2SV> lock_mount_hash ( ) ; //<S2SV> if ( mnt_get_count ( mnt ) ) { //<S2SV> rcu_read_unlock ( ) ; //<S2SV> unlock_mount_hash ( ) ; //<S2SV> return ; //<S2SV> } //<S2SV> if ( unlikely ( mnt -> mnt . mnt_flags & MNT_DOOMED ) ) { //<S2SV> rcu_read_unlock ( ) ; //<S2SV> unlock_mount_hash ( ) ; //<S2SV> return ; //<S2SV> } //<S2SV> mnt -> mnt . mnt_flags |= MNT_DOOMED ; //<S2SV> rcu_read_unlock ( ) ; //<S2SV> list_del ( & mnt -> mnt_instance ) ; //<S2SV> if ( unlikely ( ! list_empty ( & mnt -> mnt_mounts ) ) ) { //<S2SV> struct mount * p , * tmp ; //<S2SV> list_for_each_entry_safe ( p , tmp , & mnt -> mnt_mounts , mnt_child ) { //<S2SV> umount_mnt ( p ) ; //<S2SV> } //<S2SV> } //<S2SV> unlock_mount_hash ( ) ; //<S2SV> if ( likely ( ! ( mnt -> mnt . mnt_flags & MNT_INTERNAL ) ) ) { //<S2SV> struct task_struct * task = current ; //<S2SV> if ( likely ( ! ( task -> flags & PF_KTHREAD ) ) ) { //<S2SV> init_task_work ( & mnt -> mnt_rcu , __cleanup_mnt ) ; //<S2SV> if ( ! task_work_add ( task , & mnt -> mnt_rcu , true ) ) //<S2SV> return ; //<S2SV> } //<S2SV> if ( llist_add ( & mnt -> mnt_llist , & delayed_mntput_list ) ) //<S2SV> schedule_delayed_work ( & delayed_mntput_work , 1 ) ; //<S2SV> return ; //<S2SV> } //<S2SV> cleanup_mnt ( mnt ) ; //<S2SV> } //<S2SV> 