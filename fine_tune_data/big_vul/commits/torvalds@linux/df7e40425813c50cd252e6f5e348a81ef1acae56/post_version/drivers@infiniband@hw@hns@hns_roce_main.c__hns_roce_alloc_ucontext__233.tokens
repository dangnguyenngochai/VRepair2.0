static struct ib_ucontext * hns_roce_alloc_ucontext ( struct ib_device * ib_dev , //<S2SV> struct ib_udata * udata ) //<S2SV> { //<S2SV> int ret = 0 ; //<S2SV> struct hns_roce_ucontext * context ; //<S2SV> struct hns_roce_ib_alloc_ucontext_resp resp = { } ; //<S2SV> struct hns_roce_dev * hr_dev = to_hr_dev ( ib_dev ) ; //<S2SV> resp . qp_tab_size = hr_dev -> caps . num_qps ; //<S2SV> context = kmalloc ( sizeof ( * context ) , GFP_KERNEL ) ; //<S2SV> if ( ! context ) //<S2SV> return ERR_PTR ( - ENOMEM ) ; //<S2SV> ret = hns_roce_uar_alloc ( hr_dev , & context -> uar ) ; //<S2SV> if ( ret ) //<S2SV> goto error_fail_uar_alloc ; //<S2SV> if ( hr_dev -> caps . flags & HNS_ROCE_CAP_FLAG_RECORD_DB ) { //<S2SV> INIT_LIST_HEAD ( & context -> page_list ) ; //<S2SV> mutex_init ( & context -> page_mutex ) ; //<S2SV> } //<S2SV> ret = ib_copy_to_udata ( udata , & resp , sizeof ( resp ) ) ; //<S2SV> if ( ret ) //<S2SV> goto error_fail_copy_to_udata ; //<S2SV> return & context -> ibucontext ; //<S2SV> error_fail_copy_to_udata : //<S2SV> hns_roce_uar_free ( hr_dev , & context -> uar ) ; //<S2SV> error_fail_uar_alloc : //<S2SV> kfree ( context ) ; //<S2SV> return ERR_PTR ( ret ) ; //<S2SV> } //<S2SV> 