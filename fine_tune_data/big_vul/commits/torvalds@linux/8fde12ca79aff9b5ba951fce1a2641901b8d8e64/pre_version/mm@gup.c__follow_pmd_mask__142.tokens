static struct page * follow_pmd_mask ( struct vm_area_struct * vma , //<S2SV> unsigned long address , pud_t * pudp , //<S2SV> unsigned int flags , //<S2SV> struct follow_page_context * ctx ) //<S2SV> { //<S2SV> pmd_t * pmd , pmdval ; //<S2SV> spinlock_t * ptl ; //<S2SV> struct page * page ; //<S2SV> struct mm_struct * mm = vma -> vm_mm ; //<S2SV> pmd = pmd_offset ( pudp , address ) ; //<S2SV> pmdval = READ_ONCE ( * pmd ) ; //<S2SV> if ( pmd_none ( pmdval ) ) //<S2SV> return no_page_table ( vma , flags ) ; //<S2SV> if ( pmd_huge ( pmdval ) && vma -> vm_flags & VM_HUGETLB ) { //<S2SV> page = follow_huge_pmd ( mm , address , pmd , flags ) ; //<S2SV> if ( page ) //<S2SV> return page ; //<S2SV> return no_page_table ( vma , flags ) ; //<S2SV> } //<S2SV> if ( is_hugepd ( __hugepd ( pmd_val ( pmdval ) ) ) ) { //<S2SV> page = follow_huge_pd ( vma , address , //<S2SV> __hugepd ( pmd_val ( pmdval ) ) , flags , //<S2SV> PMD_SHIFT ) ; //<S2SV> if ( page ) //<S2SV> return page ; //<S2SV> return no_page_table ( vma , flags ) ; //<S2SV> } //<S2SV> retry : //<S2SV> if ( ! pmd_present ( pmdval ) ) { //<S2SV> if ( likely ( ! ( flags & FOLL_MIGRATION ) ) ) //<S2SV> return no_page_table ( vma , flags ) ; //<S2SV> VM_BUG_ON ( thp_migration_supported ( ) && //<S2SV> ! is_pmd_migration_entry ( pmdval ) ) ; //<S2SV> if ( is_pmd_migration_entry ( pmdval ) ) //<S2SV> pmd_migration_entry_wait ( mm , pmd ) ; //<S2SV> pmdval = READ_ONCE ( * pmd ) ; //<S2SV> if ( pmd_none ( pmdval ) ) //<S2SV> return no_page_table ( vma , flags ) ; //<S2SV> goto retry ; //<S2SV> } //<S2SV> if ( pmd_devmap ( pmdval ) ) { //<S2SV> ptl = pmd_lock ( mm , pmd ) ; //<S2SV> page = follow_devmap_pmd ( vma , address , pmd , flags , & ctx -> pgmap ) ; //<S2SV> spin_unlock ( ptl ) ; //<S2SV> if ( page ) //<S2SV> return page ; //<S2SV> } //<S2SV> if ( likely ( ! pmd_trans_huge ( pmdval ) ) ) //<S2SV> return follow_page_pte ( vma , address , pmd , flags , & ctx -> pgmap ) ; //<S2SV> if ( ( flags & FOLL_NUMA ) && pmd_protnone ( pmdval ) ) //<S2SV> return no_page_table ( vma , flags ) ; //<S2SV> retry_locked : //<S2SV> ptl = pmd_lock ( mm , pmd ) ; //<S2SV> if ( unlikely ( pmd_none ( * pmd ) ) ) { //<S2SV> spin_unlock ( ptl ) ; //<S2SV> return no_page_table ( vma , flags ) ; //<S2SV> } //<S2SV> if ( unlikely ( ! pmd_present ( * pmd ) ) ) { //<S2SV> spin_unlock ( ptl ) ; //<S2SV> if ( likely ( ! ( flags & FOLL_MIGRATION ) ) ) //<S2SV> return no_page_table ( vma , flags ) ; //<S2SV> pmd_migration_entry_wait ( mm , pmd ) ; //<S2SV> goto retry_locked ; //<S2SV> } //<S2SV> if ( unlikely ( ! pmd_trans_huge ( * pmd ) ) ) { //<S2SV> spin_unlock ( ptl ) ; //<S2SV> return follow_page_pte ( vma , address , pmd , flags , & ctx -> pgmap ) ; //<S2SV> } //<S2SV> if ( flags & FOLL_SPLIT ) { //<S2SV> int ret ; //<S2SV> page = pmd_page ( * pmd ) ; //<S2SV> if ( is_huge_zero_page ( page ) ) { //<S2SV> spin_unlock ( ptl ) ; //<S2SV> ret = 0 ; //<S2SV> split_huge_pmd ( vma , pmd , address ) ; //<S2SV> if ( pmd_trans_unstable ( pmd ) ) //<S2SV> ret = - EBUSY ; //<S2SV> } else { //<S2SV> get_page ( page ) ; //<S2SV> spin_unlock ( ptl ) ; //<S2SV> lock_page ( page ) ; //<S2SV> ret = split_huge_page ( page ) ; //<S2SV> unlock_page ( page ) ; //<S2SV> put_page ( page ) ; //<S2SV> if ( pmd_none ( * pmd ) ) //<S2SV> return no_page_table ( vma , flags ) ; //<S2SV> } //<S2SV> return ret ? ERR_PTR ( ret ) : //<S2SV> follow_page_pte ( vma , address , pmd , flags , & ctx -> pgmap ) ; //<S2SV> } //<S2SV> page = follow_trans_huge_pmd ( vma , address , pmd , flags ) ; //<S2SV> spin_unlock ( ptl ) ; //<S2SV> ctx -> page_mask = HPAGE_PMD_NR - 1 ; //<S2SV> return page ; //<S2SV> } //<S2SV> 