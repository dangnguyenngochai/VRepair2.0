static int prepare_vmcs02 ( struct kvm_vcpu * vcpu , struct vmcs12 * vmcs12 , //<S2SV> bool from_vmentry , u32 * entry_failure_code ) //<S2SV> { //<S2SV> struct vcpu_vmx * vmx = to_vmx ( vcpu ) ; //<S2SV> u32 exec_control , vmcs12_exec_ctrl ; //<S2SV> vmcs_write16 ( GUEST_ES_SELECTOR , vmcs12 -> guest_es_selector ) ; //<S2SV> vmcs_write16 ( GUEST_CS_SELECTOR , vmcs12 -> guest_cs_selector ) ; //<S2SV> vmcs_write16 ( GUEST_SS_SELECTOR , vmcs12 -> guest_ss_selector ) ; //<S2SV> vmcs_write16 ( GUEST_DS_SELECTOR , vmcs12 -> guest_ds_selector ) ; //<S2SV> vmcs_write16 ( GUEST_FS_SELECTOR , vmcs12 -> guest_fs_selector ) ; //<S2SV> vmcs_write16 ( GUEST_GS_SELECTOR , vmcs12 -> guest_gs_selector ) ; //<S2SV> vmcs_write16 ( GUEST_LDTR_SELECTOR , vmcs12 -> guest_ldtr_selector ) ; //<S2SV> vmcs_write16 ( GUEST_TR_SELECTOR , vmcs12 -> guest_tr_selector ) ; //<S2SV> vmcs_write32 ( GUEST_ES_LIMIT , vmcs12 -> guest_es_limit ) ; //<S2SV> vmcs_write32 ( GUEST_CS_LIMIT , vmcs12 -> guest_cs_limit ) ; //<S2SV> vmcs_write32 ( GUEST_SS_LIMIT , vmcs12 -> guest_ss_limit ) ; //<S2SV> vmcs_write32 ( GUEST_DS_LIMIT , vmcs12 -> guest_ds_limit ) ; //<S2SV> vmcs_write32 ( GUEST_FS_LIMIT , vmcs12 -> guest_fs_limit ) ; //<S2SV> vmcs_write32 ( GUEST_GS_LIMIT , vmcs12 -> guest_gs_limit ) ; //<S2SV> vmcs_write32 ( GUEST_LDTR_LIMIT , vmcs12 -> guest_ldtr_limit ) ; //<S2SV> vmcs_write32 ( GUEST_TR_LIMIT , vmcs12 -> guest_tr_limit ) ; //<S2SV> vmcs_write32 ( GUEST_GDTR_LIMIT , vmcs12 -> guest_gdtr_limit ) ; //<S2SV> vmcs_write32 ( GUEST_IDTR_LIMIT , vmcs12 -> guest_idtr_limit ) ; //<S2SV> vmcs_write32 ( GUEST_ES_AR_BYTES , vmcs12 -> guest_es_ar_bytes ) ; //<S2SV> vmcs_write32 ( GUEST_CS_AR_BYTES , vmcs12 -> guest_cs_ar_bytes ) ; //<S2SV> vmcs_write32 ( GUEST_SS_AR_BYTES , vmcs12 -> guest_ss_ar_bytes ) ; //<S2SV> vmcs_write32 ( GUEST_DS_AR_BYTES , vmcs12 -> guest_ds_ar_bytes ) ; //<S2SV> vmcs_write32 ( GUEST_FS_AR_BYTES , vmcs12 -> guest_fs_ar_bytes ) ; //<S2SV> vmcs_write32 ( GUEST_GS_AR_BYTES , vmcs12 -> guest_gs_ar_bytes ) ; //<S2SV> vmcs_write32 ( GUEST_LDTR_AR_BYTES , vmcs12 -> guest_ldtr_ar_bytes ) ; //<S2SV> vmcs_write32 ( GUEST_TR_AR_BYTES , vmcs12 -> guest_tr_ar_bytes ) ; //<S2SV> vmcs_writel ( GUEST_ES_BASE , vmcs12 -> guest_es_base ) ; //<S2SV> vmcs_writel ( GUEST_CS_BASE , vmcs12 -> guest_cs_base ) ; //<S2SV> vmcs_writel ( GUEST_SS_BASE , vmcs12 -> guest_ss_base ) ; //<S2SV> vmcs_writel ( GUEST_DS_BASE , vmcs12 -> guest_ds_base ) ; //<S2SV> vmcs_writel ( GUEST_FS_BASE , vmcs12 -> guest_fs_base ) ; //<S2SV> vmcs_writel ( GUEST_GS_BASE , vmcs12 -> guest_gs_base ) ; //<S2SV> vmcs_writel ( GUEST_LDTR_BASE , vmcs12 -> guest_ldtr_base ) ; //<S2SV> vmcs_writel ( GUEST_TR_BASE , vmcs12 -> guest_tr_base ) ; //<S2SV> vmcs_writel ( GUEST_GDTR_BASE , vmcs12 -> guest_gdtr_base ) ; //<S2SV> vmcs_writel ( GUEST_IDTR_BASE , vmcs12 -> guest_idtr_base ) ; //<S2SV> if ( from_vmentry && //<S2SV> ( vmcs12 -> vm_entry_controls & VM_ENTRY_LOAD_DEBUG_CONTROLS ) ) { //<S2SV> kvm_set_dr ( vcpu , 7 , vmcs12 -> guest_dr7 ) ; //<S2SV> vmcs_write64 ( GUEST_IA32_DEBUGCTL , vmcs12 -> guest_ia32_debugctl ) ; //<S2SV> } else { //<S2SV> kvm_set_dr ( vcpu , 7 , vcpu -> arch . dr7 ) ; //<S2SV> vmcs_write64 ( GUEST_IA32_DEBUGCTL , vmx -> nested . vmcs01_debugctl ) ; //<S2SV> } //<S2SV> if ( from_vmentry ) { //<S2SV> vmcs_write32 ( VM_ENTRY_INTR_INFO_FIELD , //<S2SV> vmcs12 -> vm_entry_intr_info_field ) ; //<S2SV> vmcs_write32 ( VM_ENTRY_EXCEPTION_ERROR_CODE , //<S2SV> vmcs12 -> vm_entry_exception_error_code ) ; //<S2SV> vmcs_write32 ( VM_ENTRY_INSTRUCTION_LEN , //<S2SV> vmcs12 -> vm_entry_instruction_len ) ; //<S2SV> vmcs_write32 ( GUEST_INTERRUPTIBILITY_INFO , //<S2SV> vmcs12 -> guest_interruptibility_info ) ; //<S2SV> vmx -> loaded_vmcs -> nmi_known_unmasked = //<S2SV> ! ( vmcs12 -> guest_interruptibility_info & GUEST_INTR_STATE_NMI ) ; //<S2SV> } else { //<S2SV> vmcs_write32 ( VM_ENTRY_INTR_INFO_FIELD , 0 ) ; //<S2SV> } //<S2SV> vmcs_write32 ( GUEST_SYSENTER_CS , vmcs12 -> guest_sysenter_cs ) ; //<S2SV> vmx_set_rflags ( vcpu , vmcs12 -> guest_rflags ) ; //<S2SV> vmcs_writel ( GUEST_PENDING_DBG_EXCEPTIONS , //<S2SV> vmcs12 -> guest_pending_dbg_exceptions ) ; //<S2SV> vmcs_writel ( GUEST_SYSENTER_ESP , vmcs12 -> guest_sysenter_esp ) ; //<S2SV> vmcs_writel ( GUEST_SYSENTER_EIP , vmcs12 -> guest_sysenter_eip ) ; //<S2SV> if ( nested_cpu_has_xsaves ( vmcs12 ) ) //<S2SV> vmcs_write64 ( XSS_EXIT_BITMAP , vmcs12 -> xss_exit_bitmap ) ; //<S2SV> vmcs_write64 ( VMCS_LINK_POINTER , - 1ull ) ; //<S2SV> exec_control = vmcs12 -> pin_based_vm_exec_control ; //<S2SV> exec_control &= ~ PIN_BASED_VMX_PREEMPTION_TIMER ; //<S2SV> exec_control |= vmcs_config . pin_based_exec_ctrl ; //<S2SV> if ( vmx -> hv_deadline_tsc == - 1 ) //<S2SV> exec_control &= ~ PIN_BASED_VMX_PREEMPTION_TIMER ; //<S2SV> if ( nested_cpu_has_posted_intr ( vmcs12 ) ) { //<S2SV> vmx -> nested . posted_intr_nv = vmcs12 -> posted_intr_nv ; //<S2SV> vmx -> nested . pi_pending = false ; //<S2SV> vmcs_write16 ( POSTED_INTR_NV , POSTED_INTR_NESTED_VECTOR ) ; //<S2SV> } else { //<S2SV> exec_control &= ~ PIN_BASED_POSTED_INTR ; //<S2SV> } //<S2SV> vmcs_write32 ( PIN_BASED_VM_EXEC_CONTROL , exec_control ) ; //<S2SV> vmx -> nested . preemption_timer_expired = false ; //<S2SV> if ( nested_cpu_has_preemption_timer ( vmcs12 ) ) //<S2SV> vmx_start_preemption_timer ( vcpu ) ; //<S2SV> vmcs_write32 ( PAGE_FAULT_ERROR_CODE_MASK , //<S2SV> enable_ept ? vmcs12 -> page_fault_error_code_mask : 0 ) ; //<S2SV> vmcs_write32 ( PAGE_FAULT_ERROR_CODE_MATCH , //<S2SV> enable_ept ? vmcs12 -> page_fault_error_code_match : 0 ) ; //<S2SV> if ( cpu_has_secondary_exec_ctrls ( ) ) { //<S2SV> exec_control = vmx -> secondary_exec_control ; //<S2SV> exec_control &= ~ ( SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES | //<S2SV> SECONDARY_EXEC_ENABLE_INVPCID | //<S2SV> SECONDARY_EXEC_RDTSCP | //<S2SV> SECONDARY_EXEC_XSAVES | //<S2SV> SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY | //<S2SV> SECONDARY_EXEC_APIC_REGISTER_VIRT | //<S2SV> SECONDARY_EXEC_ENABLE_VMFUNC ) ; //<S2SV> if ( nested_cpu_has ( vmcs12 , //<S2SV> CPU_BASED_ACTIVATE_SECONDARY_CONTROLS ) ) { //<S2SV> vmcs12_exec_ctrl = vmcs12 -> secondary_vm_exec_control & //<S2SV> ~ SECONDARY_EXEC_ENABLE_PML ; //<S2SV> exec_control |= vmcs12_exec_ctrl ; //<S2SV> } //<S2SV> if ( exec_control & SECONDARY_EXEC_ENABLE_VMFUNC ) //<S2SV> vmcs_write64 ( VM_FUNCTION_CONTROL , 0 ) ; //<S2SV> if ( exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY ) { //<S2SV> vmcs_write64 ( EOI_EXIT_BITMAP0 , //<S2SV> vmcs12 -> eoi_exit_bitmap0 ) ; //<S2SV> vmcs_write64 ( EOI_EXIT_BITMAP1 , //<S2SV> vmcs12 -> eoi_exit_bitmap1 ) ; //<S2SV> vmcs_write64 ( EOI_EXIT_BITMAP2 , //<S2SV> vmcs12 -> eoi_exit_bitmap2 ) ; //<S2SV> vmcs_write64 ( EOI_EXIT_BITMAP3 , //<S2SV> vmcs12 -> eoi_exit_bitmap3 ) ; //<S2SV> vmcs_write16 ( GUEST_INTR_STATUS , //<S2SV> vmcs12 -> guest_intr_status ) ; //<S2SV> } //<S2SV> if ( exec_control & SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES ) //<S2SV> vmcs_write64 ( APIC_ACCESS_ADDR , - 1ull ) ; //<S2SV> vmcs_write32 ( SECONDARY_VM_EXEC_CONTROL , exec_control ) ; //<S2SV> } //<S2SV> vmx_set_constant_host_state ( vmx ) ; //<S2SV> vmcs_write32 ( VM_EXIT_MSR_STORE_COUNT , 0 ) ; //<S2SV> vmcs_write32 ( VM_EXIT_MSR_LOAD_COUNT , vmx -> msr_autoload . nr ) ; //<S2SV> vmcs_write64 ( VM_EXIT_MSR_LOAD_ADDR , __pa ( vmx -> msr_autoload . host ) ) ; //<S2SV> vmcs_write32 ( VM_ENTRY_MSR_LOAD_COUNT , vmx -> msr_autoload . nr ) ; //<S2SV> vmcs_write64 ( VM_ENTRY_MSR_LOAD_ADDR , __pa ( vmx -> msr_autoload . guest ) ) ; //<S2SV> vmx -> host_rsp = 0 ; //<S2SV> exec_control = vmx_exec_control ( vmx ) ; //<S2SV> exec_control &= ~ CPU_BASED_VIRTUAL_INTR_PENDING ; //<S2SV> exec_control &= ~ CPU_BASED_VIRTUAL_NMI_PENDING ; //<S2SV> exec_control &= ~ CPU_BASED_TPR_SHADOW ; //<S2SV> exec_control |= vmcs12 -> cpu_based_vm_exec_control ; //<S2SV> if ( exec_control & CPU_BASED_TPR_SHADOW ) { //<S2SV> vmcs_write64 ( VIRTUAL_APIC_PAGE_ADDR , - 1ull ) ; //<S2SV> vmcs_write32 ( TPR_THRESHOLD , vmcs12 -> tpr_threshold ) ; //<S2SV> } //<S2SV> exec_control &= ~ CPU_BASED_USE_IO_BITMAPS ; //<S2SV> exec_control |= CPU_BASED_UNCOND_IO_EXITING ; //<S2SV> vmcs_write32 ( CPU_BASED_VM_EXEC_CONTROL , exec_control ) ; //<S2SV> update_exception_bitmap ( vcpu ) ; //<S2SV> vcpu -> arch . cr0_guest_owned_bits &= ~ vmcs12 -> cr0_guest_host_mask ; //<S2SV> vmcs_writel ( CR0_GUEST_HOST_MASK , ~ vcpu -> arch . cr0_guest_owned_bits ) ; //<S2SV> vmcs_write32 ( VM_EXIT_CONTROLS , vmcs_config . vmexit_ctrl ) ; //<S2SV> vm_entry_controls_init ( vmx , //<S2SV> ( vmcs12 -> vm_entry_controls & ~ VM_ENTRY_LOAD_IA32_EFER & //<S2SV> ~ VM_ENTRY_IA32E_MODE ) | //<S2SV> ( vmcs_config . vmentry_ctrl & ~ VM_ENTRY_IA32E_MODE ) ) ; //<S2SV> if ( from_vmentry && //<S2SV> ( vmcs12 -> vm_entry_controls & VM_ENTRY_LOAD_IA32_PAT ) ) { //<S2SV> vmcs_write64 ( GUEST_IA32_PAT , vmcs12 -> guest_ia32_pat ) ; //<S2SV> vcpu -> arch . pat = vmcs12 -> guest_ia32_pat ; //<S2SV> } else if ( vmcs_config . vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT ) { //<S2SV> vmcs_write64 ( GUEST_IA32_PAT , vmx -> vcpu . arch . pat ) ; //<S2SV> } //<S2SV> set_cr4_guest_host_mask ( vmx ) ; //<S2SV> if ( from_vmentry && //<S2SV> vmcs12 -> vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS ) //<S2SV> vmcs_write64 ( GUEST_BNDCFGS , vmcs12 -> guest_bndcfgs ) ; //<S2SV> if ( vmcs12 -> cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING ) //<S2SV> vmcs_write64 ( TSC_OFFSET , //<S2SV> vcpu -> arch . tsc_offset + vmcs12 -> tsc_offset ) ; //<S2SV> else //<S2SV> vmcs_write64 ( TSC_OFFSET , vcpu -> arch . tsc_offset ) ; //<S2SV> if ( kvm_has_tsc_control ) //<S2SV> decache_tsc_multiplier ( vmx ) ; //<S2SV> if ( enable_vpid ) { //<S2SV> if ( nested_cpu_has_vpid ( vmcs12 ) && vmx -> nested . vpid02 ) { //<S2SV> vmcs_write16 ( VIRTUAL_PROCESSOR_ID , vmx -> nested . vpid02 ) ; //<S2SV> if ( vmcs12 -> virtual_processor_id != vmx -> nested . last_vpid ) { //<S2SV> vmx -> nested . last_vpid = vmcs12 -> virtual_processor_id ; //<S2SV> __vmx_flush_tlb ( vcpu , to_vmx ( vcpu ) -> nested . vpid02 ) ; //<S2SV> } //<S2SV> } else { //<S2SV> vmcs_write16 ( VIRTUAL_PROCESSOR_ID , vmx -> vpid ) ; //<S2SV> vmx_flush_tlb ( vcpu ) ; //<S2SV> } //<S2SV> } //<S2SV> if ( enable_pml ) { //<S2SV> ASSERT ( vmx -> pml_pg ) ; //<S2SV> vmcs_write64 ( PML_ADDRESS , page_to_phys ( vmx -> pml_pg ) ) ; //<S2SV> vmcs_write16 ( GUEST_PML_INDEX , PML_ENTITY_NUM - 1 ) ; //<S2SV> } //<S2SV> if ( nested_cpu_has_ept ( vmcs12 ) ) { //<S2SV> if ( nested_ept_init_mmu_context ( vcpu ) ) { //<S2SV> * entry_failure_code = ENTRY_FAIL_DEFAULT ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> } else if ( nested_cpu_has2 ( vmcs12 , //<S2SV> SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES ) ) { //<S2SV> vmx_flush_tlb_ept_only ( vcpu ) ; //<S2SV> } //<S2SV> vmx_set_cr0 ( vcpu , vmcs12 -> guest_cr0 ) ; //<S2SV> vmcs_writel ( CR0_READ_SHADOW , nested_read_cr0 ( vmcs12 ) ) ; //<S2SV> vmx_set_cr4 ( vcpu , vmcs12 -> guest_cr4 ) ; //<S2SV> vmcs_writel ( CR4_READ_SHADOW , nested_read_cr4 ( vmcs12 ) ) ; //<S2SV> if ( from_vmentry && //<S2SV> ( vmcs12 -> vm_entry_controls & VM_ENTRY_LOAD_IA32_EFER ) ) //<S2SV> vcpu -> arch . efer = vmcs12 -> guest_ia32_efer ; //<S2SV> else if ( vmcs12 -> vm_entry_controls & VM_ENTRY_IA32E_MODE ) //<S2SV> vcpu -> arch . efer |= ( EFER_LMA | EFER_LME ) ; //<S2SV> else //<S2SV> vcpu -> arch . efer &= ~ ( EFER_LMA | EFER_LME ) ; //<S2SV> vmx_set_efer ( vcpu , vcpu -> arch . efer ) ; //<S2SV> if ( nested_vmx_load_cr3 ( vcpu , vmcs12 -> guest_cr3 , nested_cpu_has_ept ( vmcs12 ) , //<S2SV> entry_failure_code ) ) //<S2SV> return 1 ; //<S2SV> if ( ! enable_ept ) //<S2SV> vcpu -> arch . walk_mmu -> inject_page_fault = vmx_inject_page_fault_nested ; //<S2SV> if ( enable_ept ) { //<S2SV> vmcs_write64 ( GUEST_PDPTR0 , vmcs12 -> guest_pdptr0 ) ; //<S2SV> vmcs_write64 ( GUEST_PDPTR1 , vmcs12 -> guest_pdptr1 ) ; //<S2SV> vmcs_write64 ( GUEST_PDPTR2 , vmcs12 -> guest_pdptr2 ) ; //<S2SV> vmcs_write64 ( GUEST_PDPTR3 , vmcs12 -> guest_pdptr3 ) ; //<S2SV> } //<S2SV> kvm_register_write ( vcpu , VCPU_REGS_RSP , vmcs12 -> guest_rsp ) ; //<S2SV> kvm_register_write ( vcpu , VCPU_REGS_RIP , vmcs12 -> guest_rip ) ; //<S2SV> return 0 ; //<S2SV> } //<S2SV> 