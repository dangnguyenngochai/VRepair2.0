static void alpha_perf_event_irq_handler ( unsigned long la_ptr , //<S2SV> struct pt_regs * regs ) //<S2SV> { //<S2SV> struct cpu_hw_events * cpuc ; //<S2SV> struct perf_sample_data data ; //<S2SV> struct perf_event * event ; //<S2SV> struct hw_perf_event * hwc ; //<S2SV> int idx , j ; //<S2SV> __get_cpu_var ( irq_pmi_count ) ++ ; //<S2SV> cpuc = & __get_cpu_var ( cpu_hw_events ) ; //<S2SV> wrperfmon ( PERFMON_CMD_DISABLE , cpuc -> idx_mask ) ; //<S2SV> if ( unlikely ( la_ptr >= alpha_pmu -> num_pmcs ) ) { //<S2SV> irq_err_count ++ ; //<S2SV> pr_warning ( "PMI:<S2SV_blank>silly<S2SV_blank>index<S2SV_blank>%ld\\n" , la_ptr ) ; //<S2SV> wrperfmon ( PERFMON_CMD_ENABLE , cpuc -> idx_mask ) ; //<S2SV> return ; //<S2SV> } //<S2SV> idx = la_ptr ; //<S2SV> perf_sample_data_init ( & data , 0 ) ; //<S2SV> for ( j = 0 ; j < cpuc -> n_events ; j ++ ) { //<S2SV> if ( cpuc -> current_idx [ j ] == idx ) //<S2SV> break ; //<S2SV> } //<S2SV> if ( unlikely ( j == cpuc -> n_events ) ) { //<S2SV> wrperfmon ( PERFMON_CMD_ENABLE , cpuc -> idx_mask ) ; //<S2SV> return ; //<S2SV> } //<S2SV> event = cpuc -> event [ j ] ; //<S2SV> if ( unlikely ( ! event ) ) { //<S2SV> irq_err_count ++ ; //<S2SV> pr_warning ( "PMI:<S2SV_blank>No<S2SV_blank>event<S2SV_blank>at<S2SV_blank>index<S2SV_blank>%d!\\n" , idx ) ; //<S2SV> wrperfmon ( PERFMON_CMD_ENABLE , cpuc -> idx_mask ) ; //<S2SV> return ; //<S2SV> } //<S2SV> hwc = & event -> hw ; //<S2SV> alpha_perf_event_update ( event , hwc , idx , alpha_pmu -> pmc_max_period [ idx ] + 1 ) ; //<S2SV> data . period = event -> hw . last_period ; //<S2SV> if ( alpha_perf_event_set_period ( event , hwc , idx ) ) { //<S2SV> if ( perf_event_overflow ( event , & data , regs ) ) { //<S2SV> alpha_pmu_stop ( event , 0 ) ; //<S2SV> } //<S2SV> } //<S2SV> wrperfmon ( PERFMON_CMD_ENABLE , cpuc -> idx_mask ) ; //<S2SV> return ; //<S2SV> } //<S2SV> 