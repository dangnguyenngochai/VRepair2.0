static int handle_exception ( struct kvm_vcpu * vcpu ) //<S2SV> { //<S2SV> struct vcpu_vmx * vmx = to_vmx ( vcpu ) ; //<S2SV> struct kvm_run * kvm_run = vcpu -> run ; //<S2SV> u32 intr_info , ex_no , error_code ; //<S2SV> unsigned long cr2 , rip , dr6 ; //<S2SV> u32 vect_info ; //<S2SV> enum emulation_result er ; //<S2SV> vect_info = vmx -> idt_vectoring_info ; //<S2SV> intr_info = vmx -> exit_intr_info ; //<S2SV> if ( is_machine_check ( intr_info ) ) //<S2SV> return handle_machine_check ( vcpu ) ; //<S2SV> if ( ( intr_info & INTR_INFO_INTR_TYPE_MASK ) == INTR_TYPE_NMI_INTR ) //<S2SV> return 1 ; //<S2SV> if ( is_no_device ( intr_info ) ) { //<S2SV> vmx_fpu_activate ( vcpu ) ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> if ( is_invalid_opcode ( intr_info ) ) { //<S2SV> if ( is_guest_mode ( vcpu ) ) { //<S2SV> kvm_queue_exception ( vcpu , UD_VECTOR ) ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> er = emulate_instruction ( vcpu , EMULTYPE_TRAP_UD ) ; //<S2SV> if ( er != EMULATE_DONE ) //<S2SV> kvm_queue_exception ( vcpu , UD_VECTOR ) ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> error_code = 0 ; //<S2SV> if ( intr_info & INTR_INFO_DELIVER_CODE_MASK ) //<S2SV> error_code = vmcs_read32 ( VM_EXIT_INTR_ERROR_CODE ) ; //<S2SV> if ( ( vect_info & VECTORING_INFO_VALID_MASK ) && //<S2SV> ! ( is_page_fault ( intr_info ) && ! ( error_code & PFERR_RSVD_MASK ) ) ) { //<S2SV> vcpu -> run -> exit_reason = KVM_EXIT_INTERNAL_ERROR ; //<S2SV> vcpu -> run -> internal . suberror = KVM_INTERNAL_ERROR_SIMUL_EX ; //<S2SV> vcpu -> run -> internal . ndata = 3 ; //<S2SV> vcpu -> run -> internal . data [ 0 ] = vect_info ; //<S2SV> vcpu -> run -> internal . data [ 1 ] = intr_info ; //<S2SV> vcpu -> run -> internal . data [ 2 ] = error_code ; //<S2SV> return 0 ; //<S2SV> } //<S2SV> if ( is_page_fault ( intr_info ) ) { //<S2SV> BUG_ON ( enable_ept ) ; //<S2SV> cr2 = vmcs_readl ( EXIT_QUALIFICATION ) ; //<S2SV> trace_kvm_page_fault ( cr2 , error_code ) ; //<S2SV> if ( kvm_event_needs_reinjection ( vcpu ) ) //<S2SV> kvm_mmu_unprotect_page_virt ( vcpu , cr2 ) ; //<S2SV> return kvm_mmu_page_fault ( vcpu , cr2 , error_code , NULL , 0 ) ; //<S2SV> } //<S2SV> ex_no = intr_info & INTR_INFO_VECTOR_MASK ; //<S2SV> if ( vmx -> rmode . vm86_active && rmode_exception ( vcpu , ex_no ) ) //<S2SV> return handle_rmode_exception ( vcpu , ex_no , error_code ) ; //<S2SV> switch ( ex_no ) { //<S2SV> case AC_VECTOR : //<S2SV> kvm_queue_exception_e ( vcpu , AC_VECTOR , error_code ) ; //<S2SV> return 1 ; //<S2SV> case DB_VECTOR : //<S2SV> dr6 = vmcs_readl ( EXIT_QUALIFICATION ) ; //<S2SV> if ( ! ( vcpu -> guest_debug & //<S2SV> ( KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP ) ) ) { //<S2SV> vcpu -> arch . dr6 &= ~ 15 ; //<S2SV> vcpu -> arch . dr6 |= dr6 | DR6_RTM ; //<S2SV> if ( ! ( dr6 & ~ DR6_RESERVED ) ) //<S2SV> skip_emulated_instruction ( vcpu ) ; //<S2SV> kvm_queue_exception ( vcpu , DB_VECTOR ) ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> kvm_run -> debug . arch . dr6 = dr6 | DR6_FIXED_1 ; //<S2SV> kvm_run -> debug . arch . dr7 = vmcs_readl ( GUEST_DR7 ) ; //<S2SV> case BP_VECTOR : //<S2SV> vmx -> vcpu . arch . event_exit_inst_len = //<S2SV> vmcs_read32 ( VM_EXIT_INSTRUCTION_LEN ) ; //<S2SV> kvm_run -> exit_reason = KVM_EXIT_DEBUG ; //<S2SV> rip = kvm_rip_read ( vcpu ) ; //<S2SV> kvm_run -> debug . arch . pc = vmcs_readl ( GUEST_CS_BASE ) + rip ; //<S2SV> kvm_run -> debug . arch . exception = ex_no ; //<S2SV> break ; //<S2SV> default : //<S2SV> kvm_run -> exit_reason = KVM_EXIT_EXCEPTION ; //<S2SV> kvm_run -> ex . exception = ex_no ; //<S2SV> kvm_run -> ex . error_code = error_code ; //<S2SV> break ; //<S2SV> } //<S2SV> return 0 ; //<S2SV> } //<S2SV> 