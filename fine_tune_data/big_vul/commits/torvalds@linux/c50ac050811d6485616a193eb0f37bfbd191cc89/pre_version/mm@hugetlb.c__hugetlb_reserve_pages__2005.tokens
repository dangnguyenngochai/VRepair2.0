int hugetlb_reserve_pages ( struct inode * inode , //<S2SV> long from , long to , //<S2SV> struct vm_area_struct * vma , //<S2SV> vm_flags_t vm_flags ) //<S2SV> { //<S2SV> long ret , chg ; //<S2SV> struct hstate * h = hstate_inode ( inode ) ; //<S2SV> struct hugepage_subpool * spool = subpool_inode ( inode ) ; //<S2SV> if ( vm_flags & VM_NORESERVE ) //<S2SV> return 0 ; //<S2SV> if ( ! vma || vma -> vm_flags & VM_MAYSHARE ) //<S2SV> chg = region_chg ( & inode -> i_mapping -> private_list , from , to ) ; //<S2SV> else { //<S2SV> struct resv_map * resv_map = resv_map_alloc ( ) ; //<S2SV> if ( ! resv_map ) //<S2SV> return - ENOMEM ; //<S2SV> chg = to - from ; //<S2SV> set_vma_resv_map ( vma , resv_map ) ; //<S2SV> set_vma_resv_flags ( vma , HPAGE_RESV_OWNER ) ; //<S2SV> } //<S2SV> if ( chg < 0 ) //<S2SV> return chg ; //<S2SV> if ( hugepage_subpool_get_pages ( spool , chg ) ) //<S2SV> return - ENOSPC ; //<S2SV> ret = hugetlb_acct_memory ( h , chg ) ; //<S2SV> if ( ret < 0 ) { //<S2SV> hugepage_subpool_put_pages ( spool , chg ) ; //<S2SV> return ret ; //<S2SV> } //<S2SV> if ( ! vma || vma -> vm_flags & VM_MAYSHARE ) //<S2SV> region_add ( & inode -> i_mapping -> private_list , from , to ) ; //<S2SV> return 0 ; //<S2SV> } //<S2SV> 