static int em_sysenter ( struct x86_emulate_ctxt * ctxt ) //<S2SV> { //<S2SV> const struct x86_emulate_ops * ops = ctxt -> ops ; //<S2SV> struct desc_struct cs , ss ; //<S2SV> u64 msr_data ; //<S2SV> u16 cs_sel , ss_sel ; //<S2SV> u64 efer = 0 ; //<S2SV> ops -> get_msr ( ctxt , MSR_EFER , & efer ) ; //<S2SV> if ( ctxt -> mode == X86EMUL_MODE_REAL ) //<S2SV> return emulate_gp ( ctxt , 0 ) ; //<S2SV> if ( ( ctxt -> mode != X86EMUL_MODE_PROT64 ) && ( efer & EFER_LMA ) //<S2SV> && ! vendor_intel ( ctxt ) ) //<S2SV> return emulate_ud ( ctxt ) ; //<S2SV> if ( ctxt -> mode == X86EMUL_MODE_PROT64 ) //<S2SV> return X86EMUL_UNHANDLEABLE ; //<S2SV> setup_syscalls_segments ( ctxt , & cs , & ss ) ; //<S2SV> ops -> get_msr ( ctxt , MSR_IA32_SYSENTER_CS , & msr_data ) ; //<S2SV> if ( ( msr_data & 0xfffc ) == 0x0 ) //<S2SV> return emulate_gp ( ctxt , 0 ) ; //<S2SV> ctxt -> eflags &= ~ ( EFLG_VM | EFLG_IF ) ; //<S2SV> cs_sel = ( u16 ) msr_data & ~ SELECTOR_RPL_MASK ; //<S2SV> ss_sel = cs_sel + 8 ; //<S2SV> if ( efer & EFER_LMA ) { //<S2SV> cs . d = 0 ; //<S2SV> cs . l = 1 ; //<S2SV> } //<S2SV> ops -> set_segment ( ctxt , cs_sel , & cs , 0 , VCPU_SREG_CS ) ; //<S2SV> ops -> set_segment ( ctxt , ss_sel , & ss , 0 , VCPU_SREG_SS ) ; //<S2SV> ops -> get_msr ( ctxt , MSR_IA32_SYSENTER_EIP , & msr_data ) ; //<S2SV> ctxt -> _eip = ( efer & EFER_LMA ) ? msr_data : ( u32 ) msr_data ; //<S2SV> ops -> get_msr ( ctxt , MSR_IA32_SYSENTER_ESP , & msr_data ) ; //<S2SV> * reg_write ( ctxt , VCPU_REGS_RSP ) = ( efer & EFER_LMA ) ? msr_data : //<S2SV> ( u32 ) msr_data ; //<S2SV> return X86EMUL_CONTINUE ; //<S2SV> } //<S2SV> 