static int dcbnl_cee_fill ( struct sk_buff * skb , struct net_device * netdev ) //<S2SV> { //<S2SV> struct nlattr * cee , * app ; //<S2SV> struct dcb_app_type * itr ; //<S2SV> const struct dcbnl_rtnl_ops * ops = netdev -> dcbnl_ops ; //<S2SV> int dcbx , i , err = - EMSGSIZE ; //<S2SV> u8 value ; //<S2SV> if ( nla_put_string ( skb , DCB_ATTR_IFNAME , netdev -> name ) ) //<S2SV> goto nla_put_failure ; //<S2SV> cee = nla_nest_start ( skb , DCB_ATTR_CEE ) ; //<S2SV> if ( ! cee ) //<S2SV> goto nla_put_failure ; //<S2SV> if ( ops -> getpgtccfgtx && ops -> getpgbwgcfgtx ) { //<S2SV> err = dcbnl_cee_pg_fill ( skb , netdev , 1 ) ; //<S2SV> if ( err ) //<S2SV> goto nla_put_failure ; //<S2SV> } //<S2SV> if ( ops -> getpgtccfgrx && ops -> getpgbwgcfgrx ) { //<S2SV> err = dcbnl_cee_pg_fill ( skb , netdev , 0 ) ; //<S2SV> if ( err ) //<S2SV> goto nla_put_failure ; //<S2SV> } //<S2SV> if ( ops -> getpfccfg ) { //<S2SV> struct nlattr * pfc_nest = nla_nest_start ( skb , DCB_ATTR_CEE_PFC ) ; //<S2SV> if ( ! pfc_nest ) //<S2SV> goto nla_put_failure ; //<S2SV> for ( i = DCB_PFC_UP_ATTR_0 ; i <= DCB_PFC_UP_ATTR_7 ; i ++ ) { //<S2SV> ops -> getpfccfg ( netdev , i - DCB_PFC_UP_ATTR_0 , & value ) ; //<S2SV> if ( nla_put_u8 ( skb , i , value ) ) //<S2SV> goto nla_put_failure ; //<S2SV> } //<S2SV> nla_nest_end ( skb , pfc_nest ) ; //<S2SV> } //<S2SV> spin_lock ( & dcb_lock ) ; //<S2SV> app = nla_nest_start ( skb , DCB_ATTR_CEE_APP_TABLE ) ; //<S2SV> if ( ! app ) //<S2SV> goto dcb_unlock ; //<S2SV> list_for_each_entry ( itr , & dcb_app_list , list ) { //<S2SV> if ( itr -> ifindex == netdev -> ifindex ) { //<S2SV> struct nlattr * app_nest = nla_nest_start ( skb , //<S2SV> DCB_ATTR_APP ) ; //<S2SV> if ( ! app_nest ) //<S2SV> goto dcb_unlock ; //<S2SV> err = nla_put_u8 ( skb , DCB_APP_ATTR_IDTYPE , //<S2SV> itr -> app . selector ) ; //<S2SV> if ( err ) //<S2SV> goto dcb_unlock ; //<S2SV> err = nla_put_u16 ( skb , DCB_APP_ATTR_ID , //<S2SV> itr -> app . protocol ) ; //<S2SV> if ( err ) //<S2SV> goto dcb_unlock ; //<S2SV> err = nla_put_u8 ( skb , DCB_APP_ATTR_PRIORITY , //<S2SV> itr -> app . priority ) ; //<S2SV> if ( err ) //<S2SV> goto dcb_unlock ; //<S2SV> nla_nest_end ( skb , app_nest ) ; //<S2SV> } //<S2SV> } //<S2SV> nla_nest_end ( skb , app ) ; //<S2SV> if ( netdev -> dcbnl_ops -> getdcbx ) //<S2SV> dcbx = netdev -> dcbnl_ops -> getdcbx ( netdev ) ; //<S2SV> else //<S2SV> dcbx = - EOPNOTSUPP ; //<S2SV> spin_unlock ( & dcb_lock ) ; //<S2SV> if ( ops -> getfeatcfg ) { //<S2SV> struct nlattr * feat = nla_nest_start ( skb , DCB_ATTR_CEE_FEAT ) ; //<S2SV> if ( ! feat ) //<S2SV> goto nla_put_failure ; //<S2SV> for ( i = DCB_FEATCFG_ATTR_ALL + 1 ; i <= DCB_FEATCFG_ATTR_MAX ; //<S2SV> i ++ ) //<S2SV> if ( ! ops -> getfeatcfg ( netdev , i , & value ) && //<S2SV> nla_put_u8 ( skb , i , value ) ) //<S2SV> goto nla_put_failure ; //<S2SV> nla_nest_end ( skb , feat ) ; //<S2SV> } //<S2SV> if ( ops -> cee_peer_getpg ) { //<S2SV> struct cee_pg pg ; //<S2SV> err = ops -> cee_peer_getpg ( netdev , & pg ) ; //<S2SV> if ( ! err && //<S2SV> nla_put ( skb , DCB_ATTR_CEE_PEER_PG , sizeof ( pg ) , & pg ) ) //<S2SV> goto nla_put_failure ; //<S2SV> } //<S2SV> if ( ops -> cee_peer_getpfc ) { //<S2SV> struct cee_pfc pfc ; //<S2SV> err = ops -> cee_peer_getpfc ( netdev , & pfc ) ; //<S2SV> if ( ! err && //<S2SV> nla_put ( skb , DCB_ATTR_CEE_PEER_PFC , sizeof ( pfc ) , & pfc ) ) //<S2SV> goto nla_put_failure ; //<S2SV> } //<S2SV> if ( ops -> peer_getappinfo && ops -> peer_getapptable ) { //<S2SV> err = dcbnl_build_peer_app ( netdev , skb , //<S2SV> DCB_ATTR_CEE_PEER_APP_TABLE , //<S2SV> DCB_ATTR_CEE_PEER_APP_INFO , //<S2SV> DCB_ATTR_CEE_PEER_APP ) ; //<S2SV> if ( err ) //<S2SV> goto nla_put_failure ; //<S2SV> } //<S2SV> nla_nest_end ( skb , cee ) ; //<S2SV> if ( dcbx >= 0 ) { //<S2SV> err = nla_put_u8 ( skb , DCB_ATTR_DCBX , dcbx ) ; //<S2SV> if ( err ) //<S2SV> goto nla_put_failure ; //<S2SV> } //<S2SV> return 0 ; //<S2SV> dcb_unlock : //<S2SV> spin_unlock ( & dcb_lock ) ; //<S2SV> nla_put_failure : //<S2SV> return err ; //<S2SV> } //<S2SV> 