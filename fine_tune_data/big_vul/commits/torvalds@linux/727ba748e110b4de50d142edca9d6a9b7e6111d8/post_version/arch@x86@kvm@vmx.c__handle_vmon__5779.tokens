static int handle_vmon ( struct kvm_vcpu * vcpu ) //<S2SV> { //<S2SV> int ret ; //<S2SV> gpa_t vmptr ; //<S2SV> struct page * page ; //<S2SV> struct vcpu_vmx * vmx = to_vmx ( vcpu ) ; //<S2SV> const u64 VMXON_NEEDED_FEATURES = FEATURE_CONTROL_LOCKED //<S2SV> | FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX ; //<S2SV> if ( ! kvm_read_cr4_bits ( vcpu , X86_CR4_VMXE ) ) { //<S2SV> kvm_queue_exception ( vcpu , UD_VECTOR ) ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> if ( vmx_get_cpl ( vcpu ) ) { //<S2SV> kvm_queue_exception ( vcpu , UD_VECTOR ) ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> if ( vmx -> nested . vmxon ) { //<S2SV> nested_vmx_failValid ( vcpu , VMXERR_VMXON_IN_VMX_ROOT_OPERATION ) ; //<S2SV> return kvm_skip_emulated_instruction ( vcpu ) ; //<S2SV> } //<S2SV> if ( ( vmx -> msr_ia32_feature_control & VMXON_NEEDED_FEATURES ) //<S2SV> != VMXON_NEEDED_FEATURES ) { //<S2SV> kvm_inject_gp ( vcpu , 0 ) ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> if ( nested_vmx_get_vmptr ( vcpu , & vmptr ) ) //<S2SV> return 1 ; //<S2SV> if ( ! PAGE_ALIGNED ( vmptr ) || ( vmptr >> cpuid_maxphyaddr ( vcpu ) ) ) { //<S2SV> nested_vmx_failInvalid ( vcpu ) ; //<S2SV> return kvm_skip_emulated_instruction ( vcpu ) ; //<S2SV> } //<S2SV> page = kvm_vcpu_gpa_to_page ( vcpu , vmptr ) ; //<S2SV> if ( is_error_page ( page ) ) { //<S2SV> nested_vmx_failInvalid ( vcpu ) ; //<S2SV> return kvm_skip_emulated_instruction ( vcpu ) ; //<S2SV> } //<S2SV> if ( * ( u32 * ) kmap ( page ) != VMCS12_REVISION ) { //<S2SV> kunmap ( page ) ; //<S2SV> kvm_release_page_clean ( page ) ; //<S2SV> nested_vmx_failInvalid ( vcpu ) ; //<S2SV> return kvm_skip_emulated_instruction ( vcpu ) ; //<S2SV> } //<S2SV> kunmap ( page ) ; //<S2SV> kvm_release_page_clean ( page ) ; //<S2SV> vmx -> nested . vmxon_ptr = vmptr ; //<S2SV> ret = enter_vmx_operation ( vcpu ) ; //<S2SV> if ( ret ) //<S2SV> return ret ; //<S2SV> nested_vmx_succeed ( vcpu ) ; //<S2SV> return kvm_skip_emulated_instruction ( vcpu ) ; //<S2SV> } //<S2SV> 