static struct kioctx * ioctx_alloc ( unsigned nr_events ) //<S2SV> { //<S2SV> struct mm_struct * mm = current -> mm ; //<S2SV> struct kioctx * ctx ; //<S2SV> int err = - ENOMEM ; //<S2SV> nr_events = max ( nr_events , num_possible_cpus ( ) * 4 ) ; //<S2SV> nr_events *= 2 ; //<S2SV> if ( ( nr_events > ( 0x10000000U / sizeof ( struct io_event ) ) ) || //<S2SV> ( nr_events > ( 0x10000000U / sizeof ( struct kiocb ) ) ) ) { //<S2SV> pr_debug ( "ENOMEM:<S2SV_blank>nr_events<S2SV_blank>too<S2SV_blank>high\\n" ) ; //<S2SV> return ERR_PTR ( - EINVAL ) ; //<S2SV> } //<S2SV> if ( ! nr_events || ( unsigned long ) nr_events > ( aio_max_nr * 2UL ) ) //<S2SV> return ERR_PTR ( - EAGAIN ) ; //<S2SV> ctx = kmem_cache_zalloc ( kioctx_cachep , GFP_KERNEL ) ; //<S2SV> if ( ! ctx ) //<S2SV> return ERR_PTR ( - ENOMEM ) ; //<S2SV> ctx -> max_reqs = nr_events ; //<S2SV> if ( percpu_ref_init ( & ctx -> users , free_ioctx_users ) ) //<S2SV> goto err ; //<S2SV> if ( percpu_ref_init ( & ctx -> reqs , free_ioctx_reqs ) ) //<S2SV> goto err ; //<S2SV> spin_lock_init ( & ctx -> ctx_lock ) ; //<S2SV> spin_lock_init ( & ctx -> completion_lock ) ; //<S2SV> mutex_init ( & ctx -> ring_lock ) ; //<S2SV> init_waitqueue_head ( & ctx -> wait ) ; //<S2SV> INIT_LIST_HEAD ( & ctx -> active_reqs ) ; //<S2SV> ctx -> cpu = alloc_percpu ( struct kioctx_cpu ) ; //<S2SV> if ( ! ctx -> cpu ) //<S2SV> goto err ; //<S2SV> if ( aio_setup_ring ( ctx ) < 0 ) //<S2SV> goto err ; //<S2SV> atomic_set ( & ctx -> reqs_available , ctx -> nr_events - 1 ) ; //<S2SV> ctx -> req_batch = ( ctx -> nr_events - 1 ) / ( num_possible_cpus ( ) * 4 ) ; //<S2SV> if ( ctx -> req_batch < 1 ) //<S2SV> ctx -> req_batch = 1 ; //<S2SV> spin_lock ( & aio_nr_lock ) ; //<S2SV> if ( aio_nr + nr_events > ( aio_max_nr * 2UL ) || //<S2SV> aio_nr + nr_events < aio_nr ) { //<S2SV> spin_unlock ( & aio_nr_lock ) ; //<S2SV> err = - EAGAIN ; //<S2SV> goto err ; //<S2SV> } //<S2SV> aio_nr += ctx -> max_reqs ; //<S2SV> spin_unlock ( & aio_nr_lock ) ; //<S2SV> percpu_ref_get ( & ctx -> users ) ; //<S2SV> err = ioctx_add_table ( ctx , mm ) ; //<S2SV> if ( err ) //<S2SV> goto err_cleanup ; //<S2SV> pr_debug ( "allocated<S2SV_blank>ioctx<S2SV_blank>%p[%ld]:<S2SV_blank>mm=%p<S2SV_blank>mask=0x%x\\n" , //<S2SV> ctx , ctx -> user_id , mm , ctx -> nr_events ) ; //<S2SV> return ctx ; //<S2SV> err_cleanup : //<S2SV> aio_nr_sub ( ctx -> max_reqs ) ; //<S2SV> err : //<S2SV> aio_free_ring ( ctx ) ; //<S2SV> free_percpu ( ctx -> cpu ) ; //<S2SV> free_percpu ( ctx -> reqs . pcpu_count ) ; //<S2SV> free_percpu ( ctx -> users . pcpu_count ) ; //<S2SV> kmem_cache_free ( kioctx_cachep , ctx ) ; //<S2SV> pr_debug ( "error<S2SV_blank>allocating<S2SV_blank>ioctx<S2SV_blank>%d\\n" , err ) ; //<S2SV> return ERR_PTR ( err ) ; //<S2SV> } //<S2SV> 