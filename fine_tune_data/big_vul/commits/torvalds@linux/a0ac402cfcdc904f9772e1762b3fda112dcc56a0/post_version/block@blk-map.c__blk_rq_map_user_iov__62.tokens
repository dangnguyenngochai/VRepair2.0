int blk_rq_map_user_iov ( struct request_queue * q , struct request * rq , //<S2SV> struct rq_map_data * map_data , //<S2SV> const struct iov_iter * iter , gfp_t gfp_mask ) //<S2SV> { //<S2SV> bool copy = false ; //<S2SV> unsigned long align = q -> dma_pad_mask | queue_dma_alignment ( q ) ; //<S2SV> struct bio * bio = NULL ; //<S2SV> struct iov_iter i ; //<S2SV> int ret ; //<S2SV> if ( ! iter_is_iovec ( iter ) ) //<S2SV> goto fail ; //<S2SV> if ( map_data ) //<S2SV> copy = true ; //<S2SV> else if ( iov_iter_alignment ( iter ) & align ) //<S2SV> copy = true ; //<S2SV> else if ( queue_virt_boundary ( q ) ) //<S2SV> copy = queue_virt_boundary ( q ) & iov_iter_gap_alignment ( iter ) ; //<S2SV> i = * iter ; //<S2SV> do { //<S2SV> ret = __blk_rq_map_user_iov ( rq , map_data , & i , gfp_mask , copy ) ; //<S2SV> if ( ret ) //<S2SV> goto unmap_rq ; //<S2SV> if ( ! bio ) //<S2SV> bio = rq -> bio ; //<S2SV> } while ( iov_iter_count ( & i ) ) ; //<S2SV> if ( ! bio_flagged ( bio , BIO_USER_MAPPED ) ) //<S2SV> rq -> cmd_flags |= REQ_COPY_USER ; //<S2SV> return 0 ; //<S2SV> unmap_rq : //<S2SV> __blk_rq_unmap_user ( bio ) ; //<S2SV> fail : //<S2SV> rq -> bio = NULL ; //<S2SV> return - EINVAL ; //<S2SV> } //<S2SV> 