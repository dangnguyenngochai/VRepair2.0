static int ext4_fill_flex_info ( struct super_block * sb ) //<S2SV> { //<S2SV> struct ext4_sb_info * sbi = EXT4_SB ( sb ) ; //<S2SV> struct ext4_group_desc * gdp = NULL ; //<S2SV> ext4_group_t flex_group_count ; //<S2SV> ext4_group_t flex_group ; //<S2SV> int groups_per_flex = 0 ; //<S2SV> size_t size ; //<S2SV> int i ; //<S2SV> sbi -> s_log_groups_per_flex = sbi -> s_es -> s_log_groups_per_flex ; //<S2SV> groups_per_flex = 1 << sbi -> s_log_groups_per_flex ; //<S2SV> if ( groups_per_flex < 2 ) { //<S2SV> sbi -> s_log_groups_per_flex = 0 ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> flex_group_count = ( ( sbi -> s_groups_count + groups_per_flex - 1 ) + //<S2SV> ( ( le16_to_cpu ( sbi -> s_es -> s_reserved_gdt_blocks ) + 1 ) << //<S2SV> EXT4_DESC_PER_BLOCK_BITS ( sb ) ) ) / groups_per_flex ; //<S2SV> size = flex_group_count * sizeof ( struct flex_groups ) ; //<S2SV> sbi -> s_flex_groups = ext4_kvzalloc ( size , GFP_KERNEL ) ; //<S2SV> if ( sbi -> s_flex_groups == NULL ) { //<S2SV> ext4_msg ( sb , KERN_ERR , "not<S2SV_blank>enough<S2SV_blank>memory<S2SV_blank>for<S2SV_blank>%u<S2SV_blank>flex<S2SV_blank>groups" , //<S2SV> flex_group_count ) ; //<S2SV> goto failed ; //<S2SV> } //<S2SV> for ( i = 0 ; i < sbi -> s_groups_count ; i ++ ) { //<S2SV> gdp = ext4_get_group_desc ( sb , i , NULL ) ; //<S2SV> flex_group = ext4_flex_group ( sbi , i ) ; //<S2SV> atomic_add ( ext4_free_inodes_count ( sb , gdp ) , //<S2SV> & sbi -> s_flex_groups [ flex_group ] . free_inodes ) ; //<S2SV> atomic_add ( ext4_free_group_clusters ( sb , gdp ) , //<S2SV> & sbi -> s_flex_groups [ flex_group ] . free_clusters ) ; //<S2SV> atomic_add ( ext4_used_dirs_count ( sb , gdp ) , //<S2SV> & sbi -> s_flex_groups [ flex_group ] . used_dirs ) ; //<S2SV> } //<S2SV> return 1 ; //<S2SV> failed : //<S2SV> return 0 ; //<S2SV> } //<S2SV> 