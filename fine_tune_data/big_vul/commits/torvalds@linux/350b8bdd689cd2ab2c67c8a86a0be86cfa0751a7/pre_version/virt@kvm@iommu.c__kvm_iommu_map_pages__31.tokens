int kvm_iommu_map_pages ( struct kvm * kvm , struct kvm_memory_slot * slot ) //<S2SV> { //<S2SV> gfn_t gfn , end_gfn ; //<S2SV> pfn_t pfn ; //<S2SV> int r = 0 ; //<S2SV> struct iommu_domain * domain = kvm -> arch . iommu_domain ; //<S2SV> int flags ; //<S2SV> if ( ! domain ) //<S2SV> return 0 ; //<S2SV> gfn = slot -> base_gfn ; //<S2SV> end_gfn = gfn + slot -> npages ; //<S2SV> flags = IOMMU_READ ; //<S2SV> if ( ! ( slot -> flags & KVM_MEM_READONLY ) ) //<S2SV> flags |= IOMMU_WRITE ; //<S2SV> if ( ! kvm -> arch . iommu_noncoherent ) //<S2SV> flags |= IOMMU_CACHE ; //<S2SV> while ( gfn < end_gfn ) { //<S2SV> unsigned long page_size ; //<S2SV> if ( iommu_iova_to_phys ( domain , gfn_to_gpa ( gfn ) ) ) { //<S2SV> gfn += 1 ; //<S2SV> continue ; //<S2SV> } //<S2SV> page_size = kvm_host_page_size ( kvm , gfn ) ; //<S2SV> while ( ( gfn + ( page_size >> PAGE_SHIFT ) ) > end_gfn ) //<S2SV> page_size >>= 1 ; //<S2SV> while ( ( gfn << PAGE_SHIFT ) & ( page_size - 1 ) ) //<S2SV> page_size >>= 1 ; //<S2SV> while ( __gfn_to_hva_memslot ( slot , gfn ) & ( page_size - 1 ) ) //<S2SV> page_size >>= 1 ; //<S2SV> pfn = kvm_pin_pages ( slot , gfn , page_size ) ; //<S2SV> if ( is_error_noslot_pfn ( pfn ) ) { //<S2SV> gfn += 1 ; //<S2SV> continue ; //<S2SV> } //<S2SV> r = iommu_map ( domain , gfn_to_gpa ( gfn ) , pfn_to_hpa ( pfn ) , //<S2SV> page_size , flags ) ; //<S2SV> if ( r ) { //<S2SV> printk ( KERN_ERR "kvm_iommu_map_address:" //<S2SV> "iommu<S2SV_blank>failed<S2SV_blank>to<S2SV_blank>map<S2SV_blank>pfn=%llx\\n" , pfn ) ; //<S2SV> goto unmap_pages ; //<S2SV> } //<S2SV> gfn += page_size >> PAGE_SHIFT ; //<S2SV> } //<S2SV> return 0 ; //<S2SV> unmap_pages : //<S2SV> kvm_iommu_put_pages ( kvm , slot -> base_gfn , gfn ) ; //<S2SV> return r ; //<S2SV> } //<S2SV> 