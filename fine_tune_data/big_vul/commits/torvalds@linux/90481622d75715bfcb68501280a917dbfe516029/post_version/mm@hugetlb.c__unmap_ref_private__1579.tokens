static int unmap_ref_private ( struct mm_struct * mm , struct vm_area_struct * vma , //<S2SV> struct page * page , unsigned long address ) //<S2SV> { //<S2SV> struct hstate * h = hstate_vma ( vma ) ; //<S2SV> struct vm_area_struct * iter_vma ; //<S2SV> struct address_space * mapping ; //<S2SV> struct prio_tree_iter iter ; //<S2SV> pgoff_t pgoff ; //<S2SV> address = address & huge_page_mask ( h ) ; //<S2SV> pgoff = vma_hugecache_offset ( h , vma , address ) ; //<S2SV> mapping = vma -> vm_file -> f_dentry -> d_inode -> i_mapping ; //<S2SV> mutex_lock ( & mapping -> i_mmap_mutex ) ; //<S2SV> vma_prio_tree_foreach ( iter_vma , & iter , & mapping -> i_mmap , pgoff , pgoff ) { //<S2SV> if ( iter_vma == vma ) //<S2SV> continue ; //<S2SV> if ( ! is_vma_resv_set ( iter_vma , HPAGE_RESV_OWNER ) ) //<S2SV> __unmap_hugepage_range ( iter_vma , //<S2SV> address , address + huge_page_size ( h ) , //<S2SV> page ) ; //<S2SV> } //<S2SV> mutex_unlock ( & mapping -> i_mmap_mutex ) ; //<S2SV> return 1 ; //<S2SV> } //<S2SV> 