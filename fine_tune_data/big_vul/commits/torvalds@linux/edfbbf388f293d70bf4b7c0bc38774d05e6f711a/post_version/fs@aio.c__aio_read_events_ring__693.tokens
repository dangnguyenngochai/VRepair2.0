static long aio_read_events_ring ( struct kioctx * ctx , //<S2SV> struct io_event __user * event , long nr ) //<S2SV> { //<S2SV> struct aio_ring * ring ; //<S2SV> unsigned head , tail , pos ; //<S2SV> long ret = 0 ; //<S2SV> int copy_ret ; //<S2SV> mutex_lock ( & ctx -> ring_lock ) ; //<S2SV> ring = kmap_atomic ( ctx -> ring_pages [ 0 ] ) ; //<S2SV> head = ring -> head ; //<S2SV> tail = ring -> tail ; //<S2SV> kunmap_atomic ( ring ) ; //<S2SV> pr_debug ( "h%u<S2SV_blank>t%u<S2SV_blank>m%u\\n" , head , tail , ctx -> nr_events ) ; //<S2SV> if ( head == tail ) //<S2SV> goto out ; //<S2SV> head %= ctx -> nr_events ; //<S2SV> tail %= ctx -> nr_events ; //<S2SV> while ( ret < nr ) { //<S2SV> long avail ; //<S2SV> struct io_event * ev ; //<S2SV> struct page * page ; //<S2SV> avail = ( head <= tail ? tail : ctx -> nr_events ) - head ; //<S2SV> if ( head == tail ) //<S2SV> break ; //<S2SV> avail = min ( avail , nr - ret ) ; //<S2SV> avail = min_t ( long , avail , AIO_EVENTS_PER_PAGE - //<S2SV> ( ( head + AIO_EVENTS_OFFSET ) % AIO_EVENTS_PER_PAGE ) ) ; //<S2SV> pos = head + AIO_EVENTS_OFFSET ; //<S2SV> page = ctx -> ring_pages [ pos / AIO_EVENTS_PER_PAGE ] ; //<S2SV> pos %= AIO_EVENTS_PER_PAGE ; //<S2SV> ev = kmap ( page ) ; //<S2SV> copy_ret = copy_to_user ( event + ret , ev + pos , //<S2SV> sizeof ( * ev ) * avail ) ; //<S2SV> kunmap ( page ) ; //<S2SV> if ( unlikely ( copy_ret ) ) { //<S2SV> ret = - EFAULT ; //<S2SV> goto out ; //<S2SV> } //<S2SV> ret += avail ; //<S2SV> head += avail ; //<S2SV> head %= ctx -> nr_events ; //<S2SV> } //<S2SV> ring = kmap_atomic ( ctx -> ring_pages [ 0 ] ) ; //<S2SV> ring -> head = head ; //<S2SV> kunmap_atomic ( ring ) ; //<S2SV> flush_dcache_page ( ctx -> ring_pages [ 0 ] ) ; //<S2SV> pr_debug ( "%li<S2SV_blank><S2SV_blank>h%u<S2SV_blank>t%u\\n" , ret , head , tail ) ; //<S2SV> out : //<S2SV> mutex_unlock ( & ctx -> ring_lock ) ; //<S2SV> return ret ; //<S2SV> } //<S2SV> 