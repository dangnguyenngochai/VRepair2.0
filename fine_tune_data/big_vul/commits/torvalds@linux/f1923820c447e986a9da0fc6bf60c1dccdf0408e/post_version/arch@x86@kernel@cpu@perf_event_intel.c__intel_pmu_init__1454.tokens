__init int intel_pmu_init ( void ) //<S2SV> { //<S2SV> union cpuid10_edx edx ; //<S2SV> union cpuid10_eax eax ; //<S2SV> union cpuid10_ebx ebx ; //<S2SV> struct event_constraint * c ; //<S2SV> unsigned int unused ; //<S2SV> int version ; //<S2SV> if ( ! cpu_has ( & boot_cpu_data , X86_FEATURE_ARCH_PERFMON ) ) { //<S2SV> switch ( boot_cpu_data . x86 ) { //<S2SV> case 0x6 : //<S2SV> return p6_pmu_init ( ) ; //<S2SV> case 0xb : //<S2SV> return knc_pmu_init ( ) ; //<S2SV> case 0xf : //<S2SV> return p4_pmu_init ( ) ; //<S2SV> } //<S2SV> return - ENODEV ; //<S2SV> } //<S2SV> cpuid ( 10 , & eax . full , & ebx . full , & unused , & edx . full ) ; //<S2SV> if ( eax . split . mask_length < ARCH_PERFMON_EVENTS_COUNT ) //<S2SV> return - ENODEV ; //<S2SV> version = eax . split . version_id ; //<S2SV> if ( version < 2 ) //<S2SV> x86_pmu = core_pmu ; //<S2SV> else //<S2SV> x86_pmu = intel_pmu ; //<S2SV> x86_pmu . version = version ; //<S2SV> x86_pmu . num_counters = eax . split . num_counters ; //<S2SV> x86_pmu . cntval_bits = eax . split . bit_width ; //<S2SV> x86_pmu . cntval_mask = ( 1ULL << eax . split . bit_width ) - 1 ; //<S2SV> x86_pmu . events_maskl = ebx . full ; //<S2SV> x86_pmu . events_mask_len = eax . split . mask_length ; //<S2SV> x86_pmu . max_pebs_events = min_t ( unsigned , MAX_PEBS_EVENTS , x86_pmu . num_counters ) ; //<S2SV> if ( version > 1 ) //<S2SV> x86_pmu . num_counters_fixed = max ( ( int ) edx . split . num_counters_fixed , 3 ) ; //<S2SV> if ( version > 1 ) { //<S2SV> u64 capabilities ; //<S2SV> rdmsrl ( MSR_IA32_PERF_CAPABILITIES , capabilities ) ; //<S2SV> x86_pmu . intel_cap . capabilities = capabilities ; //<S2SV> } //<S2SV> intel_ds_init ( ) ; //<S2SV> x86_add_quirk ( intel_arch_events_quirk ) ; //<S2SV> switch ( boot_cpu_data . x86_model ) { //<S2SV> case 14 : //<S2SV> pr_cont ( "Core<S2SV_blank>events,<S2SV_blank>" ) ; //<S2SV> break ; //<S2SV> case 15 : //<S2SV> x86_add_quirk ( intel_clovertown_quirk ) ; //<S2SV> case 22 : //<S2SV> case 23 : //<S2SV> case 29 : //<S2SV> memcpy ( hw_cache_event_ids , core2_hw_cache_event_ids , //<S2SV> sizeof ( hw_cache_event_ids ) ) ; //<S2SV> intel_pmu_lbr_init_core ( ) ; //<S2SV> x86_pmu . event_constraints = intel_core2_event_constraints ; //<S2SV> x86_pmu . pebs_constraints = intel_core2_pebs_event_constraints ; //<S2SV> pr_cont ( "Core2<S2SV_blank>events,<S2SV_blank>" ) ; //<S2SV> break ; //<S2SV> case 26 : //<S2SV> case 30 : //<S2SV> case 46 : //<S2SV> memcpy ( hw_cache_event_ids , nehalem_hw_cache_event_ids , //<S2SV> sizeof ( hw_cache_event_ids ) ) ; //<S2SV> memcpy ( hw_cache_extra_regs , nehalem_hw_cache_extra_regs , //<S2SV> sizeof ( hw_cache_extra_regs ) ) ; //<S2SV> intel_pmu_lbr_init_nhm ( ) ; //<S2SV> x86_pmu . event_constraints = intel_nehalem_event_constraints ; //<S2SV> x86_pmu . pebs_constraints = intel_nehalem_pebs_event_constraints ; //<S2SV> x86_pmu . enable_all = intel_pmu_nhm_enable_all ; //<S2SV> x86_pmu . extra_regs = intel_nehalem_extra_regs ; //<S2SV> intel_perfmon_event_map [ PERF_COUNT_HW_STALLED_CYCLES_FRONTEND ] = //<S2SV> X86_CONFIG ( . event = 0x0e , . umask = 0x01 , . inv = 1 , . cmask = 1 ) ; //<S2SV> intel_perfmon_event_map [ PERF_COUNT_HW_STALLED_CYCLES_BACKEND ] = //<S2SV> X86_CONFIG ( . event = 0xb1 , . umask = 0x3f , . inv = 1 , . cmask = 1 ) ; //<S2SV> x86_add_quirk ( intel_nehalem_quirk ) ; //<S2SV> pr_cont ( "Nehalem<S2SV_blank>events,<S2SV_blank>" ) ; //<S2SV> break ; //<S2SV> case 28 : //<S2SV> case 38 : //<S2SV> case 39 : //<S2SV> case 53 : //<S2SV> case 54 : //<S2SV> memcpy ( hw_cache_event_ids , atom_hw_cache_event_ids , //<S2SV> sizeof ( hw_cache_event_ids ) ) ; //<S2SV> intel_pmu_lbr_init_atom ( ) ; //<S2SV> x86_pmu . event_constraints = intel_gen_event_constraints ; //<S2SV> x86_pmu . pebs_constraints = intel_atom_pebs_event_constraints ; //<S2SV> pr_cont ( "Atom<S2SV_blank>events,<S2SV_blank>" ) ; //<S2SV> break ; //<S2SV> case 37 : //<S2SV> case 44 : //<S2SV> case 47 : //<S2SV> memcpy ( hw_cache_event_ids , westmere_hw_cache_event_ids , //<S2SV> sizeof ( hw_cache_event_ids ) ) ; //<S2SV> memcpy ( hw_cache_extra_regs , nehalem_hw_cache_extra_regs , //<S2SV> sizeof ( hw_cache_extra_regs ) ) ; //<S2SV> intel_pmu_lbr_init_nhm ( ) ; //<S2SV> x86_pmu . event_constraints = intel_westmere_event_constraints ; //<S2SV> x86_pmu . enable_all = intel_pmu_nhm_enable_all ; //<S2SV> x86_pmu . pebs_constraints = intel_westmere_pebs_event_constraints ; //<S2SV> x86_pmu . extra_regs = intel_westmere_extra_regs ; //<S2SV> x86_pmu . er_flags |= ERF_HAS_RSP_1 ; //<S2SV> intel_perfmon_event_map [ PERF_COUNT_HW_STALLED_CYCLES_FRONTEND ] = //<S2SV> X86_CONFIG ( . event = 0x0e , . umask = 0x01 , . inv = 1 , . cmask = 1 ) ; //<S2SV> intel_perfmon_event_map [ PERF_COUNT_HW_STALLED_CYCLES_BACKEND ] = //<S2SV> X86_CONFIG ( . event = 0xb1 , . umask = 0x3f , . inv = 1 , . cmask = 1 ) ; //<S2SV> pr_cont ( "Westmere<S2SV_blank>events,<S2SV_blank>" ) ; //<S2SV> break ; //<S2SV> case 42 : //<S2SV> case 45 : //<S2SV> x86_add_quirk ( intel_sandybridge_quirk ) ; //<S2SV> memcpy ( hw_cache_event_ids , snb_hw_cache_event_ids , //<S2SV> sizeof ( hw_cache_event_ids ) ) ; //<S2SV> memcpy ( hw_cache_extra_regs , snb_hw_cache_extra_regs , //<S2SV> sizeof ( hw_cache_extra_regs ) ) ; //<S2SV> intel_pmu_lbr_init_snb ( ) ; //<S2SV> x86_pmu . event_constraints = intel_snb_event_constraints ; //<S2SV> x86_pmu . pebs_constraints = intel_snb_pebs_event_constraints ; //<S2SV> x86_pmu . pebs_aliases = intel_pebs_aliases_snb ; //<S2SV> if ( boot_cpu_data . x86_model == 45 ) //<S2SV> x86_pmu . extra_regs = intel_snbep_extra_regs ; //<S2SV> else //<S2SV> x86_pmu . extra_regs = intel_snb_extra_regs ; //<S2SV> x86_pmu . er_flags |= ERF_HAS_RSP_1 ; //<S2SV> x86_pmu . er_flags |= ERF_NO_HT_SHARING ; //<S2SV> intel_perfmon_event_map [ PERF_COUNT_HW_STALLED_CYCLES_FRONTEND ] = //<S2SV> X86_CONFIG ( . event = 0x0e , . umask = 0x01 , . inv = 1 , . cmask = 1 ) ; //<S2SV> intel_perfmon_event_map [ PERF_COUNT_HW_STALLED_CYCLES_BACKEND ] = //<S2SV> X86_CONFIG ( . event = 0xb1 , . umask = 0x01 , . inv = 1 , . cmask = 1 ) ; //<S2SV> pr_cont ( "SandyBridge<S2SV_blank>events,<S2SV_blank>" ) ; //<S2SV> break ; //<S2SV> case 58 : //<S2SV> case 62 : //<S2SV> memcpy ( hw_cache_event_ids , snb_hw_cache_event_ids , //<S2SV> sizeof ( hw_cache_event_ids ) ) ; //<S2SV> memcpy ( hw_cache_extra_regs , snb_hw_cache_extra_regs , //<S2SV> sizeof ( hw_cache_extra_regs ) ) ; //<S2SV> intel_pmu_lbr_init_snb ( ) ; //<S2SV> x86_pmu . event_constraints = intel_ivb_event_constraints ; //<S2SV> x86_pmu . pebs_constraints = intel_ivb_pebs_event_constraints ; //<S2SV> x86_pmu . pebs_aliases = intel_pebs_aliases_snb ; //<S2SV> if ( boot_cpu_data . x86_model == 62 ) //<S2SV> x86_pmu . extra_regs = intel_snbep_extra_regs ; //<S2SV> else //<S2SV> x86_pmu . extra_regs = intel_snb_extra_regs ; //<S2SV> x86_pmu . er_flags |= ERF_HAS_RSP_1 ; //<S2SV> x86_pmu . er_flags |= ERF_NO_HT_SHARING ; //<S2SV> intel_perfmon_event_map [ PERF_COUNT_HW_STALLED_CYCLES_FRONTEND ] = //<S2SV> X86_CONFIG ( . event = 0x0e , . umask = 0x01 , . inv = 1 , . cmask = 1 ) ; //<S2SV> pr_cont ( "IvyBridge<S2SV_blank>events,<S2SV_blank>" ) ; //<S2SV> break ; //<S2SV> default : //<S2SV> switch ( x86_pmu . version ) { //<S2SV> case 1 : //<S2SV> x86_pmu . event_constraints = intel_v1_event_constraints ; //<S2SV> pr_cont ( "generic<S2SV_blank>architected<S2SV_blank>perfmon<S2SV_blank>v1,<S2SV_blank>" ) ; //<S2SV> break ; //<S2SV> default : //<S2SV> x86_pmu . event_constraints = intel_gen_event_constraints ; //<S2SV> pr_cont ( "generic<S2SV_blank>architected<S2SV_blank>perfmon,<S2SV_blank>" ) ; //<S2SV> break ; //<S2SV> } //<S2SV> } //<S2SV> if ( x86_pmu . num_counters > INTEL_PMC_MAX_GENERIC ) { //<S2SV> WARN ( 1 , KERN_ERR "hw<S2SV_blank>perf<S2SV_blank>events<S2SV_blank>%d<S2SV_blank>><S2SV_blank>max(%d),<S2SV_blank>clipping!" , //<S2SV> x86_pmu . num_counters , INTEL_PMC_MAX_GENERIC ) ; //<S2SV> x86_pmu . num_counters = INTEL_PMC_MAX_GENERIC ; //<S2SV> } //<S2SV> x86_pmu . intel_ctrl = ( 1 << x86_pmu . num_counters ) - 1 ; //<S2SV> if ( x86_pmu . num_counters_fixed > INTEL_PMC_MAX_FIXED ) { //<S2SV> WARN ( 1 , KERN_ERR "hw<S2SV_blank>perf<S2SV_blank>events<S2SV_blank>fixed<S2SV_blank>%d<S2SV_blank>><S2SV_blank>max(%d),<S2SV_blank>clipping!" , //<S2SV> x86_pmu . num_counters_fixed , INTEL_PMC_MAX_FIXED ) ; //<S2SV> x86_pmu . num_counters_fixed = INTEL_PMC_MAX_FIXED ; //<S2SV> } //<S2SV> x86_pmu . intel_ctrl |= //<S2SV> ( ( 1LL << x86_pmu . num_counters_fixed ) - 1 ) << INTEL_PMC_IDX_FIXED ; //<S2SV> if ( x86_pmu . event_constraints ) { //<S2SV> for_each_event_constraint ( c , x86_pmu . event_constraints ) { //<S2SV> if ( c -> cmask != X86_RAW_EVENT_MASK //<S2SV> || c -> idxmsk64 == INTEL_PMC_MSK_FIXED_REF_CYCLES ) { //<S2SV> continue ; //<S2SV> } //<S2SV> c -> idxmsk64 |= ( 1ULL << x86_pmu . num_counters ) - 1 ; //<S2SV> c -> weight += x86_pmu . num_counters ; //<S2SV> } //<S2SV> } //<S2SV> return 0 ; //<S2SV> } //<S2SV> 