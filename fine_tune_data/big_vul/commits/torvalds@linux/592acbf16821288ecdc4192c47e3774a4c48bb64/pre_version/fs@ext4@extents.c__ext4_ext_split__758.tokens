static int ext4_ext_split ( handle_t * handle , struct inode * inode , //<S2SV> unsigned int flags , //<S2SV> struct ext4_ext_path * path , //<S2SV> struct ext4_extent * newext , int at ) //<S2SV> { //<S2SV> struct buffer_head * bh = NULL ; //<S2SV> int depth = ext_depth ( inode ) ; //<S2SV> struct ext4_extent_header * neh ; //<S2SV> struct ext4_extent_idx * fidx ; //<S2SV> int i = at , k , m , a ; //<S2SV> ext4_fsblk_t newblock , oldblock ; //<S2SV> __le32 border ; //<S2SV> ext4_fsblk_t * ablocks = NULL ; //<S2SV> int err = 0 ; //<S2SV> if ( unlikely ( path [ depth ] . p_ext > EXT_MAX_EXTENT ( path [ depth ] . p_hdr ) ) ) { //<S2SV> EXT4_ERROR_INODE ( inode , "p_ext<S2SV_blank>><S2SV_blank>EXT_MAX_EXTENT!" ) ; //<S2SV> return - EFSCORRUPTED ; //<S2SV> } //<S2SV> if ( path [ depth ] . p_ext != EXT_MAX_EXTENT ( path [ depth ] . p_hdr ) ) { //<S2SV> border = path [ depth ] . p_ext [ 1 ] . ee_block ; //<S2SV> ext_debug ( "leaf<S2SV_blank>will<S2SV_blank>be<S2SV_blank>split." //<S2SV> "<S2SV_blank>next<S2SV_blank>leaf<S2SV_blank>starts<S2SV_blank>at<S2SV_blank>%d\\n" , //<S2SV> le32_to_cpu ( border ) ) ; //<S2SV> } else { //<S2SV> border = newext -> ee_block ; //<S2SV> ext_debug ( "leaf<S2SV_blank>will<S2SV_blank>be<S2SV_blank>added." //<S2SV> "<S2SV_blank>next<S2SV_blank>leaf<S2SV_blank>starts<S2SV_blank>at<S2SV_blank>%d\\n" , //<S2SV> le32_to_cpu ( border ) ) ; //<S2SV> } //<S2SV> ablocks = kcalloc ( depth , sizeof ( ext4_fsblk_t ) , GFP_NOFS ) ; //<S2SV> if ( ! ablocks ) //<S2SV> return - ENOMEM ; //<S2SV> ext_debug ( "allocate<S2SV_blank>%d<S2SV_blank>blocks<S2SV_blank>for<S2SV_blank>indexes/leaf\\n" , depth - at ) ; //<S2SV> for ( a = 0 ; a < depth - at ; a ++ ) { //<S2SV> newblock = ext4_ext_new_meta_block ( handle , inode , path , //<S2SV> newext , & err , flags ) ; //<S2SV> if ( newblock == 0 ) //<S2SV> goto cleanup ; //<S2SV> ablocks [ a ] = newblock ; //<S2SV> } //<S2SV> newblock = ablocks [ -- a ] ; //<S2SV> if ( unlikely ( newblock == 0 ) ) { //<S2SV> EXT4_ERROR_INODE ( inode , "newblock<S2SV_blank>==<S2SV_blank>0!" ) ; //<S2SV> err = - EFSCORRUPTED ; //<S2SV> goto cleanup ; //<S2SV> } //<S2SV> bh = sb_getblk_gfp ( inode -> i_sb , newblock , __GFP_MOVABLE | GFP_NOFS ) ; //<S2SV> if ( unlikely ( ! bh ) ) { //<S2SV> err = - ENOMEM ; //<S2SV> goto cleanup ; //<S2SV> } //<S2SV> lock_buffer ( bh ) ; //<S2SV> err = ext4_journal_get_create_access ( handle , bh ) ; //<S2SV> if ( err ) //<S2SV> goto cleanup ; //<S2SV> neh = ext_block_hdr ( bh ) ; //<S2SV> neh -> eh_entries = 0 ; //<S2SV> neh -> eh_max = cpu_to_le16 ( ext4_ext_space_block ( inode , 0 ) ) ; //<S2SV> neh -> eh_magic = EXT4_EXT_MAGIC ; //<S2SV> neh -> eh_depth = 0 ; //<S2SV> if ( unlikely ( path [ depth ] . p_hdr -> eh_entries != //<S2SV> path [ depth ] . p_hdr -> eh_max ) ) { //<S2SV> EXT4_ERROR_INODE ( inode , "eh_entries<S2SV_blank>%d<S2SV_blank>!=<S2SV_blank>eh_max<S2SV_blank>%d!" , //<S2SV> path [ depth ] . p_hdr -> eh_entries , //<S2SV> path [ depth ] . p_hdr -> eh_max ) ; //<S2SV> err = - EFSCORRUPTED ; //<S2SV> goto cleanup ; //<S2SV> } //<S2SV> m = EXT_MAX_EXTENT ( path [ depth ] . p_hdr ) - path [ depth ] . p_ext ++ ; //<S2SV> ext4_ext_show_move ( inode , path , newblock , depth ) ; //<S2SV> if ( m ) { //<S2SV> struct ext4_extent * ex ; //<S2SV> ex = EXT_FIRST_EXTENT ( neh ) ; //<S2SV> memmove ( ex , path [ depth ] . p_ext , sizeof ( struct ext4_extent ) * m ) ; //<S2SV> le16_add_cpu ( & neh -> eh_entries , m ) ; //<S2SV> } //<S2SV> ext4_extent_block_csum_set ( inode , neh ) ; //<S2SV> set_buffer_uptodate ( bh ) ; //<S2SV> unlock_buffer ( bh ) ; //<S2SV> err = ext4_handle_dirty_metadata ( handle , inode , bh ) ; //<S2SV> if ( err ) //<S2SV> goto cleanup ; //<S2SV> brelse ( bh ) ; //<S2SV> bh = NULL ; //<S2SV> if ( m ) { //<S2SV> err = ext4_ext_get_access ( handle , inode , path + depth ) ; //<S2SV> if ( err ) //<S2SV> goto cleanup ; //<S2SV> le16_add_cpu ( & path [ depth ] . p_hdr -> eh_entries , - m ) ; //<S2SV> err = ext4_ext_dirty ( handle , inode , path + depth ) ; //<S2SV> if ( err ) //<S2SV> goto cleanup ; //<S2SV> } //<S2SV> k = depth - at - 1 ; //<S2SV> if ( unlikely ( k < 0 ) ) { //<S2SV> EXT4_ERROR_INODE ( inode , "k<S2SV_blank>%d<S2SV_blank><<S2SV_blank>0!" , k ) ; //<S2SV> err = - EFSCORRUPTED ; //<S2SV> goto cleanup ; //<S2SV> } //<S2SV> if ( k ) //<S2SV> ext_debug ( "create<S2SV_blank>%d<S2SV_blank>intermediate<S2SV_blank>indices\\n" , k ) ; //<S2SV> i = depth - 1 ; //<S2SV> while ( k -- ) { //<S2SV> oldblock = newblock ; //<S2SV> newblock = ablocks [ -- a ] ; //<S2SV> bh = sb_getblk ( inode -> i_sb , newblock ) ; //<S2SV> if ( unlikely ( ! bh ) ) { //<S2SV> err = - ENOMEM ; //<S2SV> goto cleanup ; //<S2SV> } //<S2SV> lock_buffer ( bh ) ; //<S2SV> err = ext4_journal_get_create_access ( handle , bh ) ; //<S2SV> if ( err ) //<S2SV> goto cleanup ; //<S2SV> neh = ext_block_hdr ( bh ) ; //<S2SV> neh -> eh_entries = cpu_to_le16 ( 1 ) ; //<S2SV> neh -> eh_magic = EXT4_EXT_MAGIC ; //<S2SV> neh -> eh_max = cpu_to_le16 ( ext4_ext_space_block_idx ( inode , 0 ) ) ; //<S2SV> neh -> eh_depth = cpu_to_le16 ( depth - i ) ; //<S2SV> fidx = EXT_FIRST_INDEX ( neh ) ; //<S2SV> fidx -> ei_block = border ; //<S2SV> ext4_idx_store_pblock ( fidx , oldblock ) ; //<S2SV> ext_debug ( "int.index<S2SV_blank>at<S2SV_blank>%d<S2SV_blank>(block<S2SV_blank>%llu):<S2SV_blank>%u<S2SV_blank>-><S2SV_blank>%llu\\n" , //<S2SV> i , newblock , le32_to_cpu ( border ) , oldblock ) ; //<S2SV> if ( unlikely ( EXT_MAX_INDEX ( path [ i ] . p_hdr ) != //<S2SV> EXT_LAST_INDEX ( path [ i ] . p_hdr ) ) ) { //<S2SV> EXT4_ERROR_INODE ( inode , //<S2SV> "EXT_MAX_INDEX<S2SV_blank>!=<S2SV_blank>EXT_LAST_INDEX<S2SV_blank>ee_block<S2SV_blank>%d!" , //<S2SV> le32_to_cpu ( path [ i ] . p_ext -> ee_block ) ) ; //<S2SV> err = - EFSCORRUPTED ; //<S2SV> goto cleanup ; //<S2SV> } //<S2SV> m = EXT_MAX_INDEX ( path [ i ] . p_hdr ) - path [ i ] . p_idx ++ ; //<S2SV> ext_debug ( "cur<S2SV_blank>0x%p,<S2SV_blank>last<S2SV_blank>0x%p\\n" , path [ i ] . p_idx , //<S2SV> EXT_MAX_INDEX ( path [ i ] . p_hdr ) ) ; //<S2SV> ext4_ext_show_move ( inode , path , newblock , i ) ; //<S2SV> if ( m ) { //<S2SV> memmove ( ++ fidx , path [ i ] . p_idx , //<S2SV> sizeof ( struct ext4_extent_idx ) * m ) ; //<S2SV> le16_add_cpu ( & neh -> eh_entries , m ) ; //<S2SV> } //<S2SV> ext4_extent_block_csum_set ( inode , neh ) ; //<S2SV> set_buffer_uptodate ( bh ) ; //<S2SV> unlock_buffer ( bh ) ; //<S2SV> err = ext4_handle_dirty_metadata ( handle , inode , bh ) ; //<S2SV> if ( err ) //<S2SV> goto cleanup ; //<S2SV> brelse ( bh ) ; //<S2SV> bh = NULL ; //<S2SV> if ( m ) { //<S2SV> err = ext4_ext_get_access ( handle , inode , path + i ) ; //<S2SV> if ( err ) //<S2SV> goto cleanup ; //<S2SV> le16_add_cpu ( & path [ i ] . p_hdr -> eh_entries , - m ) ; //<S2SV> err = ext4_ext_dirty ( handle , inode , path + i ) ; //<S2SV> if ( err ) //<S2SV> goto cleanup ; //<S2SV> } //<S2SV> i -- ; //<S2SV> } //<S2SV> err = ext4_ext_insert_index ( handle , inode , path + at , //<S2SV> le32_to_cpu ( border ) , newblock ) ; //<S2SV> cleanup : //<S2SV> if ( bh ) { //<S2SV> if ( buffer_locked ( bh ) ) //<S2SV> unlock_buffer ( bh ) ; //<S2SV> brelse ( bh ) ; //<S2SV> } //<S2SV> if ( err ) { //<S2SV> for ( i = 0 ; i < depth ; i ++ ) { //<S2SV> if ( ! ablocks [ i ] ) //<S2SV> continue ; //<S2SV> ext4_free_blocks ( handle , inode , NULL , ablocks [ i ] , 1 , //<S2SV> EXT4_FREE_BLOCKS_METADATA ) ; //<S2SV> } //<S2SV> } //<S2SV> kfree ( ablocks ) ; //<S2SV> return err ; //<S2SV> } //<S2SV> 