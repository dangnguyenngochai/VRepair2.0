static void sctp_sock_migrate ( struct sock * oldsk , struct sock * newsk , //<S2SV> struct sctp_association * assoc , //<S2SV> sctp_socket_type_t type ) //<S2SV> { //<S2SV> struct sctp_sock * oldsp = sctp_sk ( oldsk ) ; //<S2SV> struct sctp_sock * newsp = sctp_sk ( newsk ) ; //<S2SV> struct sctp_bind_bucket * pp ; //<S2SV> struct sctp_endpoint * newep = newsp -> ep ; //<S2SV> struct sk_buff * skb , * tmp ; //<S2SV> struct sctp_ulpevent * event ; //<S2SV> struct sctp_bind_hashbucket * head ; //<S2SV> struct list_head tmplist ; //<S2SV> newsk -> sk_sndbuf = oldsk -> sk_sndbuf ; //<S2SV> newsk -> sk_rcvbuf = oldsk -> sk_rcvbuf ; //<S2SV> if ( oldsp -> do_auto_asconf ) { //<S2SV> memcpy ( & tmplist , & newsp -> auto_asconf_list , sizeof ( tmplist ) ) ; //<S2SV> inet_sk_copy_descendant ( newsk , oldsk ) ; //<S2SV> memcpy ( & newsp -> auto_asconf_list , & tmplist , sizeof ( tmplist ) ) ; //<S2SV> } else //<S2SV> inet_sk_copy_descendant ( newsk , oldsk ) ; //<S2SV> newsp -> ep = newep ; //<S2SV> newsp -> hmac = NULL ; //<S2SV> head = & sctp_port_hashtable [ sctp_phashfn ( sock_net ( oldsk ) , //<S2SV> inet_sk ( oldsk ) -> inet_num ) ] ; //<S2SV> local_bh_disable ( ) ; //<S2SV> spin_lock ( & head -> lock ) ; //<S2SV> pp = sctp_sk ( oldsk ) -> bind_hash ; //<S2SV> sk_add_bind_node ( newsk , & pp -> owner ) ; //<S2SV> sctp_sk ( newsk ) -> bind_hash = pp ; //<S2SV> inet_sk ( newsk ) -> inet_num = inet_sk ( oldsk ) -> inet_num ; //<S2SV> spin_unlock ( & head -> lock ) ; //<S2SV> local_bh_enable ( ) ; //<S2SV> sctp_bind_addr_dup ( & newsp -> ep -> base . bind_addr , //<S2SV> & oldsp -> ep -> base . bind_addr , GFP_KERNEL ) ; //<S2SV> sctp_skb_for_each ( skb , & oldsk -> sk_receive_queue , tmp ) { //<S2SV> event = sctp_skb2event ( skb ) ; //<S2SV> if ( event -> asoc == assoc ) { //<S2SV> __skb_unlink ( skb , & oldsk -> sk_receive_queue ) ; //<S2SV> __skb_queue_tail ( & newsk -> sk_receive_queue , skb ) ; //<S2SV> sctp_skb_set_owner_r_frag ( skb , newsk ) ; //<S2SV> } //<S2SV> } //<S2SV> skb_queue_head_init ( & newsp -> pd_lobby ) ; //<S2SV> atomic_set ( & sctp_sk ( newsk ) -> pd_mode , assoc -> ulpq . pd_mode ) ; //<S2SV> if ( atomic_read ( & sctp_sk ( oldsk ) -> pd_mode ) ) { //<S2SV> struct sk_buff_head * queue ; //<S2SV> if ( assoc -> ulpq . pd_mode ) { //<S2SV> queue = & newsp -> pd_lobby ; //<S2SV> } else //<S2SV> queue = & newsk -> sk_receive_queue ; //<S2SV> sctp_skb_for_each ( skb , & oldsp -> pd_lobby , tmp ) { //<S2SV> event = sctp_skb2event ( skb ) ; //<S2SV> if ( event -> asoc == assoc ) { //<S2SV> __skb_unlink ( skb , & oldsp -> pd_lobby ) ; //<S2SV> __skb_queue_tail ( queue , skb ) ; //<S2SV> sctp_skb_set_owner_r_frag ( skb , newsk ) ; //<S2SV> } //<S2SV> } //<S2SV> if ( assoc -> ulpq . pd_mode ) //<S2SV> sctp_clear_pd ( oldsk , NULL ) ; //<S2SV> } //<S2SV> sctp_skb_for_each ( skb , & assoc -> ulpq . reasm , tmp ) //<S2SV> sctp_skb_set_owner_r_frag ( skb , newsk ) ; //<S2SV> sctp_skb_for_each ( skb , & assoc -> ulpq . lobby , tmp ) //<S2SV> sctp_skb_set_owner_r_frag ( skb , newsk ) ; //<S2SV> newsp -> type = type ; //<S2SV> lock_sock_nested ( newsk , SINGLE_DEPTH_NESTING ) ; //<S2SV> sctp_assoc_migrate ( assoc , newsk ) ; //<S2SV> if ( sctp_state ( assoc , CLOSED ) && sctp_style ( newsk , TCP ) ) //<S2SV> newsk -> sk_shutdown |= RCV_SHUTDOWN ; //<S2SV> newsk -> sk_state = SCTP_SS_ESTABLISHED ; //<S2SV> release_sock ( newsk ) ; //<S2SV> } //<S2SV> 