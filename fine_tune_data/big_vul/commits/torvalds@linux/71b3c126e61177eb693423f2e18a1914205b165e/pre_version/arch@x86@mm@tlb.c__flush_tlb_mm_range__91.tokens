void flush_tlb_mm_range ( struct mm_struct * mm , unsigned long start , //<S2SV> unsigned long end , unsigned long vmflag ) //<S2SV> { //<S2SV> unsigned long addr ; //<S2SV> unsigned long base_pages_to_flush = TLB_FLUSH_ALL ; //<S2SV> preempt_disable ( ) ; //<S2SV> if ( current -> active_mm != mm ) //<S2SV> goto out ; //<S2SV> if ( ! current -> mm ) { //<S2SV> leave_mm ( smp_processor_id ( ) ) ; //<S2SV> goto out ; //<S2SV> } //<S2SV> if ( ( end != TLB_FLUSH_ALL ) && ! ( vmflag & VM_HUGETLB ) ) //<S2SV> base_pages_to_flush = ( end - start ) >> PAGE_SHIFT ; //<S2SV> if ( base_pages_to_flush > tlb_single_page_flush_ceiling ) { //<S2SV> base_pages_to_flush = TLB_FLUSH_ALL ; //<S2SV> count_vm_tlb_event ( NR_TLB_LOCAL_FLUSH_ALL ) ; //<S2SV> local_flush_tlb ( ) ; //<S2SV> } else { //<S2SV> for ( addr = start ; addr < end ; addr += PAGE_SIZE ) { //<S2SV> count_vm_tlb_event ( NR_TLB_LOCAL_FLUSH_ONE ) ; //<S2SV> __flush_tlb_single ( addr ) ; //<S2SV> } //<S2SV> } //<S2SV> trace_tlb_flush ( TLB_LOCAL_MM_SHOOTDOWN , base_pages_to_flush ) ; //<S2SV> out : //<S2SV> if ( base_pages_to_flush == TLB_FLUSH_ALL ) { //<S2SV> start = 0UL ; //<S2SV> end = TLB_FLUSH_ALL ; //<S2SV> } //<S2SV> if ( cpumask_any_but ( mm_cpumask ( mm ) , smp_processor_id ( ) ) < nr_cpu_ids ) //<S2SV> flush_tlb_others ( mm_cpumask ( mm ) , mm , start , end ) ; //<S2SV> preempt_enable ( ) ; //<S2SV> } //<S2SV> 