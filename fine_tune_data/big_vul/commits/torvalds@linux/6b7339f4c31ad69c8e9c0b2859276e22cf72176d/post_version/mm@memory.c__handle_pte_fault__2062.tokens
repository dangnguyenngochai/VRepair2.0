static int handle_pte_fault ( struct mm_struct * mm , //<S2SV> struct vm_area_struct * vma , unsigned long address , //<S2SV> pte_t * pte , pmd_t * pmd , unsigned int flags ) //<S2SV> { //<S2SV> pte_t entry ; //<S2SV> spinlock_t * ptl ; //<S2SV> entry = * pte ; //<S2SV> barrier ( ) ; //<S2SV> if ( ! pte_present ( entry ) ) { //<S2SV> if ( pte_none ( entry ) ) { //<S2SV> if ( vma -> vm_ops ) //<S2SV> return do_fault ( mm , vma , address , pte , pmd , //<S2SV> flags , entry ) ; //<S2SV> return do_anonymous_page ( mm , vma , address , pte , pmd , //<S2SV> flags ) ; //<S2SV> } //<S2SV> return do_swap_page ( mm , vma , address , //<S2SV> pte , pmd , flags , entry ) ; //<S2SV> } //<S2SV> if ( pte_protnone ( entry ) ) //<S2SV> return do_numa_page ( mm , vma , address , entry , pte , pmd ) ; //<S2SV> ptl = pte_lockptr ( mm , pmd ) ; //<S2SV> spin_lock ( ptl ) ; //<S2SV> if ( unlikely ( ! pte_same ( * pte , entry ) ) ) //<S2SV> goto unlock ; //<S2SV> if ( flags & FAULT_FLAG_WRITE ) { //<S2SV> if ( ! pte_write ( entry ) ) //<S2SV> return do_wp_page ( mm , vma , address , //<S2SV> pte , pmd , ptl , entry ) ; //<S2SV> entry = pte_mkdirty ( entry ) ; //<S2SV> } //<S2SV> entry = pte_mkyoung ( entry ) ; //<S2SV> if ( ptep_set_access_flags ( vma , address , pte , entry , flags & FAULT_FLAG_WRITE ) ) { //<S2SV> update_mmu_cache ( vma , address , pte ) ; //<S2SV> } else { //<S2SV> if ( flags & FAULT_FLAG_WRITE ) //<S2SV> flush_tlb_fix_spurious_fault ( vma , address ) ; //<S2SV> } //<S2SV> unlock : //<S2SV> pte_unmap_unlock ( pte , ptl ) ; //<S2SV> return 0 ; //<S2SV> } //<S2SV> 